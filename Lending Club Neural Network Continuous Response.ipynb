{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Adjust Pandas DF View Options\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skip down until you see continue here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lc = pickle.load(open('D:/ML Case Study/lending-club/lc_continuous.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1285998, 96)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                    0\n",
       "loan_amnt                             0\n",
       "funded_amnt                           0\n",
       "funded_amnt_inv                       0\n",
       "term                                  0\n",
       "int_rate                              0\n",
       "installment                           0\n",
       "grade                                 0\n",
       "sub_grade                             0\n",
       "home_ownership                        0\n",
       "annual_inc                            0\n",
       "verification_status                   0\n",
       "issue_d                               0\n",
       "loan_status                           0\n",
       "purpose                               0\n",
       "zip_code                              0\n",
       "addr_state                            0\n",
       "dti                                   0\n",
       "delinq_2yrs                           0\n",
       "earliest_cr_line                      0\n",
       "inq_last_6mths                        0\n",
       "open_acc                              0\n",
       "pub_rec                               0\n",
       "revol_bal                             0\n",
       "revol_util                            0\n",
       "total_acc                             0\n",
       "initial_list_status                   0\n",
       "out_prncp                             0\n",
       "out_prncp_inv                         0\n",
       "total_pymnt                           0\n",
       "total_pymnt_inv                       0\n",
       "total_rec_prncp                       0\n",
       "total_rec_int                         0\n",
       "total_rec_late_fee                    0\n",
       "recoveries                            0\n",
       "collection_recovery_fee               0\n",
       "last_pymnt_amnt                       0\n",
       "last_credit_pull_d                    0\n",
       "last_fico_range_high                  0\n",
       "last_fico_range_low                   0\n",
       "collections_12_mths_ex_med            0\n",
       "application_type                      0\n",
       "acc_now_delinq                        0\n",
       "tot_coll_amt                          0\n",
       "tot_cur_bal                           0\n",
       "total_rev_hi_lim                      0\n",
       "acc_open_past_24mths                  0\n",
       "avg_cur_bal                           0\n",
       "bc_open_to_buy                        0\n",
       "bc_util                               0\n",
       "chargeoff_within_12_mths              0\n",
       "delinq_amnt                           0\n",
       "mo_sin_old_rev_tl_op                  0\n",
       "mo_sin_rcnt_rev_tl_op                 0\n",
       "mo_sin_rcnt_tl                        0\n",
       "mort_acc                              0\n",
       "num_accts_ever_120_pd                 0\n",
       "num_actv_bc_tl                        0\n",
       "num_actv_rev_tl                       0\n",
       "num_bc_sats                           0\n",
       "num_bc_tl                             0\n",
       "num_il_tl                             0\n",
       "num_op_rev_tl                         0\n",
       "num_rev_accts                         0\n",
       "num_rev_tl_bal_gt_0                   0\n",
       "num_sats                              0\n",
       "num_tl_120dpd_2m                      0\n",
       "num_tl_30dpd                          0\n",
       "num_tl_90g_dpd_24m                    0\n",
       "num_tl_op_past_12m                    0\n",
       "pct_tl_nvr_dlq                        0\n",
       "percent_bc_gt_75                      0\n",
       "pub_rec_bankruptcies                  0\n",
       "tax_liens                             0\n",
       "tot_hi_cred_lim                       0\n",
       "total_bal_ex_mort                     0\n",
       "total_bc_limit                        0\n",
       "total_il_high_credit_limit            0\n",
       "disbursement_method                   0\n",
       "debt_settlement_flag                  0\n",
       "sec_app_flag                          0\n",
       "sec_app_fico_best                     0\n",
       "annual_inc_final                      0\n",
       "earliest_cr_line_months               0\n",
       "emp_length_floats                     0\n",
       "average_fico                          0\n",
       "dti_final                             0\n",
       "mths_since_recent_bc_cat              0\n",
       "mo_sin_old_il_acct_cat                0\n",
       "mths_since_recent_inq_cat             0\n",
       "mths_since_last_delinq_cat            0\n",
       "mths_since_rcnt_il_cat                0\n",
       "mths_since_recent_revol_delinq_cat    0\n",
       "mths_since_last_major_derog_cat       0\n",
       "mths_since_recent_bc_dlq_cat          0\n",
       "mths_since_last_record_cat            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Are there any NAs?\n",
    "pd.isnull(lc).sum()\n",
    "#nope!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lc.term.head()\n",
    "lc['term'] = lc['term'].str.split(' ').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc['term']=pd.to_numeric(lc['term'])\n",
    "lc['term'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36    909462\n",
       "60    376536\n",
       "Name: term, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.term.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "earliest_cr_line    datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.dtypes.loc[lc.dtypes=='datetime64[ns]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lc[['earliest_cr_line_year','earliest_cr_line_month','earliest_cr_line_day']] = lc.earliest_cr_line.apply(lambda x: pd.Series(x.strftime(\"%Y,%m,%d\").split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "01    1285998\n",
       "Name: earliest_cr_line_day, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.earliest_cr_line_day.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del lc['earliest_cr_line_day']\n",
    "del lc['earliest_cr_line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "d = dict((v,k) for k,v in enumerate(calendar.month_abbr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lc['last_credit_pull_month']=lc.last_credit_pull_d.str.split('-').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lc['last_credit_pull_year']=lc.last_credit_pull_d.str.split('-').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lc.last_credit_pull_month = lc.last_credit_pull_month.map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del lc['last_credit_pull_d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed columns that had identical relationships to the analysis in question (E.g. funded_amnt and funded_amnt_inv as they are always the same as loan_amt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10400.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>10400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21425.0</td>\n",
       "      <td>21425.0</td>\n",
       "      <td>21425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12800.0</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>12800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9600.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>9600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23325.0</td>\n",
       "      <td>23325.0</td>\n",
       "      <td>23325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5250.0</td>\n",
       "      <td>5250.0</td>\n",
       "      <td>5250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12975.0</td>\n",
       "      <td>12975.0</td>\n",
       "      <td>12975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21075.0</td>\n",
       "      <td>21075.0</td>\n",
       "      <td>21075.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13550.0</td>\n",
       "      <td>13550.0</td>\n",
       "      <td>13550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7200.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>7200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10800.0</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>10800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9600.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>9600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>11175.0</td>\n",
       "      <td>11175.0</td>\n",
       "      <td>11175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5675.0</td>\n",
       "      <td>5675.0</td>\n",
       "      <td>5675.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6825.0</td>\n",
       "      <td>6825.0</td>\n",
       "      <td>6825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4200.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>4200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4500.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>32800.0</td>\n",
       "      <td>32800.0</td>\n",
       "      <td>32800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>10975.0</td>\n",
       "      <td>10975.0</td>\n",
       "      <td>10975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>8750.0</td>\n",
       "      <td>8750.0</td>\n",
       "      <td>8750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>27525.0</td>\n",
       "      <td>27525.0</td>\n",
       "      <td>27525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>4600.0</td>\n",
       "      <td>4600.0</td>\n",
       "      <td>4600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6400.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>6400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2800.0</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>2800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>13375.0</td>\n",
       "      <td>13375.0</td>\n",
       "      <td>13375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>19200.0</td>\n",
       "      <td>19200.0</td>\n",
       "      <td>19200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>15600.0</td>\n",
       "      <td>15600.0</td>\n",
       "      <td>15600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>22200.0</td>\n",
       "      <td>22200.0</td>\n",
       "      <td>22200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>10375.0</td>\n",
       "      <td>10375.0</td>\n",
       "      <td>10375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>5600.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>5600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>12600.0</td>\n",
       "      <td>12600.0</td>\n",
       "      <td>12600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>4800.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>4800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>22500.0</td>\n",
       "      <td>22500.0</td>\n",
       "      <td>22500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>16950.0</td>\n",
       "      <td>16950.0</td>\n",
       "      <td>16950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>16375.0</td>\n",
       "      <td>16375.0</td>\n",
       "      <td>16375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>6700.0</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>6700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>18700.0</td>\n",
       "      <td>18700.0</td>\n",
       "      <td>18700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>26000.0</td>\n",
       "      <td>26000.0</td>\n",
       "      <td>26000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>6850.0</td>\n",
       "      <td>6850.0</td>\n",
       "      <td>6850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>13400.0</td>\n",
       "      <td>13400.0</td>\n",
       "      <td>13400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>14575.0</td>\n",
       "      <td>14575.0</td>\n",
       "      <td>14575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>16400.0</td>\n",
       "      <td>16400.0</td>\n",
       "      <td>16400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>27200.0</td>\n",
       "      <td>27200.0</td>\n",
       "      <td>27200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>22400.0</td>\n",
       "      <td>22400.0</td>\n",
       "      <td>22400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>22000.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>22000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2850.0</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>2850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>15850.0</td>\n",
       "      <td>15850.0</td>\n",
       "      <td>15850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6150.0</td>\n",
       "      <td>6150.0</td>\n",
       "      <td>6150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>17500.0</td>\n",
       "      <td>17500.0</td>\n",
       "      <td>17500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>10200.0</td>\n",
       "      <td>10200.0</td>\n",
       "      <td>10200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>6500.0</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>6500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>26400.0</td>\n",
       "      <td>26400.0</td>\n",
       "      <td>26400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>14400.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>14400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>11200.0</td>\n",
       "      <td>11200.0</td>\n",
       "      <td>11200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>11325.0</td>\n",
       "      <td>11325.0</td>\n",
       "      <td>11325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>19500.0</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>19500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>16475.0</td>\n",
       "      <td>16475.0</td>\n",
       "      <td>16475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>18500.0</td>\n",
       "      <td>18500.0</td>\n",
       "      <td>18500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>19150.0</td>\n",
       "      <td>19150.0</td>\n",
       "      <td>19150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>33950.0</td>\n",
       "      <td>33950.0</td>\n",
       "      <td>33950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>8250.0</td>\n",
       "      <td>8250.0</td>\n",
       "      <td>8250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>13825.0</td>\n",
       "      <td>13825.0</td>\n",
       "      <td>13825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>23475.0</td>\n",
       "      <td>23475.0</td>\n",
       "      <td>23475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>11850.0</td>\n",
       "      <td>11850.0</td>\n",
       "      <td>11850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>3525.0</td>\n",
       "      <td>3525.0</td>\n",
       "      <td>3525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>8325.0</td>\n",
       "      <td>8325.0</td>\n",
       "      <td>8325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>9500.0</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>9500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>3750.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>3750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>13500.0</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>13500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>27975.0</td>\n",
       "      <td>27975.0</td>\n",
       "      <td>27975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>6400.0</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>6400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>22000.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>22000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>23000.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>23000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>31000.0</td>\n",
       "      <td>31000.0</td>\n",
       "      <td>31000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>22500.0</td>\n",
       "      <td>22500.0</td>\n",
       "      <td>22500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>19200.0</td>\n",
       "      <td>19200.0</td>\n",
       "      <td>19200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>16950.0</td>\n",
       "      <td>16950.0</td>\n",
       "      <td>16950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>13425.0</td>\n",
       "      <td>13425.0</td>\n",
       "      <td>13425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>26150.0</td>\n",
       "      <td>26150.0</td>\n",
       "      <td>26150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>23925.0</td>\n",
       "      <td>23925.0</td>\n",
       "      <td>23925.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>5500.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>5500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>9600.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>9600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>13225.0</td>\n",
       "      <td>13225.0</td>\n",
       "      <td>13225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>27000.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>27000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>13475.0</td>\n",
       "      <td>13475.0</td>\n",
       "      <td>13475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>23325.0</td>\n",
       "      <td>23325.0</td>\n",
       "      <td>23325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>11500.0</td>\n",
       "      <td>11500.0</td>\n",
       "      <td>11500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>3500.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645175</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645176</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645177</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645178</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645180</th>\n",
       "      <td>9200.0</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>9200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645183</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645184</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645185</th>\n",
       "      <td>8300.0</td>\n",
       "      <td>8300.0</td>\n",
       "      <td>8300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645187</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645188</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645189</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645190</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645191</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645192</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645193</th>\n",
       "      <td>6800.0</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>6800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645194</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645196</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645198</th>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645200</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645201</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645202</th>\n",
       "      <td>28800.0</td>\n",
       "      <td>28800.0</td>\n",
       "      <td>28800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645203</th>\n",
       "      <td>11200.0</td>\n",
       "      <td>11200.0</td>\n",
       "      <td>11200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645204</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645205</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645206</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>11950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645207</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645208</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645210</th>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645212</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645214</th>\n",
       "      <td>12850.0</td>\n",
       "      <td>12850.0</td>\n",
       "      <td>12850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645215</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645217</th>\n",
       "      <td>33600.0</td>\n",
       "      <td>33600.0</td>\n",
       "      <td>33600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645218</th>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645220</th>\n",
       "      <td>20450.0</td>\n",
       "      <td>20450.0</td>\n",
       "      <td>20450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645221</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645222</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645224</th>\n",
       "      <td>3500.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645226</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645227</th>\n",
       "      <td>24500.0</td>\n",
       "      <td>24500.0</td>\n",
       "      <td>24500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645228</th>\n",
       "      <td>12950.0</td>\n",
       "      <td>12950.0</td>\n",
       "      <td>12950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645229</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645230</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645232</th>\n",
       "      <td>8400.0</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>8400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645233</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645234</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645236</th>\n",
       "      <td>19800.0</td>\n",
       "      <td>19800.0</td>\n",
       "      <td>19800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645237</th>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645238</th>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645239</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645241</th>\n",
       "      <td>7825.0</td>\n",
       "      <td>7825.0</td>\n",
       "      <td>7825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645242</th>\n",
       "      <td>4800.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>4800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645243</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645245</th>\n",
       "      <td>12800.0</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>12800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645246</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645247</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645248</th>\n",
       "      <td>21250.0</td>\n",
       "      <td>21250.0</td>\n",
       "      <td>21200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645249</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645250</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645251</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645252</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645253</th>\n",
       "      <td>5150.0</td>\n",
       "      <td>5150.0</td>\n",
       "      <td>5150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645254</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645255</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645256</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645258</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645259</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645260</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645261</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645262</th>\n",
       "      <td>9600.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>9600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645264</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645265</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645267</th>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645268</th>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>24950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645269</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645270</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645271</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645272</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645273</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645274</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645275</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645277</th>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645278</th>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645281</th>\n",
       "      <td>32000.0</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>32000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645282</th>\n",
       "      <td>19075.0</td>\n",
       "      <td>19075.0</td>\n",
       "      <td>19075.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645284</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645285</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645287</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645288</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645289</th>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645290</th>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645293</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645294</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645295</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645298</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645299</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645300</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645301</th>\n",
       "      <td>11250.0</td>\n",
       "      <td>11250.0</td>\n",
       "      <td>11250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645303</th>\n",
       "      <td>19600.0</td>\n",
       "      <td>19600.0</td>\n",
       "      <td>19600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645307</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645308</th>\n",
       "      <td>14400.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>14400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645309</th>\n",
       "      <td>29475.0</td>\n",
       "      <td>29475.0</td>\n",
       "      <td>29475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645310</th>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645312</th>\n",
       "      <td>32000.0</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>32000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645313</th>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645314</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645315</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645316</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645317</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645319</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645320</th>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>14000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645324</th>\n",
       "      <td>7150.0</td>\n",
       "      <td>7150.0</td>\n",
       "      <td>7150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645325</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645326</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645327</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645328</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645329</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645331</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645332</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645333</th>\n",
       "      <td>26000.0</td>\n",
       "      <td>26000.0</td>\n",
       "      <td>26000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645335</th>\n",
       "      <td>14475.0</td>\n",
       "      <td>14475.0</td>\n",
       "      <td>14475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645336</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645337</th>\n",
       "      <td>4800.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>4800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645338</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645339</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645340</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645341</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645342</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645343</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645345</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645347</th>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>13000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645348</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645351</th>\n",
       "      <td>8550.0</td>\n",
       "      <td>8550.0</td>\n",
       "      <td>8550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645352</th>\n",
       "      <td>24375.0</td>\n",
       "      <td>24375.0</td>\n",
       "      <td>24350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645353</th>\n",
       "      <td>10200.0</td>\n",
       "      <td>10200.0</td>\n",
       "      <td>10200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645354</th>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645356</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645358</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645359</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645361</th>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645363</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645364</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645365</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645366</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645367</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645368</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645369</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645370</th>\n",
       "      <td>15400.0</td>\n",
       "      <td>15400.0</td>\n",
       "      <td>15400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645373</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645374</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645375</th>\n",
       "      <td>12050.0</td>\n",
       "      <td>12050.0</td>\n",
       "      <td>11900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645377</th>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645378</th>\n",
       "      <td>13600.0</td>\n",
       "      <td>13600.0</td>\n",
       "      <td>13600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645379</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>14850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645380</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645381</th>\n",
       "      <td>13200.0</td>\n",
       "      <td>13200.0</td>\n",
       "      <td>13200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645382</th>\n",
       "      <td>16800.0</td>\n",
       "      <td>16800.0</td>\n",
       "      <td>16800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645383</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645384</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645387</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645388</th>\n",
       "      <td>6200.0</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>6200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645389</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645390</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645391</th>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645393</th>\n",
       "      <td>5600.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>5600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645394</th>\n",
       "      <td>4750.0</td>\n",
       "      <td>4750.0</td>\n",
       "      <td>4750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645395</th>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645396</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645397</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645398</th>\n",
       "      <td>3825.0</td>\n",
       "      <td>3825.0</td>\n",
       "      <td>3825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645399</th>\n",
       "      <td>32000.0</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>32000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645400</th>\n",
       "      <td>15700.0</td>\n",
       "      <td>15700.0</td>\n",
       "      <td>15700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645401</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645402</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645404</th>\n",
       "      <td>17325.0</td>\n",
       "      <td>17325.0</td>\n",
       "      <td>17325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645405</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645406</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>27950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645407</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645408</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645409</th>\n",
       "      <td>10975.0</td>\n",
       "      <td>10975.0</td>\n",
       "      <td>10975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645412</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645413</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645414</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645417</th>\n",
       "      <td>4500.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645418</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645420</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645423</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645424</th>\n",
       "      <td>16600.0</td>\n",
       "      <td>16600.0</td>\n",
       "      <td>16600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645425</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645426</th>\n",
       "      <td>5600.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>5600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645429</th>\n",
       "      <td>5025.0</td>\n",
       "      <td>5025.0</td>\n",
       "      <td>4875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645430</th>\n",
       "      <td>11050.0</td>\n",
       "      <td>11050.0</td>\n",
       "      <td>10900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645431</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645432</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645433</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645434</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645435</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645436</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645437</th>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645438</th>\n",
       "      <td>12950.0</td>\n",
       "      <td>12950.0</td>\n",
       "      <td>12800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645439</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645440</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645441</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645442</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645444</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645445</th>\n",
       "      <td>3600.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645446</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645447</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645448</th>\n",
       "      <td>8400.0</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>8400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645451</th>\n",
       "      <td>9500.0</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>9500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645453</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645454</th>\n",
       "      <td>7225.0</td>\n",
       "      <td>7225.0</td>\n",
       "      <td>7225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645455</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645456</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645457</th>\n",
       "      <td>19300.0</td>\n",
       "      <td>19300.0</td>\n",
       "      <td>19300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645458</th>\n",
       "      <td>10575.0</td>\n",
       "      <td>10575.0</td>\n",
       "      <td>10575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645459</th>\n",
       "      <td>16800.0</td>\n",
       "      <td>16800.0</td>\n",
       "      <td>16800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645460</th>\n",
       "      <td>13200.0</td>\n",
       "      <td>13200.0</td>\n",
       "      <td>13175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645461</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645462</th>\n",
       "      <td>12800.0</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>12800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645463</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645464</th>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645466</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>29850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645470</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645471</th>\n",
       "      <td>24700.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>24700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645472</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645474</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>30000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645476</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645478</th>\n",
       "      <td>7625.0</td>\n",
       "      <td>7625.0</td>\n",
       "      <td>7625.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645479</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645480</th>\n",
       "      <td>10975.0</td>\n",
       "      <td>10975.0</td>\n",
       "      <td>10825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645481</th>\n",
       "      <td>3975.0</td>\n",
       "      <td>3975.0</td>\n",
       "      <td>3975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645485</th>\n",
       "      <td>5200.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>5200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645487</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645488</th>\n",
       "      <td>19000.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>18750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645489</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645490</th>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645492</th>\n",
       "      <td>4500.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645493</th>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645495</th>\n",
       "      <td>34000.0</td>\n",
       "      <td>34000.0</td>\n",
       "      <td>34000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645496</th>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645497</th>\n",
       "      <td>25000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>24800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645499</th>\n",
       "      <td>12600.0</td>\n",
       "      <td>12600.0</td>\n",
       "      <td>12600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645501</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645502</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645504</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645507</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645508</th>\n",
       "      <td>24100.0</td>\n",
       "      <td>24100.0</td>\n",
       "      <td>24100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645509</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645514</th>\n",
       "      <td>34050.0</td>\n",
       "      <td>34050.0</td>\n",
       "      <td>34050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645515</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1285998 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loan_amnt  funded_amnt  funded_amnt_inv\n",
       "0          15000.0      15000.0          15000.0\n",
       "1          10400.0      10400.0          10400.0\n",
       "2          21425.0      21425.0          21425.0\n",
       "3          12800.0      12800.0          12800.0\n",
       "5           9600.0       9600.0           9600.0\n",
       "7          16000.0      16000.0          16000.0\n",
       "8          23325.0      23325.0          23325.0\n",
       "9           5250.0       5250.0           5250.0\n",
       "10         10000.0      10000.0          10000.0\n",
       "11         12975.0      12975.0          12975.0\n",
       "12         17000.0      17000.0          17000.0\n",
       "13         21075.0      21075.0          21075.0\n",
       "14          6000.0       6000.0           6000.0\n",
       "15          2000.0       2000.0           2000.0\n",
       "16          3000.0       3000.0           3000.0\n",
       "17          2000.0       2000.0           2000.0\n",
       "18          2000.0       2000.0           2000.0\n",
       "19         13550.0      13550.0          13550.0\n",
       "20          7200.0       7200.0           7200.0\n",
       "21         14000.0      14000.0          14000.0\n",
       "22          1000.0       1000.0           1000.0\n",
       "23          6000.0       6000.0           6000.0\n",
       "24         18000.0      18000.0          18000.0\n",
       "26         10800.0      10800.0          10800.0\n",
       "28         28000.0      28000.0          28000.0\n",
       "29          4000.0       4000.0           4000.0\n",
       "30         15000.0      15000.0          15000.0\n",
       "32          9600.0       9600.0           9600.0\n",
       "35         11175.0      11175.0          11175.0\n",
       "36         21000.0      21000.0          21000.0\n",
       "37          8000.0       8000.0           8000.0\n",
       "39         28000.0      28000.0          28000.0\n",
       "40         11000.0      11000.0          11000.0\n",
       "41          5675.0       5675.0           5675.0\n",
       "42          6825.0       6825.0           6825.0\n",
       "43          5000.0       5000.0           5000.0\n",
       "44          1500.0       1500.0           1500.0\n",
       "46          3000.0       3000.0           3000.0\n",
       "47          8000.0       8000.0           8000.0\n",
       "48         15000.0      15000.0          15000.0\n",
       "49         14000.0      14000.0          14000.0\n",
       "50          5000.0       5000.0           5000.0\n",
       "51         16000.0      16000.0          16000.0\n",
       "52          4200.0       4200.0           4200.0\n",
       "53         14000.0      14000.0          14000.0\n",
       "54          2400.0       2400.0           2400.0\n",
       "55         18000.0      18000.0          18000.0\n",
       "56          1000.0       1000.0           1000.0\n",
       "57         24000.0      24000.0          24000.0\n",
       "58         12000.0      12000.0          12000.0\n",
       "59         20000.0      20000.0          20000.0\n",
       "61          4500.0       4500.0           4500.0\n",
       "62         32800.0      32800.0          32800.0\n",
       "63          5000.0       5000.0           5000.0\n",
       "64         10975.0      10975.0          10975.0\n",
       "65         10000.0      10000.0          10000.0\n",
       "66          8750.0       8750.0           8750.0\n",
       "67          9000.0       9000.0           9000.0\n",
       "68          6000.0       6000.0           6000.0\n",
       "69         21000.0      21000.0          21000.0\n",
       "70         10000.0      10000.0          10000.0\n",
       "71         27525.0      27525.0          27525.0\n",
       "72          2400.0       2400.0           2400.0\n",
       "73         12000.0      12000.0          12000.0\n",
       "74          4600.0       4600.0           4600.0\n",
       "75         24000.0      24000.0          24000.0\n",
       "76          6400.0       6400.0           6400.0\n",
       "77         18000.0      18000.0          18000.0\n",
       "78         24000.0      24000.0          24000.0\n",
       "79         21000.0      21000.0          21000.0\n",
       "80          2800.0       2800.0           2800.0\n",
       "81         13375.0      13375.0          13375.0\n",
       "82         15000.0      15000.0          15000.0\n",
       "83         13000.0      13000.0          13000.0\n",
       "84         35000.0      35000.0          35000.0\n",
       "85         35000.0      35000.0          35000.0\n",
       "86         15000.0      15000.0          15000.0\n",
       "87          7500.0       7500.0           7500.0\n",
       "89         19200.0      19200.0          19200.0\n",
       "90         12000.0      12000.0          12000.0\n",
       "91         28000.0      28000.0          28000.0\n",
       "92         24000.0      24000.0          24000.0\n",
       "93         15600.0      15600.0          15600.0\n",
       "94          5000.0       5000.0           5000.0\n",
       "95         22200.0      22200.0          22200.0\n",
       "96         10375.0      10375.0          10375.0\n",
       "97          5000.0       5000.0           5000.0\n",
       "98         15000.0      15000.0          15000.0\n",
       "99          5000.0       5000.0           5000.0\n",
       "100         6000.0       6000.0           6000.0\n",
       "101        19000.0      19000.0          19000.0\n",
       "102        28000.0      28000.0          28000.0\n",
       "103        35000.0      35000.0          35000.0\n",
       "104        18000.0      18000.0          18000.0\n",
       "105         6000.0       6000.0           6000.0\n",
       "107        28000.0      28000.0          28000.0\n",
       "108         5600.0       5600.0           5600.0\n",
       "110        12600.0      12600.0          12600.0\n",
       "111         8000.0       8000.0           8000.0\n",
       "112         8000.0       8000.0           8000.0\n",
       "113         4800.0       4800.0           4800.0\n",
       "114        22500.0      22500.0          22500.0\n",
       "115        16950.0      16950.0          16950.0\n",
       "116        16375.0      16375.0          16375.0\n",
       "117         3000.0       3000.0           3000.0\n",
       "118         5000.0       5000.0           5000.0\n",
       "119        21000.0      21000.0          21000.0\n",
       "120         8000.0       8000.0           8000.0\n",
       "121         6700.0       6700.0           6700.0\n",
       "122        12000.0      12000.0          12000.0\n",
       "123         3000.0       3000.0           3000.0\n",
       "124        18700.0      18700.0          18700.0\n",
       "125        26000.0      26000.0          26000.0\n",
       "126         7000.0       7000.0           7000.0\n",
       "127        21000.0      21000.0          21000.0\n",
       "128        15000.0      15000.0          15000.0\n",
       "129         6850.0       6850.0           6850.0\n",
       "130         1200.0       1200.0           1200.0\n",
       "131        13400.0      13400.0          13400.0\n",
       "134        14575.0      14575.0          14575.0\n",
       "135        14000.0      14000.0          14000.0\n",
       "136        16400.0      16400.0          16400.0\n",
       "137         8000.0       8000.0           8000.0\n",
       "138        27200.0      27200.0          27200.0\n",
       "139        22400.0      22400.0          22400.0\n",
       "140        22000.0      22000.0          22000.0\n",
       "141        20000.0      20000.0          20000.0\n",
       "142         2850.0       2850.0           2850.0\n",
       "144        10000.0      10000.0          10000.0\n",
       "145        15850.0      15850.0          15850.0\n",
       "146         6150.0       6150.0           6150.0\n",
       "147        25000.0      25000.0          25000.0\n",
       "148        10000.0      10000.0          10000.0\n",
       "149        28000.0      28000.0          28000.0\n",
       "151        17500.0      17500.0          17500.0\n",
       "152        10200.0      10200.0          10200.0\n",
       "153         3000.0       3000.0           3000.0\n",
       "154         7500.0       7500.0           7500.0\n",
       "155         6500.0       6500.0           6500.0\n",
       "156        10000.0      10000.0          10000.0\n",
       "157        26400.0      26400.0          26400.0\n",
       "158         9000.0       9000.0           9000.0\n",
       "159        14400.0      14400.0          14400.0\n",
       "160        18000.0      18000.0          18000.0\n",
       "161        11200.0      11200.0          11200.0\n",
       "162        30000.0      30000.0          30000.0\n",
       "163        15000.0      15000.0          15000.0\n",
       "164         5000.0       5000.0           5000.0\n",
       "165        20000.0      20000.0          20000.0\n",
       "167         2000.0       2000.0           2000.0\n",
       "168        24000.0      24000.0          24000.0\n",
       "171        17000.0      17000.0          17000.0\n",
       "172        21000.0      21000.0          21000.0\n",
       "173        10000.0      10000.0          10000.0\n",
       "175        12000.0      12000.0          12000.0\n",
       "176        35000.0      35000.0          35000.0\n",
       "177        12000.0      12000.0          12000.0\n",
       "178         5000.0       5000.0           5000.0\n",
       "179        14000.0      14000.0          14000.0\n",
       "180        11325.0      11325.0          11325.0\n",
       "181        19500.0      19500.0          19500.0\n",
       "182        16475.0      16475.0          16475.0\n",
       "184        18500.0      18500.0          18500.0\n",
       "185        19150.0      19150.0          19150.0\n",
       "186        21000.0      21000.0          21000.0\n",
       "187        33950.0      33950.0          33950.0\n",
       "189        10000.0      10000.0          10000.0\n",
       "190         8250.0       8250.0           8250.0\n",
       "192        13825.0      13825.0          13825.0\n",
       "193        16000.0      16000.0          16000.0\n",
       "194        20000.0      20000.0          20000.0\n",
       "195        23475.0      23475.0          23475.0\n",
       "196        10000.0      10000.0          10000.0\n",
       "198        28000.0      28000.0          28000.0\n",
       "200         5000.0       5000.0           5000.0\n",
       "201         8000.0       8000.0           8000.0\n",
       "203         5000.0       5000.0           5000.0\n",
       "204        35000.0      35000.0          35000.0\n",
       "205        11850.0      11850.0          11850.0\n",
       "206         3525.0       3525.0           3525.0\n",
       "208         8500.0       8500.0           8500.0\n",
       "209         8325.0       8325.0           8325.0\n",
       "210         9500.0       9500.0           9500.0\n",
       "211         6000.0       6000.0           6000.0\n",
       "212        35000.0      35000.0          35000.0\n",
       "213        20000.0      20000.0          20000.0\n",
       "214        21000.0      21000.0          21000.0\n",
       "215         9000.0       9000.0           9000.0\n",
       "216         7000.0       7000.0           7000.0\n",
       "217        24000.0      24000.0          24000.0\n",
       "218        15000.0      15000.0          15000.0\n",
       "219         7000.0       7000.0           7000.0\n",
       "220         3750.0       3750.0           3750.0\n",
       "221        12000.0      12000.0          12000.0\n",
       "222        35000.0      35000.0          35000.0\n",
       "223        20000.0      20000.0          20000.0\n",
       "225        13500.0      13500.0          13500.0\n",
       "227        10000.0      10000.0          10000.0\n",
       "229        27975.0      27975.0          27975.0\n",
       "230        12000.0      12000.0          12000.0\n",
       "231        20000.0      20000.0          20000.0\n",
       "232        18000.0      18000.0          18000.0\n",
       "233        10000.0      10000.0          10000.0\n",
       "234        20000.0      20000.0          20000.0\n",
       "235        24000.0      24000.0          24000.0\n",
       "236         6400.0       6400.0           6400.0\n",
       "237        25000.0      25000.0          25000.0\n",
       "238         2000.0       2000.0           2000.0\n",
       "239        22000.0      22000.0          22000.0\n",
       "240        35000.0      35000.0          35000.0\n",
       "241        23000.0      23000.0          23000.0\n",
       "242        10000.0      10000.0          10000.0\n",
       "243        31000.0      31000.0          31000.0\n",
       "244        22500.0      22500.0          22500.0\n",
       "247         3000.0       3000.0           3000.0\n",
       "248        19200.0      19200.0          19200.0\n",
       "249        16950.0      16950.0          16950.0\n",
       "250        13425.0      13425.0          13425.0\n",
       "251         6000.0       6000.0           6000.0\n",
       "252         2000.0       2000.0           2000.0\n",
       "253        26150.0      26150.0          26150.0\n",
       "254         2500.0       2500.0           2500.0\n",
       "255        12000.0      12000.0          12000.0\n",
       "256        23925.0      23925.0          23925.0\n",
       "257         5500.0       5500.0           5500.0\n",
       "258         9600.0       9600.0           9600.0\n",
       "259        12000.0      12000.0          12000.0\n",
       "260        15000.0      15000.0          15000.0\n",
       "261        13225.0      13225.0          13225.0\n",
       "262        27000.0      27000.0          27000.0\n",
       "263         6000.0       6000.0           6000.0\n",
       "264        35000.0      35000.0          35000.0\n",
       "265        15000.0      15000.0          15000.0\n",
       "266        12000.0      12000.0          12000.0\n",
       "267        10000.0      10000.0          10000.0\n",
       "270         4000.0       4000.0           4000.0\n",
       "271        13475.0      13475.0          13475.0\n",
       "272        14000.0      14000.0          14000.0\n",
       "274        23325.0      23325.0          23325.0\n",
       "275        11500.0      11500.0          11500.0\n",
       "276        20000.0      20000.0          20000.0\n",
       "278         7000.0       7000.0           7000.0\n",
       "279        25000.0      25000.0          25000.0\n",
       "280         9000.0       9000.0           9000.0\n",
       "281        10000.0      10000.0          10000.0\n",
       "282         7500.0       7500.0           7500.0\n",
       "283         6000.0       6000.0           6000.0\n",
       "285        20000.0      20000.0          20000.0\n",
       "287         3500.0       3500.0           3500.0\n",
       "288         2000.0       2000.0           2000.0\n",
       "...            ...          ...              ...\n",
       "1645175    15000.0      15000.0          15000.0\n",
       "1645176    12000.0      12000.0          12000.0\n",
       "1645177    18000.0      18000.0          18000.0\n",
       "1645178     4000.0       4000.0           4000.0\n",
       "1645180     9200.0       9200.0           9200.0\n",
       "1645183    35000.0      35000.0          35000.0\n",
       "1645184    20000.0      20000.0          20000.0\n",
       "1645185     8300.0       8300.0           8300.0\n",
       "1645187    15000.0      15000.0          15000.0\n",
       "1645188    10000.0      10000.0          10000.0\n",
       "1645189    12000.0      12000.0          12000.0\n",
       "1645190    15000.0      15000.0          15000.0\n",
       "1645191    20000.0      20000.0          20000.0\n",
       "1645192    24000.0      24000.0          24000.0\n",
       "1645193     6800.0       6800.0           6800.0\n",
       "1645194    10000.0      10000.0          10000.0\n",
       "1645196     5000.0       5000.0           5000.0\n",
       "1645198    13000.0      13000.0          13000.0\n",
       "1645200     5000.0       5000.0           5000.0\n",
       "1645201    20000.0      20000.0          20000.0\n",
       "1645202    28800.0      28800.0          28800.0\n",
       "1645203    11200.0      11200.0          11200.0\n",
       "1645204    10000.0      10000.0          10000.0\n",
       "1645205    20000.0      20000.0          20000.0\n",
       "1645206    12000.0      12000.0          11950.0\n",
       "1645207     3000.0       3000.0           3000.0\n",
       "1645208    21000.0      21000.0          21000.0\n",
       "1645210    13000.0      13000.0          13000.0\n",
       "1645212    20000.0      20000.0          20000.0\n",
       "1645214    12850.0      12850.0          12850.0\n",
       "1645215    24000.0      24000.0          24000.0\n",
       "1645217    33600.0      33600.0          33600.0\n",
       "1645218     2100.0       2100.0           2100.0\n",
       "1645220    20450.0      20450.0          20450.0\n",
       "1645221    20000.0      20000.0          20000.0\n",
       "1645222    28000.0      28000.0          28000.0\n",
       "1645224     3500.0       3500.0           3500.0\n",
       "1645226     6000.0       6000.0           6000.0\n",
       "1645227    24500.0      24500.0          24500.0\n",
       "1645228    12950.0      12950.0          12950.0\n",
       "1645229    10000.0      10000.0          10000.0\n",
       "1645230    15000.0      15000.0          15000.0\n",
       "1645232     8400.0       8400.0           8400.0\n",
       "1645233    15000.0      15000.0          15000.0\n",
       "1645234    12000.0      12000.0          12000.0\n",
       "1645236    19800.0      19800.0          19800.0\n",
       "1645237     9000.0       9000.0           9000.0\n",
       "1645238    11000.0      11000.0          11000.0\n",
       "1645239     5000.0       5000.0           5000.0\n",
       "1645241     7825.0       7825.0           7825.0\n",
       "1645242     4800.0       4800.0           4800.0\n",
       "1645243    10000.0      10000.0          10000.0\n",
       "1645245    12800.0      12800.0          12800.0\n",
       "1645246    35000.0      35000.0          35000.0\n",
       "1645247    15000.0      15000.0          15000.0\n",
       "1645248    21250.0      21250.0          21200.0\n",
       "1645249     8000.0       8000.0           8000.0\n",
       "1645250    12000.0      12000.0          12000.0\n",
       "1645251     8000.0       8000.0           8000.0\n",
       "1645252     5000.0       5000.0           5000.0\n",
       "1645253     5150.0       5150.0           5150.0\n",
       "1645254     2400.0       2400.0           2400.0\n",
       "1645255    30000.0      30000.0          30000.0\n",
       "1645256    35000.0      35000.0          35000.0\n",
       "1645258     4000.0       4000.0           4000.0\n",
       "1645259    28000.0      28000.0          28000.0\n",
       "1645260     8000.0       8000.0           8000.0\n",
       "1645261     2000.0       2000.0           2000.0\n",
       "1645262     9600.0       9600.0           9600.0\n",
       "1645264    10000.0      10000.0          10000.0\n",
       "1645265    28000.0      28000.0          28000.0\n",
       "1645267    13000.0      13000.0          13000.0\n",
       "1645268    25000.0      25000.0          24950.0\n",
       "1645269    15000.0      15000.0          15000.0\n",
       "1645270     4000.0       4000.0           4000.0\n",
       "1645271    35000.0      35000.0          35000.0\n",
       "1645272    10000.0      10000.0          10000.0\n",
       "1645273    35000.0      35000.0          35000.0\n",
       "1645274    35000.0      35000.0          35000.0\n",
       "1645275     8000.0       8000.0           8000.0\n",
       "1645277    14000.0      14000.0          14000.0\n",
       "1645278    21000.0      21000.0          21000.0\n",
       "1645281    32000.0      32000.0          32000.0\n",
       "1645282    19075.0      19075.0          19075.0\n",
       "1645284    15000.0      15000.0          15000.0\n",
       "1645285     2400.0       2400.0           2400.0\n",
       "1645287    35000.0      35000.0          35000.0\n",
       "1645288    16000.0      16000.0          16000.0\n",
       "1645289    11000.0      11000.0          11000.0\n",
       "1645290    13000.0      13000.0          13000.0\n",
       "1645293    28000.0      28000.0          28000.0\n",
       "1645294    20000.0      20000.0          20000.0\n",
       "1645295    35000.0      35000.0          35000.0\n",
       "1645298    10000.0      10000.0          10000.0\n",
       "1645299    20000.0      20000.0          20000.0\n",
       "1645300    18000.0      18000.0          18000.0\n",
       "1645301    11250.0      11250.0          11250.0\n",
       "1645303    19600.0      19600.0          19600.0\n",
       "1645307    30000.0      30000.0          30000.0\n",
       "1645308    14400.0      14400.0          14400.0\n",
       "1645309    29475.0      29475.0          29475.0\n",
       "1645310     8500.0       8500.0           8500.0\n",
       "1645312    32000.0      32000.0          32000.0\n",
       "1645313    17000.0      17000.0          17000.0\n",
       "1645314    35000.0      35000.0          35000.0\n",
       "1645315    20000.0      20000.0          20000.0\n",
       "1645316    20000.0      20000.0          20000.0\n",
       "1645317    18000.0      18000.0          18000.0\n",
       "1645319    15000.0      15000.0          15000.0\n",
       "1645320    14000.0      14000.0          14000.0\n",
       "1645324     7150.0       7150.0           7150.0\n",
       "1645325    30000.0      30000.0          30000.0\n",
       "1645326    30000.0      30000.0          30000.0\n",
       "1645327    10000.0      10000.0          10000.0\n",
       "1645328    10000.0      10000.0          10000.0\n",
       "1645329    30000.0      30000.0          30000.0\n",
       "1645331    35000.0      35000.0          35000.0\n",
       "1645332    24000.0      24000.0          24000.0\n",
       "1645333    26000.0      26000.0          26000.0\n",
       "1645335    14475.0      14475.0          14475.0\n",
       "1645336    28000.0      28000.0          28000.0\n",
       "1645337     4800.0       4800.0           4800.0\n",
       "1645338    20000.0      20000.0          20000.0\n",
       "1645339    12000.0      12000.0          12000.0\n",
       "1645340    16000.0      16000.0          16000.0\n",
       "1645341    30000.0      30000.0          30000.0\n",
       "1645342    18000.0      18000.0          18000.0\n",
       "1645343    16000.0      16000.0          16000.0\n",
       "1645345    15000.0      15000.0          15000.0\n",
       "1645347    13000.0      13000.0          13000.0\n",
       "1645348    30000.0      30000.0          30000.0\n",
       "1645351     8550.0       8550.0           8550.0\n",
       "1645352    24375.0      24375.0          24350.0\n",
       "1645353    10200.0      10200.0          10200.0\n",
       "1645354    25000.0      25000.0          25000.0\n",
       "1645356    30000.0      30000.0          30000.0\n",
       "1645358    35000.0      35000.0          35000.0\n",
       "1645359    10000.0      10000.0          10000.0\n",
       "1645361     2500.0       2500.0           2500.0\n",
       "1645363    10000.0      10000.0          10000.0\n",
       "1645364    30000.0      30000.0          30000.0\n",
       "1645365    10000.0      10000.0          10000.0\n",
       "1645366    20000.0      20000.0          20000.0\n",
       "1645367     8000.0       8000.0           8000.0\n",
       "1645368    24000.0      24000.0          24000.0\n",
       "1645369    35000.0      35000.0          35000.0\n",
       "1645370    15400.0      15400.0          15400.0\n",
       "1645373    35000.0      35000.0          35000.0\n",
       "1645374    12000.0      12000.0          12000.0\n",
       "1645375    12050.0      12050.0          11900.0\n",
       "1645377    11000.0      11000.0          11000.0\n",
       "1645378    13600.0      13600.0          13600.0\n",
       "1645379    15000.0      15000.0          14850.0\n",
       "1645380     8000.0       8000.0           8000.0\n",
       "1645381    13200.0      13200.0          13200.0\n",
       "1645382    16800.0      16800.0          16800.0\n",
       "1645383    10000.0      10000.0          10000.0\n",
       "1645384     5000.0       5000.0           5000.0\n",
       "1645387    12000.0      12000.0          12000.0\n",
       "1645388     6200.0       6200.0           6200.0\n",
       "1645389    15000.0      15000.0          15000.0\n",
       "1645390    16000.0      16000.0          16000.0\n",
       "1645391    19000.0      19000.0          19000.0\n",
       "1645393     5600.0       5600.0           5600.0\n",
       "1645394     4750.0       4750.0           4750.0\n",
       "1645395    19000.0      19000.0          19000.0\n",
       "1645396    12000.0      12000.0          12000.0\n",
       "1645397    10000.0      10000.0          10000.0\n",
       "1645398     3825.0       3825.0           3825.0\n",
       "1645399    32000.0      32000.0          32000.0\n",
       "1645400    15700.0      15700.0          15700.0\n",
       "1645401    24000.0      24000.0          24000.0\n",
       "1645402     7500.0       7500.0           7500.0\n",
       "1645404    17325.0      17325.0          17325.0\n",
       "1645405    20000.0      20000.0          20000.0\n",
       "1645406    28000.0      28000.0          27950.0\n",
       "1645407     5000.0       5000.0           4850.0\n",
       "1645408     5000.0       5000.0           5000.0\n",
       "1645409    10975.0      10975.0          10975.0\n",
       "1645412     6000.0       6000.0           6000.0\n",
       "1645413    35000.0      35000.0          35000.0\n",
       "1645414    24000.0      24000.0          24000.0\n",
       "1645417     4500.0       4500.0           4500.0\n",
       "1645418    10000.0      10000.0          10000.0\n",
       "1645420     8000.0       8000.0           8000.0\n",
       "1645423    35000.0      35000.0          35000.0\n",
       "1645424    16600.0      16600.0          16600.0\n",
       "1645425     8000.0       8000.0           8000.0\n",
       "1645426     5600.0       5600.0           5600.0\n",
       "1645429     5025.0       5025.0           4875.0\n",
       "1645430    11050.0      11050.0          10900.0\n",
       "1645431    20000.0      20000.0          20000.0\n",
       "1645432    20000.0      20000.0          20000.0\n",
       "1645433    35000.0      35000.0          35000.0\n",
       "1645434     8000.0       8000.0           8000.0\n",
       "1645435    16000.0      16000.0          16000.0\n",
       "1645436    18000.0      18000.0          18000.0\n",
       "1645437    25000.0      25000.0          25000.0\n",
       "1645438    12950.0      12950.0          12800.0\n",
       "1645439    10000.0      10000.0          10000.0\n",
       "1645440    30000.0      30000.0          30000.0\n",
       "1645441    24000.0      24000.0          24000.0\n",
       "1645442    20000.0      20000.0          20000.0\n",
       "1645444    24000.0      24000.0          24000.0\n",
       "1645445     3600.0       3600.0           3600.0\n",
       "1645446    16000.0      16000.0          16000.0\n",
       "1645447    12000.0      12000.0          12000.0\n",
       "1645448     8400.0       8400.0           8400.0\n",
       "1645451     9500.0       9500.0           9500.0\n",
       "1645453    12000.0      12000.0          12000.0\n",
       "1645454     7225.0       7225.0           7225.0\n",
       "1645455    16000.0      16000.0          16000.0\n",
       "1645456    24000.0      24000.0          24000.0\n",
       "1645457    19300.0      19300.0          19300.0\n",
       "1645458    10575.0      10575.0          10575.0\n",
       "1645459    16800.0      16800.0          16800.0\n",
       "1645460    13200.0      13200.0          13175.0\n",
       "1645461    35000.0      35000.0          35000.0\n",
       "1645462    12800.0      12800.0          12800.0\n",
       "1645463    12000.0      12000.0          12000.0\n",
       "1645464    19000.0      19000.0          19000.0\n",
       "1645466    30000.0      30000.0          29850.0\n",
       "1645470    35000.0      35000.0          35000.0\n",
       "1645471    24700.0      24700.0          24700.0\n",
       "1645472    15000.0      15000.0          15000.0\n",
       "1645474    30000.0      30000.0          30000.0\n",
       "1645476    24000.0      24000.0          24000.0\n",
       "1645478     7625.0       7625.0           7625.0\n",
       "1645479    24000.0      24000.0          24000.0\n",
       "1645480    10975.0      10975.0          10825.0\n",
       "1645481     3975.0       3975.0           3975.0\n",
       "1645485     5200.0       5200.0           5200.0\n",
       "1645487    15000.0      15000.0          15000.0\n",
       "1645488    19000.0      19000.0          18750.0\n",
       "1645489    10000.0      10000.0          10000.0\n",
       "1645490    11000.0      11000.0          11000.0\n",
       "1645492     4500.0       4500.0           4500.0\n",
       "1645493     7500.0       7500.0           7500.0\n",
       "1645495    34000.0      34000.0          34000.0\n",
       "1645496     8500.0       8500.0           8500.0\n",
       "1645497    25000.0      25000.0          24800.0\n",
       "1645499    12600.0      12600.0          12600.0\n",
       "1645501     5000.0       5000.0           5000.0\n",
       "1645502     5000.0       5000.0           5000.0\n",
       "1645504    24000.0      24000.0          24000.0\n",
       "1645507     6000.0       6000.0           6000.0\n",
       "1645508    24100.0      24100.0          24100.0\n",
       "1645509     4000.0       4000.0           4000.0\n",
       "1645514    34050.0      34050.0          34050.0\n",
       "1645515     5000.0       5000.0           5000.0\n",
       "\n",
       "[1285998 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are all the same. \n",
    "# lc[['loan_amnt', 'funded_amnt', 'funded_amnt_inv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop the duplicates.\n",
    "del lc['funded_amnt']\n",
    "del lc['funded_amnt_inv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    C1\n",
       "1    A3\n",
       "2    D1\n",
       "3    D4\n",
       "5    C3\n",
       "Name: sub_grade, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Break these out\n",
    "lc['sub_grade'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lc['sub_grade'] = lc['sub_grade'].map(lambda x: int(x.lstrip('ABCDEFG')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w    822678\n",
       "f    463320\n",
       "Name: initial_list_status, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.initial_list_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'loan_amnt', 'term', 'int_rate', 'installment', 'grade',\n",
       "       'sub_grade', 'home_ownership', 'annual_inc', 'verification_status',\n",
       "       'issue_d', 'loan_status', 'purpose', 'zip_code', 'addr_state', 'dti',\n",
       "       'delinq_2yrs', 'earliest_cr_line', 'inq_last_6mths', 'open_acc',\n",
       "       'pub_rec', 'revol_bal', 'revol_util', 'total_acc',\n",
       "       'initial_list_status', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n",
       "       'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int',\n",
       "       'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
       "       'last_pymnt_amnt', 'last_credit_pull_d', 'last_fico_range_high',\n",
       "       'last_fico_range_low', 'collections_12_mths_ex_med', 'application_type',\n",
       "       'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim',\n",
       "       'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util',\n",
       "       'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_rev_tl_op',\n",
       "       'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc',\n",
       "       'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl',\n",
       "       'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl',\n",
       "       'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m',\n",
       "       'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m',\n",
       "       'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies',\n",
       "       'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit',\n",
       "       'total_il_high_credit_limit', 'disbursement_method',\n",
       "       'debt_settlement_flag', 'sec_app_flag', 'sec_app_fico_best',\n",
       "       'annual_inc_final', 'earliest_cr_line_months', 'emp_length_floats',\n",
       "       'average_fico', 'dti_final', 'mths_since_recent_bc_cat',\n",
       "       'mo_sin_old_il_acct_cat', 'mths_since_recent_inq_cat',\n",
       "       'mths_since_last_delinq_cat', 'mths_since_rcnt_il_cat',\n",
       "       'mths_since_recent_revol_delinq_cat', 'mths_since_last_major_derog_cat',\n",
       "       'mths_since_recent_bc_dlq_cat', 'mths_since_last_record_cat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(lc, open('D:/ML Case Study/lending-club/lc_continuous_1.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lc1 = pickle.load(open('D:/ML Case Study/lending-club/lc_continuous_1.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determine if variables are continuous or categorical:\n",
    "\n",
    "categorical = []\n",
    "continuous = []\n",
    "\n",
    "for col in lc1.columns.tolist():\n",
    "    try:\n",
    "        lc1[col].astype(float)\n",
    "    except:\n",
    "        categorical.append(col)\n",
    "        \n",
    "continuous = [x for x in lc1.columns.tolist() if x not in categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grade', 'home_ownership', 'verification_status', 'issue_d', 'loan_status', 'purpose', 'zip_code', 'addr_state', 'initial_list_status', 'application_type', 'disbursement_method', 'debt_settlement_flag', 'mths_since_recent_bc_cat', 'mo_sin_old_il_acct_cat', 'mths_since_recent_inq_cat', 'mths_since_last_delinq_cat', 'mths_since_rcnt_il_cat', 'mths_since_recent_revol_delinq_cat', 'mths_since_last_major_derog_cat', 'mths_since_recent_bc_dlq_cat', 'mths_since_last_record_cat']\n"
     ]
    }
   ],
   "source": [
    "# get continuous variables that should be categorical\n",
    "print(categorical)\n",
    "# remove dates from categorical\n",
    "#not_continuous = ['id','last_credit_pull_month', 'last_credit_pull_year', 'earliest_cr_line_year', 'earliest_cr_line_month', 'term', 'sec_app_flag']\n",
    "\n",
    "#\n",
    "not_continuous = ['id', 'sec_app_flag','last_credit_pull_month', 'last_credit_pull_year', 'earliest_cr_line_year',\n",
    "                  'earliest_cr_line_month', 'issue_month', 'issue_year', 'term', 'sub_grade']\n",
    "dates = ['last_credit_pull_month', 'last_credit_pull_year', 'earliest_cr_line_year', 'earliest_cr_line_month', 'issue_month', 'issue_year']\n",
    "\n",
    "#Extract the Y variable and set it aside. \n",
    "int_rates = lc1[['int_rate']]\n",
    "# Remove dates from categorical listing\n",
    "categorical = [x for x in categorical if x not in dates]\n",
    "\n",
    "# remove categorical from continuos listing\n",
    "continuous = [x for x in continuous if x not in not_continuous]\n",
    "continuous = [x for x in continuous if x not in ['int_rate', 'total_rec_int']]\n",
    "# add categorical variables misclassified as continuous\n",
    "categorical.extend(not_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Variables:\n",
      "['grade', 'home_ownership', 'verification_status', 'issue_d', 'loan_status', 'purpose', 'zip_code', 'addr_state', 'initial_list_status', 'application_type', 'disbursement_method', 'debt_settlement_flag', 'mths_since_recent_bc_cat', 'mo_sin_old_il_acct_cat', 'mths_since_recent_inq_cat', 'mths_since_last_delinq_cat', 'mths_since_rcnt_il_cat', 'mths_since_recent_revol_delinq_cat', 'mths_since_last_major_derog_cat', 'mths_since_recent_bc_dlq_cat', 'mths_since_last_record_cat', 'id', 'sec_app_flag', 'last_credit_pull_month', 'last_credit_pull_year', 'earliest_cr_line_year', 'earliest_cr_line_month', 'issue_month', 'issue_year', 'term', 'sub_grade']\n",
      "---------------\n",
      "\n",
      "Continuous Variables\n",
      "['loan_amnt', 'installment', 'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_amnt', 'last_fico_range_high', 'last_fico_range_low', 'collections_12_mths_ex_med', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim', 'acc_open_past_24mths', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit', 'sec_app_fico_best', 'annual_inc_final', 'earliest_cr_line_months', 'emp_length_floats', 'average_fico', 'dti_final']\n"
     ]
    }
   ],
   "source": [
    "print('Categorical Variables:')\n",
    "print(categorical)\n",
    "print('---------------\\n')\n",
    "print('Continuous Variables')\n",
    "print(continuous)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# continuous = ['loan_amnt', 'installment', 'annual_inc', 'dti', 'inq_last_6mths', 'revol_bal', 'revol_util', 'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',\n",
    "# 'total_rec_int', 'last_fico_range_high', 'annual_inc_final', 'earliest_cr_line_months', 'emp_length_floats', 'average_fico', 'dti_final', 'chargeoff_within_12_mths',\n",
    "#              'last_pymnt_amnt', 'num_il_tl' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorical = ['grade', 'loan_status', 'addr_state', 'issue_d', 'initial_list_status', 'last_credit_pull_year', 'verification_status', 'sub_grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# out_prncp, out_prncp_inv, acc_now_delinq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find categorical variables with excessive number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_cat = ['id','title','emp_title','zip_code', 'issue_month', 'issue_year', 'last_credit_pull_d', 'int_rate'] #'issue_d' for example\n",
    "# Remove categories with an excessive number of unique categories from the categorical listing\n",
    "categorical = [x for x in categorical if x not in remove_cat]\n",
    "continuous  = [i for i in continuous if i not in remove_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grade  :  7\n",
      "home_ownership  :  6\n",
      "verification_status  :  3\n",
      "issue_d  :  62\n",
      "loan_status  :  7\n",
      "purpose  :  14\n",
      "addr_state  :  51\n",
      "initial_list_status  :  2\n",
      "application_type  :  2\n",
      "disbursement_method  :  2\n",
      "debt_settlement_flag  :  2\n",
      "mths_since_recent_bc_cat  :  3\n",
      "mo_sin_old_il_acct_cat  :  4\n",
      "mths_since_recent_inq_cat  :  4\n",
      "mths_since_last_delinq_cat  :  4\n",
      "mths_since_rcnt_il_cat  :  3\n",
      "mths_since_recent_revol_delinq_cat  :  4\n",
      "mths_since_last_major_derog_cat  :  4\n",
      "mths_since_recent_bc_dlq_cat  :  4\n",
      "mths_since_last_record_cat  :  4\n",
      "sec_app_flag  :  2\n",
      "last_credit_pull_month  :  12\n",
      "last_credit_pull_year  :  6\n",
      "earliest_cr_line_year  :  72\n",
      "earliest_cr_line_month  :  12\n",
      "term  :  2\n",
      "sub_grade  :  5\n"
     ]
    }
   ],
   "source": [
    "for cat in categorical:\n",
    "    print(cat ,' : ', lc1[cat].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical\n",
    "# continuous\n",
    "[print(x) for x in continuous if x in ['int_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lc1 = lc1[continuous+categorical+['int_rate']]\n",
    "lc = lc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lc = lc1[['grade', 'sub_grade', 'issue_d', 'int_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>out_prncp</th>\n",
       "      <th>out_prncp_inv</th>\n",
       "      <th>total_pymnt</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <th>total_rec_late_fee</th>\n",
       "      <th>recoveries</th>\n",
       "      <th>collection_recovery_fee</th>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <th>last_fico_range_high</th>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>tot_coll_amt</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>mo_sin_old_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>num_accts_ever_120_pd</th>\n",
       "      <th>num_actv_bc_tl</th>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <th>num_bc_sats</th>\n",
       "      <th>num_bc_tl</th>\n",
       "      <th>num_il_tl</th>\n",
       "      <th>num_op_rev_tl</th>\n",
       "      <th>num_rev_accts</th>\n",
       "      <th>num_rev_tl_bal_gt_0</th>\n",
       "      <th>num_sats</th>\n",
       "      <th>num_tl_120dpd_2m</th>\n",
       "      <th>num_tl_30dpd</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>sec_app_fico_best</th>\n",
       "      <th>annual_inc_final</th>\n",
       "      <th>earliest_cr_line_months</th>\n",
       "      <th>emp_length_floats</th>\n",
       "      <th>average_fico</th>\n",
       "      <th>dti_final</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>application_type</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>mths_since_recent_bc_cat</th>\n",
       "      <th>mo_sin_old_il_acct_cat</th>\n",
       "      <th>mths_since_recent_inq_cat</th>\n",
       "      <th>mths_since_last_delinq_cat</th>\n",
       "      <th>mths_since_rcnt_il_cat</th>\n",
       "      <th>mths_since_recent_revol_delinq_cat</th>\n",
       "      <th>mths_since_last_major_derog_cat</th>\n",
       "      <th>mths_since_recent_bc_dlq_cat</th>\n",
       "      <th>mths_since_last_record_cat</th>\n",
       "      <th>sec_app_flag</th>\n",
       "      <th>last_credit_pull_month</th>\n",
       "      <th>last_credit_pull_year</th>\n",
       "      <th>earliest_cr_line_year</th>\n",
       "      <th>earliest_cr_line_month</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>336.64</td>\n",
       "      <td>78000.0</td>\n",
       "      <td>12.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138008.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17392.37</td>\n",
       "      <td>17392.37</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12017.81</td>\n",
       "      <td>684.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149140.0</td>\n",
       "      <td>184500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29828.0</td>\n",
       "      <td>9525.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>196500.0</td>\n",
       "      <td>149140.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>78000.0</td>\n",
       "      <td>278</td>\n",
       "      <td>10.0</td>\n",
       "      <td>752.0</td>\n",
       "      <td>12.03</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Dec-2014</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>VA</td>\n",
       "      <td>w</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>0-150</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2017</td>\n",
       "      <td>1994</td>\n",
       "      <td>08</td>\n",
       "      <td>60</td>\n",
       "      <td>12.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10400.0</td>\n",
       "      <td>321.08</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>14.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6133.0</td>\n",
       "      <td>31.6</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6611.69</td>\n",
       "      <td>6611.69</td>\n",
       "      <td>5217.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>521.27</td>\n",
       "      <td>93.8286</td>\n",
       "      <td>321.08</td>\n",
       "      <td>564.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162110.0</td>\n",
       "      <td>19400.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9536.0</td>\n",
       "      <td>7599.0</td>\n",
       "      <td>41.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83.3</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179407.0</td>\n",
       "      <td>15030.0</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>11325.0</td>\n",
       "      <td>0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>337</td>\n",
       "      <td>8.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>14.92</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Dec-2014</td>\n",
       "      <td>Charged Off</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>CA</td>\n",
       "      <td>w</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>0-150</td>\n",
       "      <td>0-5</td>\n",
       "      <td>0-50</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0-50</td>\n",
       "      <td>50-75</td>\n",
       "      <td>0-50</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>1989</td>\n",
       "      <td>09</td>\n",
       "      <td>36</td>\n",
       "      <td>6.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21425.0</td>\n",
       "      <td>516.36</td>\n",
       "      <td>63800.0</td>\n",
       "      <td>18.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16374.0</td>\n",
       "      <td>76.2</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25512.20</td>\n",
       "      <td>25512.20</td>\n",
       "      <td>21425.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>17813.19</td>\n",
       "      <td>704.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42315.0</td>\n",
       "      <td>21500.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4232.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>97.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>91.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57073.0</td>\n",
       "      <td>42315.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>35573.0</td>\n",
       "      <td>0</td>\n",
       "      <td>63800.0</td>\n",
       "      <td>170</td>\n",
       "      <td>6.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>18.49</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Dec-2014</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>MO</td>\n",
       "      <td>w</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>0-150</td>\n",
       "      <td>5-15</td>\n",
       "      <td>50-75</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>50-75</td>\n",
       "      <td>50-75</td>\n",
       "      <td>50-75</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>2003</td>\n",
       "      <td>08</td>\n",
       "      <td>60</td>\n",
       "      <td>15.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12800.0</td>\n",
       "      <td>319.08</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>8.31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5753.0</td>\n",
       "      <td>100.9</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6673.54</td>\n",
       "      <td>6673.54</td>\n",
       "      <td>11207.67</td>\n",
       "      <td>11207.67</td>\n",
       "      <td>6126.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>319.08</td>\n",
       "      <td>629.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>261815.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32727.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>368700.0</td>\n",
       "      <td>18007.0</td>\n",
       "      <td>4400.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>204</td>\n",
       "      <td>10.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>8.31</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Dec-2014</td>\n",
       "      <td>Current</td>\n",
       "      <td>car</td>\n",
       "      <td>CA</td>\n",
       "      <td>w</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>0-150</td>\n",
       "      <td>0-5</td>\n",
       "      <td>0-50</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0-50</td>\n",
       "      <td>0-50</td>\n",
       "      <td>0-50</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>2000</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>17.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9600.0</td>\n",
       "      <td>326.53</td>\n",
       "      <td>69000.0</td>\n",
       "      <td>25.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16388.0</td>\n",
       "      <td>59.4</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9973.43</td>\n",
       "      <td>9973.43</td>\n",
       "      <td>9600.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9338.58</td>\n",
       "      <td>724.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38566.0</td>\n",
       "      <td>27600.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3214.0</td>\n",
       "      <td>6494.0</td>\n",
       "      <td>69.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52490.0</td>\n",
       "      <td>38566.0</td>\n",
       "      <td>21100.0</td>\n",
       "      <td>24890.0</td>\n",
       "      <td>0</td>\n",
       "      <td>69000.0</td>\n",
       "      <td>299</td>\n",
       "      <td>10.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>25.81</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Dec-2014</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>NJ</td>\n",
       "      <td>f</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>150-200</td>\n",
       "      <td>15+</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>1992</td>\n",
       "      <td>11</td>\n",
       "      <td>36</td>\n",
       "      <td>13.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>351.40</td>\n",
       "      <td>109777.0</td>\n",
       "      <td>11.63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7253.0</td>\n",
       "      <td>60.4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18586.72</td>\n",
       "      <td>18586.72</td>\n",
       "      <td>16000.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4935.54</td>\n",
       "      <td>669.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>373743.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>53392.0</td>\n",
       "      <td>2559.0</td>\n",
       "      <td>72.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.4</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>428032.0</td>\n",
       "      <td>34475.0</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>33000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>109777.0</td>\n",
       "      <td>167</td>\n",
       "      <td>6.0</td>\n",
       "      <td>702.0</td>\n",
       "      <td>11.63</td>\n",
       "      <td>OWN</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Dec-2014</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>VA</td>\n",
       "      <td>w</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>50+</td>\n",
       "      <td>0-150</td>\n",
       "      <td>15+</td>\n",
       "      <td>0-50</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0-50</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0-50</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "      <td>2003</td>\n",
       "      <td>11</td>\n",
       "      <td>60</td>\n",
       "      <td>11.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23325.0</td>\n",
       "      <td>800.71</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>27.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32372.0</td>\n",
       "      <td>82.2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13907.79</td>\n",
       "      <td>13907.79</td>\n",
       "      <td>7301.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3544.92</td>\n",
       "      <td>638.0856</td>\n",
       "      <td>800.71</td>\n",
       "      <td>499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>393558.0</td>\n",
       "      <td>39400.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39356.0</td>\n",
       "      <td>3977.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.7</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>443889.0</td>\n",
       "      <td>79490.0</td>\n",
       "      <td>36000.0</td>\n",
       "      <td>78126.0</td>\n",
       "      <td>0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>228</td>\n",
       "      <td>10.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>27.03</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Dec-2014</td>\n",
       "      <td>Charged Off</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>WA</td>\n",
       "      <td>f</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>0-150</td>\n",
       "      <td>15+</td>\n",
       "      <td>0-50</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0-50</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>14.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5250.0</td>\n",
       "      <td>172.98</td>\n",
       "      <td>26000.0</td>\n",
       "      <td>14.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4448.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5431.68</td>\n",
       "      <td>5431.68</td>\n",
       "      <td>5250.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>43.40</td>\n",
       "      <td>764.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10133.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1267.0</td>\n",
       "      <td>12152.0</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29971.0</td>\n",
       "      <td>10133.0</td>\n",
       "      <td>16600.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>26000.0</td>\n",
       "      <td>101</td>\n",
       "      <td>2.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>14.36</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Dec-2014</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>MD</td>\n",
       "      <td>f</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>0-150</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>2009</td>\n",
       "      <td>05</td>\n",
       "      <td>36</td>\n",
       "      <td>11.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>332.10</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23723.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6957.45</td>\n",
       "      <td>6957.45</td>\n",
       "      <td>5395.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>332.10</td>\n",
       "      <td>499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23723.0</td>\n",
       "      <td>24200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4745.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>98.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24200.0</td>\n",
       "      <td>23723.0</td>\n",
       "      <td>21200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>171</td>\n",
       "      <td>8.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>8.44</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Dec-2014</td>\n",
       "      <td>Charged Off</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>MI</td>\n",
       "      <td>f</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>0-150</td>\n",
       "      <td>0-5</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>2003</td>\n",
       "      <td>07</td>\n",
       "      <td>36</td>\n",
       "      <td>11.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12975.0</td>\n",
       "      <td>468.17</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>22.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>33.1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5746.89</td>\n",
       "      <td>5746.89</td>\n",
       "      <td>2628.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1515.55</td>\n",
       "      <td>272.7990</td>\n",
       "      <td>50.00</td>\n",
       "      <td>664.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>17281.0</td>\n",
       "      <td>15700.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1571.0</td>\n",
       "      <td>2604.0</td>\n",
       "      <td>52.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>89.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42943.0</td>\n",
       "      <td>17281.0</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>27243.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>225</td>\n",
       "      <td>10.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>22.42</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Dec-2014</td>\n",
       "      <td>Charged Off</td>\n",
       "      <td>house</td>\n",
       "      <td>FL</td>\n",
       "      <td>f</td>\n",
       "      <td>Individual</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>150-200</td>\n",
       "      <td>0-5</td>\n",
       "      <td>0-50</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>Not Reported</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "      <td>1999</td>\n",
       "      <td>01</td>\n",
       "      <td>36</td>\n",
       "      <td>17.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    loan_amnt  installment  annual_inc    dti  delinq_2yrs  inq_last_6mths  \\\n",
       "0     15000.0       336.64     78000.0  12.03          0.0             0.0   \n",
       "1     10400.0       321.08     58000.0  14.92          0.0             2.0   \n",
       "2     21425.0       516.36     63800.0  18.49          0.0             0.0   \n",
       "3     12800.0       319.08    125000.0   8.31          1.0             0.0   \n",
       "5      9600.0       326.53     69000.0  25.81          0.0             0.0   \n",
       "7     16000.0       351.40    109777.0  11.63          1.0             0.0   \n",
       "8     23325.0       800.71     72000.0  27.03          1.0             0.0   \n",
       "9      5250.0       172.98     26000.0  14.36          0.0             0.0   \n",
       "10    10000.0       332.10     90000.0   8.44          0.0             1.0   \n",
       "11    12975.0       468.17     60000.0  22.42          0.0             0.0   \n",
       "\n",
       "    open_acc  pub_rec  revol_bal  revol_util  total_acc  out_prncp  \\\n",
       "0        6.0      0.0   138008.0        29.0       17.0       0.00   \n",
       "1       17.0      0.0     6133.0        31.6       36.0       0.00   \n",
       "2       10.0      0.0    16374.0        76.2       35.0       0.00   \n",
       "3        8.0      0.0     5753.0       100.9       13.0    6673.54   \n",
       "5       12.0      0.0    16388.0        59.4       44.0       0.00   \n",
       "7        7.0      0.0     7253.0        60.4       14.0       0.00   \n",
       "8       14.0      0.0    32372.0        82.2       23.0       0.00   \n",
       "9        8.0      0.0     4448.0        20.2       13.0       0.00   \n",
       "10       5.0      0.0    23723.0        98.0       10.0       0.00   \n",
       "11      11.0      0.0     5200.0        33.1       19.0       0.00   \n",
       "\n",
       "    out_prncp_inv  total_pymnt  total_pymnt_inv  total_rec_prncp  \\\n",
       "0            0.00     17392.37         17392.37         15000.00   \n",
       "1            0.00      6611.69          6611.69          5217.75   \n",
       "2            0.00     25512.20         25512.20         21425.00   \n",
       "3         6673.54     11207.67         11207.67          6126.46   \n",
       "5            0.00      9973.43          9973.43          9600.00   \n",
       "7            0.00     18586.72         18586.72         16000.00   \n",
       "8            0.00     13907.79         13907.79          7301.23   \n",
       "9            0.00      5431.68          5431.68          5250.00   \n",
       "10           0.00      6957.45          6957.45          5395.29   \n",
       "11           0.00      5746.89          5746.89          2628.14   \n",
       "\n",
       "    total_rec_late_fee  recoveries  collection_recovery_fee  last_pymnt_amnt  \\\n",
       "0                  0.0        0.00                   0.0000         12017.81   \n",
       "1                  0.0      521.27                  93.8286           321.08   \n",
       "2                  0.0        0.00                   0.0000         17813.19   \n",
       "3                  0.0        0.00                   0.0000           319.08   \n",
       "5                  0.0        0.00                   0.0000          9338.58   \n",
       "7                  0.0        0.00                   0.0000          4935.54   \n",
       "8                  0.0     3544.92                 638.0856           800.71   \n",
       "9                  0.0        0.00                   0.0000            43.40   \n",
       "10                 0.0        0.00                   0.0000           332.10   \n",
       "11                 0.0     1515.55                 272.7990            50.00   \n",
       "\n",
       "    last_fico_range_high  last_fico_range_low  collections_12_mths_ex_med  \\\n",
       "0                  684.0                680.0                         0.0   \n",
       "1                  564.0                560.0                         0.0   \n",
       "2                  704.0                700.0                         0.0   \n",
       "3                  629.0                625.0                         0.0   \n",
       "5                  724.0                720.0                         0.0   \n",
       "7                  669.0                665.0                         0.0   \n",
       "8                  499.0                  0.0                         0.0   \n",
       "9                  764.0                760.0                         0.0   \n",
       "10                 499.0                  0.0                         0.0   \n",
       "11                 664.0                660.0                         0.0   \n",
       "\n",
       "    acc_now_delinq  tot_coll_amt  tot_cur_bal  total_rev_hi_lim  \\\n",
       "0              0.0           0.0     149140.0          184500.0   \n",
       "1              0.0           0.0     162110.0           19400.0   \n",
       "2              0.0           0.0      42315.0           21500.0   \n",
       "3              0.0           0.0     261815.0            5700.0   \n",
       "5              0.0           0.0      38566.0           27600.0   \n",
       "7              0.0           0.0     373743.0           12000.0   \n",
       "8              0.0           0.0     393558.0           39400.0   \n",
       "9              0.0           0.0      10133.0           22000.0   \n",
       "10             0.0           0.0      23723.0           24200.0   \n",
       "11             0.0         900.0      17281.0           15700.0   \n",
       "\n",
       "    acc_open_past_24mths  avg_cur_bal  bc_open_to_buy  bc_util  \\\n",
       "0                    5.0      29828.0          9525.0      4.7   \n",
       "1                    7.0       9536.0          7599.0     41.5   \n",
       "2                    4.0       4232.0           324.0     97.8   \n",
       "3                    2.0      32727.0             0.0    103.2   \n",
       "5                    8.0       3214.0          6494.0     69.2   \n",
       "7                    3.0      53392.0          2559.0     72.2   \n",
       "8                    6.0      39356.0          3977.0     89.0   \n",
       "9                    4.0       1267.0         12152.0     26.8   \n",
       "10                   0.0       4745.0           324.0     98.5   \n",
       "11                   7.0       1571.0          2604.0     52.7   \n",
       "\n",
       "    chargeoff_within_12_mths  delinq_amnt  mo_sin_old_rev_tl_op  \\\n",
       "0                        0.0          0.0                 244.0   \n",
       "1                        0.0          0.0                 290.0   \n",
       "2                        0.0          0.0                 136.0   \n",
       "3                        0.0          0.0                 170.0   \n",
       "5                        0.0          0.0                 265.0   \n",
       "7                        0.0          0.0                 133.0   \n",
       "8                        0.0          0.0                 194.0   \n",
       "9                        0.0          0.0                  67.0   \n",
       "10                       0.0          0.0                 124.0   \n",
       "11                       0.0          0.0                 122.0   \n",
       "\n",
       "    mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  num_accts_ever_120_pd  \\\n",
       "0                     1.0             1.0       0.0                    0.0   \n",
       "1                     1.0             1.0       1.0                    4.0   \n",
       "2                     7.0             7.0       0.0                    1.0   \n",
       "3                    21.0            16.0       5.0                    1.0   \n",
       "5                    23.0             3.0       0.0                    0.0   \n",
       "7                    17.0            17.0       2.0                    0.0   \n",
       "8                    15.0            12.0       6.0                    0.0   \n",
       "9                    12.0             6.0       0.0                    0.0   \n",
       "10                   40.0            31.0       0.0                    0.0   \n",
       "11                    2.0             2.0       0.0                    0.0   \n",
       "\n",
       "    num_actv_bc_tl  num_actv_rev_tl  num_bc_sats  num_bc_tl  num_il_tl  \\\n",
       "0              1.0              4.0          1.0        2.0        8.0   \n",
       "1              6.0              9.0          7.0       18.0        2.0   \n",
       "2              3.0              4.0          3.0       12.0       16.0   \n",
       "3              3.0              5.0          3.0        5.0        1.0   \n",
       "5              4.0              7.0          5.0       16.0       17.0   \n",
       "7              2.0              3.0          3.0        6.0        2.0   \n",
       "8              3.0              5.0          7.0        9.0        4.0   \n",
       "9              2.0              2.0          3.0        4.0        3.0   \n",
       "10             4.0              5.0          4.0        6.0        2.0   \n",
       "11             3.0              8.0          3.0        6.0        6.0   \n",
       "\n",
       "    num_op_rev_tl  num_rev_accts  num_rev_tl_bal_gt_0  num_sats  \\\n",
       "0             5.0            9.0                  4.0       6.0   \n",
       "1            14.0           32.0                  9.0      17.0   \n",
       "2             5.0           18.0                  4.0      10.0   \n",
       "3             5.0            7.0                  5.0       8.0   \n",
       "5             8.0           26.0                  7.0      12.0   \n",
       "7             5.0           10.0                  3.0       7.0   \n",
       "8            10.0           13.0                  5.0      14.0   \n",
       "9             6.0            9.0                  2.0       8.0   \n",
       "10            5.0            8.0                  5.0       5.0   \n",
       "11            9.0           13.0                  8.0      11.0   \n",
       "\n",
       "    num_tl_120dpd_2m  num_tl_30dpd  num_tl_90g_dpd_24m  num_tl_op_past_12m  \\\n",
       "0                0.0           0.0                 0.0                 4.0   \n",
       "1                0.0           0.0                 0.0                 4.0   \n",
       "2                0.0           0.0                 0.0                 2.0   \n",
       "3                0.0           0.0                 0.0                 0.0   \n",
       "5                0.0           0.0                 0.0                 3.0   \n",
       "7                0.0           0.0                 0.0                 0.0   \n",
       "8                0.0           0.0                 0.0                 1.0   \n",
       "9                0.0           0.0                 0.0                 2.0   \n",
       "10               0.0           0.0                 0.0                 0.0   \n",
       "11               0.0           0.0                 0.0                 4.0   \n",
       "\n",
       "    pct_tl_nvr_dlq  percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  \\\n",
       "0            100.0               0.0                   0.0        0.0   \n",
       "1             83.3              14.3                   0.0        0.0   \n",
       "2             91.4             100.0                   0.0        0.0   \n",
       "3             76.9             100.0                   0.0        0.0   \n",
       "5            100.0              60.0                   0.0        0.0   \n",
       "7             71.4              66.7                   0.0        0.0   \n",
       "8             95.7              66.7                   0.0        0.0   \n",
       "9            100.0               0.0                   0.0        0.0   \n",
       "10           100.0             100.0                   0.0        0.0   \n",
       "11            89.5               0.0                   0.0        0.0   \n",
       "\n",
       "    tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n",
       "0          196500.0           149140.0         10000.0   \n",
       "1          179407.0            15030.0         13000.0   \n",
       "2           57073.0            42315.0         15000.0   \n",
       "3          368700.0            18007.0          4400.0   \n",
       "5           52490.0            38566.0         21100.0   \n",
       "7          428032.0            34475.0          9200.0   \n",
       "8          443889.0            79490.0         36000.0   \n",
       "9           29971.0            10133.0         16600.0   \n",
       "10          24200.0            23723.0         21200.0   \n",
       "11          42943.0            17281.0          5500.0   \n",
       "\n",
       "    total_il_high_credit_limit  sec_app_fico_best  annual_inc_final  \\\n",
       "0                      12000.0                  0           78000.0   \n",
       "1                      11325.0                  0           58000.0   \n",
       "2                      35573.0                  0           63800.0   \n",
       "3                      18000.0                  0          125000.0   \n",
       "5                      24890.0                  0           69000.0   \n",
       "7                      33000.0                  0          109777.0   \n",
       "8                      78126.0                  0           72000.0   \n",
       "9                       6000.0                  0           26000.0   \n",
       "10                         0.0                  0           90000.0   \n",
       "11                     27243.0                  0           60000.0   \n",
       "\n",
       "    earliest_cr_line_months  emp_length_floats  average_fico  dti_final  \\\n",
       "0                       278               10.0         752.0      12.03   \n",
       "1                       337                8.0         712.0      14.92   \n",
       "2                       170                6.0         687.0      18.49   \n",
       "3                       204               10.0         667.0       8.31   \n",
       "5                       299               10.0         682.0      25.81   \n",
       "7                       167                6.0         702.0      11.63   \n",
       "8                       228               10.0         667.0      27.03   \n",
       "9                       101                2.0         747.0      14.36   \n",
       "10                      171                8.0         677.0       8.44   \n",
       "11                      225               10.0         682.0      22.42   \n",
       "\n",
       "   home_ownership verification_status   issue_d  loan_status  \\\n",
       "0            RENT     Source Verified  Dec-2014   Fully Paid   \n",
       "1        MORTGAGE        Not Verified  Dec-2014  Charged Off   \n",
       "2            RENT     Source Verified  Dec-2014   Fully Paid   \n",
       "3        MORTGAGE            Verified  Dec-2014      Current   \n",
       "5            RENT     Source Verified  Dec-2014   Fully Paid   \n",
       "7             OWN            Verified  Dec-2014   Fully Paid   \n",
       "8            RENT     Source Verified  Dec-2014  Charged Off   \n",
       "9            RENT        Not Verified  Dec-2014   Fully Paid   \n",
       "10           RENT            Verified  Dec-2014  Charged Off   \n",
       "11           RENT     Source Verified  Dec-2014  Charged Off   \n",
       "\n",
       "               purpose addr_state initial_list_status application_type  \\\n",
       "0   debt_consolidation         VA                   w       Individual   \n",
       "1          credit_card         CA                   w       Individual   \n",
       "2          credit_card         MO                   w       Individual   \n",
       "3                  car         CA                   w       Individual   \n",
       "5   debt_consolidation         NJ                   f       Individual   \n",
       "7   debt_consolidation         VA                   w       Individual   \n",
       "8          credit_card         WA                   f       Individual   \n",
       "9   debt_consolidation         MD                   f       Individual   \n",
       "10  debt_consolidation         MI                   f       Individual   \n",
       "11               house         FL                   f       Individual   \n",
       "\n",
       "   disbursement_method debt_settlement_flag mths_since_recent_bc_cat  \\\n",
       "0                 Cash                    N                      <50   \n",
       "1                 Cash                    N                      <50   \n",
       "2                 Cash                    N                      <50   \n",
       "3                 Cash                    N                      <50   \n",
       "5                 Cash                    N                      <50   \n",
       "7                 Cash                    N                      50+   \n",
       "8                 Cash                    N                      <50   \n",
       "9                 Cash                    N                      <50   \n",
       "10                Cash                    N                      <50   \n",
       "11                Cash                    N                      <50   \n",
       "\n",
       "   mo_sin_old_il_acct_cat mths_since_recent_inq_cat  \\\n",
       "0                   0-150              Not Reported   \n",
       "1                   0-150                       0-5   \n",
       "2                   0-150                      5-15   \n",
       "3                   0-150                       0-5   \n",
       "5                 150-200                       15+   \n",
       "7                   0-150                       15+   \n",
       "8                   0-150                       15+   \n",
       "9                   0-150              Not Reported   \n",
       "10                  0-150                       0-5   \n",
       "11                150-200                       0-5   \n",
       "\n",
       "   mths_since_last_delinq_cat mths_since_rcnt_il_cat  \\\n",
       "0                Not Reported           Not Reported   \n",
       "1                        0-50           Not Reported   \n",
       "2                       50-75           Not Reported   \n",
       "3                        0-50           Not Reported   \n",
       "5                Not Reported           Not Reported   \n",
       "7                        0-50           Not Reported   \n",
       "8                        0-50           Not Reported   \n",
       "9                Not Reported           Not Reported   \n",
       "10               Not Reported           Not Reported   \n",
       "11                       0-50           Not Reported   \n",
       "\n",
       "   mths_since_recent_revol_delinq_cat mths_since_last_major_derog_cat  \\\n",
       "0                        Not Reported                    Not Reported   \n",
       "1                                0-50                           50-75   \n",
       "2                               50-75                           50-75   \n",
       "3                                0-50                            0-50   \n",
       "5                        Not Reported                    Not Reported   \n",
       "7                                0-50                    Not Reported   \n",
       "8                                0-50                    Not Reported   \n",
       "9                        Not Reported                    Not Reported   \n",
       "10                       Not Reported                    Not Reported   \n",
       "11                       Not Reported                    Not Reported   \n",
       "\n",
       "   mths_since_recent_bc_dlq_cat mths_since_last_record_cat  sec_app_flag  \\\n",
       "0                  Not Reported               Not Reported             0   \n",
       "1                          0-50               Not Reported             0   \n",
       "2                         50-75               Not Reported             0   \n",
       "3                          0-50               Not Reported             0   \n",
       "5                  Not Reported               Not Reported             0   \n",
       "7                          0-50               Not Reported             0   \n",
       "8                  Not Reported               Not Reported             0   \n",
       "9                  Not Reported               Not Reported             0   \n",
       "10                 Not Reported               Not Reported             0   \n",
       "11                 Not Reported               Not Reported             0   \n",
       "\n",
       "    last_credit_pull_month last_credit_pull_year earliest_cr_line_year  \\\n",
       "0                       11                  2017                  1994   \n",
       "1                        2                  2017                  1989   \n",
       "2                       12                  2017                  2003   \n",
       "3                       12                  2017                  2000   \n",
       "5                       12                  2017                  1992   \n",
       "7                       10                  2016                  2003   \n",
       "8                       10                  2016                  1998   \n",
       "9                        4                  2015                  2009   \n",
       "10                       8                  2017                  2003   \n",
       "11                      10                  2016                  1999   \n",
       "\n",
       "   earliest_cr_line_month  term  int_rate  \n",
       "0                      08    60     12.39  \n",
       "1                      09    36      6.99  \n",
       "2                      08    60     15.59  \n",
       "3                      10    60     17.14  \n",
       "5                      11    36     13.66  \n",
       "7                      11    60     11.44  \n",
       "8                      10    36     14.31  \n",
       "9                      05    36     11.44  \n",
       "10                     07    36     11.99  \n",
       "11                     01    36     17.86  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1285998, 91)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lc.drop('int_rate', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_type_preprocessing(df,y, var_type = 'continuous',\n",
    "                            continuous_list = None, categorical_list = None, categorical_only= False, seed = None):\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    ''' The following function will preprocess the data and create a train test split.\n",
    "    It will be able to handle dataframes that are all categorical, all continuous, or mixed.\n",
    "    it will create dummies for categorical variables. It will normalize continous variables.\n",
    "    it will also create a train test split. For mixed dataframes it will return the indexes of \n",
    "    the continuous and categorical variables.\n",
    "    \n",
    "    var_type\n",
    "    --------\n",
    "    var_type = 'continuous' (by default. Indicates only continous variables in the dataframe)\n",
    "    var_type = 'categorical' ('Indicates only a categorical dataframe)\n",
    "    var_type = 'Mixed ('Indicates both categorical and continous variables present. \n",
    "                        Requires both categorical list, and continuous lists.)\n",
    "    '''\n",
    "    \n",
    "    #get variable names from the function\n",
    "    #------------------------------------\n",
    "    \n",
    "    #get the y col\n",
    "    y_col = y\n",
    "    \n",
    "    #create the y array \n",
    "    y = np.array(df[y].astype(str))\n",
    "    \n",
    "    \"\"\" Determine the type variables we are working with and \n",
    "    process accrodingly\"\"\"\n",
    "    \n",
    "    if var_type == 'mixed':\n",
    "        #get categorical & continuous variables\n",
    "        cont = continuous_list\n",
    "        cat = categorical_list\n",
    "        \n",
    "        #subset continuous\n",
    "        cont_df = df[cont]\n",
    "        \n",
    "        #get the length of continuous and categorical to slice arrays\n",
    "        split_position = len(cont)\n",
    "        \n",
    "        #get dummies for categorical variables\n",
    "        cat_df = pd.get_dummies(df[cat].astype(str))\n",
    "    \n",
    "        #recreate the dataframe with dummies\n",
    "        X = pd.merge(cont_df,cat_df, how = 'left', left_index=True, right_index = True)\n",
    "        \n",
    "    elif var_type == 'categorical':\n",
    "        #create a dataframe of dummy variables that do not include y\n",
    "        dummy_list=[]\n",
    "        for i in categorical_list:\n",
    "            dummy_list.append(pd.get_dummies(lc[i]))\n",
    "        X=pd.concat(dummy_list, axis=1)\n",
    "        \n",
    "    elif var_type == 'continuous':\n",
    "        X = df[[col for col in df.columns if col not in y_col]]\n",
    "    \n",
    "    \n",
    "    # Get column names\n",
    "    col_names = X.columns\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Create the train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = seed)\n",
    "    \n",
    "    \n",
    "    #preprocess continuous variables\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "        \n",
    "    if var_type == 'mixed':\n",
    "        #idenify the array index of categorical and continuous columns\n",
    "        cont_idx = [i for i in range(split_position)]\n",
    "        cat_idx = [i for i in range(split_position, col_names.shape[0])] \n",
    "        return X_train, X_test, y_train, y_test, col_names\n",
    "    else:\n",
    "        return X_train, X_test, y_train, y_test, col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, col_names = data_type_preprocessing(lc,\n",
    "                                                                      y = 'int_rate', \n",
    "                                                                      var_type = 'mixed',\n",
    "                                                                     continuous_list = continuous, #contiuous\n",
    "                                                                      categorical_only= True,\n",
    "                                                                      categorical_list = categorical, #categorical\n",
    "                                                                      seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>out_prncp</th>\n",
       "      <th>out_prncp_inv</th>\n",
       "      <th>total_pymnt</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <th>total_rec_late_fee</th>\n",
       "      <th>recoveries</th>\n",
       "      <th>collection_recovery_fee</th>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <th>last_fico_range_high</th>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>tot_coll_amt</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>mo_sin_old_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>num_accts_ever_120_pd</th>\n",
       "      <th>num_actv_bc_tl</th>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <th>num_bc_sats</th>\n",
       "      <th>num_bc_tl</th>\n",
       "      <th>num_il_tl</th>\n",
       "      <th>num_op_rev_tl</th>\n",
       "      <th>num_rev_accts</th>\n",
       "      <th>num_rev_tl_bal_gt_0</th>\n",
       "      <th>num_sats</th>\n",
       "      <th>num_tl_120dpd_2m</th>\n",
       "      <th>num_tl_30dpd</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>sec_app_fico_best</th>\n",
       "      <th>annual_inc_final</th>\n",
       "      <th>earliest_cr_line_months</th>\n",
       "      <th>emp_length_floats</th>\n",
       "      <th>average_fico</th>\n",
       "      <th>dti_final</th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>home_ownership_ANY</th>\n",
       "      <th>home_ownership_MORTGAGE</th>\n",
       "      <th>home_ownership_NONE</th>\n",
       "      <th>home_ownership_OTHER</th>\n",
       "      <th>home_ownership_OWN</th>\n",
       "      <th>home_ownership_RENT</th>\n",
       "      <th>verification_status_Not Verified</th>\n",
       "      <th>verification_status_Source Verified</th>\n",
       "      <th>verification_status_Verified</th>\n",
       "      <th>issue_d_Apr-2013</th>\n",
       "      <th>issue_d_Apr-2014</th>\n",
       "      <th>issue_d_Apr-2015</th>\n",
       "      <th>issue_d_Apr-2016</th>\n",
       "      <th>issue_d_Apr-2017</th>\n",
       "      <th>issue_d_Aug-2012</th>\n",
       "      <th>issue_d_Aug-2013</th>\n",
       "      <th>issue_d_Aug-2014</th>\n",
       "      <th>issue_d_Aug-2015</th>\n",
       "      <th>issue_d_Aug-2016</th>\n",
       "      <th>issue_d_Aug-2017</th>\n",
       "      <th>issue_d_Dec-2012</th>\n",
       "      <th>issue_d_Dec-2013</th>\n",
       "      <th>issue_d_Dec-2014</th>\n",
       "      <th>issue_d_Dec-2015</th>\n",
       "      <th>issue_d_Dec-2016</th>\n",
       "      <th>issue_d_Feb-2013</th>\n",
       "      <th>issue_d_Feb-2014</th>\n",
       "      <th>issue_d_Feb-2015</th>\n",
       "      <th>issue_d_Feb-2016</th>\n",
       "      <th>issue_d_Feb-2017</th>\n",
       "      <th>issue_d_Jan-2013</th>\n",
       "      <th>issue_d_Jan-2014</th>\n",
       "      <th>issue_d_Jan-2015</th>\n",
       "      <th>issue_d_Jan-2016</th>\n",
       "      <th>issue_d_Jan-2017</th>\n",
       "      <th>issue_d_Jul-2013</th>\n",
       "      <th>issue_d_Jul-2014</th>\n",
       "      <th>issue_d_Jul-2015</th>\n",
       "      <th>issue_d_Jul-2016</th>\n",
       "      <th>issue_d_Jul-2017</th>\n",
       "      <th>issue_d_Jun-2013</th>\n",
       "      <th>issue_d_Jun-2014</th>\n",
       "      <th>issue_d_Jun-2015</th>\n",
       "      <th>issue_d_Jun-2016</th>\n",
       "      <th>issue_d_Jun-2017</th>\n",
       "      <th>issue_d_Mar-2013</th>\n",
       "      <th>issue_d_Mar-2014</th>\n",
       "      <th>issue_d_Mar-2015</th>\n",
       "      <th>issue_d_Mar-2016</th>\n",
       "      <th>issue_d_Mar-2017</th>\n",
       "      <th>issue_d_May-2013</th>\n",
       "      <th>issue_d_May-2014</th>\n",
       "      <th>issue_d_May-2015</th>\n",
       "      <th>issue_d_May-2016</th>\n",
       "      <th>issue_d_May-2017</th>\n",
       "      <th>issue_d_Nov-2012</th>\n",
       "      <th>issue_d_Nov-2013</th>\n",
       "      <th>issue_d_Nov-2014</th>\n",
       "      <th>issue_d_Nov-2015</th>\n",
       "      <th>issue_d_Nov-2016</th>\n",
       "      <th>issue_d_Oct-2012</th>\n",
       "      <th>issue_d_Oct-2013</th>\n",
       "      <th>issue_d_Oct-2014</th>\n",
       "      <th>issue_d_Oct-2015</th>\n",
       "      <th>issue_d_Oct-2016</th>\n",
       "      <th>issue_d_Sep-2012</th>\n",
       "      <th>issue_d_Sep-2013</th>\n",
       "      <th>issue_d_Sep-2014</th>\n",
       "      <th>issue_d_Sep-2015</th>\n",
       "      <th>issue_d_Sep-2016</th>\n",
       "      <th>issue_d_Sep-2017</th>\n",
       "      <th>loan_status_Charged Off</th>\n",
       "      <th>loan_status_Current</th>\n",
       "      <th>loan_status_Default</th>\n",
       "      <th>loan_status_Fully Paid</th>\n",
       "      <th>loan_status_In Grace Period</th>\n",
       "      <th>loan_status_Late (16-30 days)</th>\n",
       "      <th>loan_status_Late (31-120 days)</th>\n",
       "      <th>purpose_car</th>\n",
       "      <th>purpose_credit_card</th>\n",
       "      <th>purpose_debt_consolidation</th>\n",
       "      <th>purpose_educational</th>\n",
       "      <th>purpose_home_improvement</th>\n",
       "      <th>purpose_house</th>\n",
       "      <th>purpose_major_purchase</th>\n",
       "      <th>purpose_medical</th>\n",
       "      <th>purpose_moving</th>\n",
       "      <th>purpose_other</th>\n",
       "      <th>purpose_renewable_energy</th>\n",
       "      <th>purpose_small_business</th>\n",
       "      <th>purpose_vacation</th>\n",
       "      <th>purpose_wedding</th>\n",
       "      <th>addr_state_AK</th>\n",
       "      <th>addr_state_AL</th>\n",
       "      <th>addr_state_AR</th>\n",
       "      <th>addr_state_AZ</th>\n",
       "      <th>addr_state_CA</th>\n",
       "      <th>addr_state_CO</th>\n",
       "      <th>addr_state_CT</th>\n",
       "      <th>addr_state_DC</th>\n",
       "      <th>addr_state_DE</th>\n",
       "      <th>addr_state_FL</th>\n",
       "      <th>addr_state_GA</th>\n",
       "      <th>addr_state_HI</th>\n",
       "      <th>addr_state_IA</th>\n",
       "      <th>addr_state_ID</th>\n",
       "      <th>addr_state_IL</th>\n",
       "      <th>addr_state_IN</th>\n",
       "      <th>addr_state_KS</th>\n",
       "      <th>addr_state_KY</th>\n",
       "      <th>addr_state_LA</th>\n",
       "      <th>addr_state_MA</th>\n",
       "      <th>addr_state_MD</th>\n",
       "      <th>addr_state_ME</th>\n",
       "      <th>addr_state_MI</th>\n",
       "      <th>addr_state_MN</th>\n",
       "      <th>addr_state_MO</th>\n",
       "      <th>addr_state_MS</th>\n",
       "      <th>addr_state_MT</th>\n",
       "      <th>addr_state_NC</th>\n",
       "      <th>addr_state_ND</th>\n",
       "      <th>addr_state_NE</th>\n",
       "      <th>addr_state_NH</th>\n",
       "      <th>addr_state_NJ</th>\n",
       "      <th>addr_state_NM</th>\n",
       "      <th>addr_state_NV</th>\n",
       "      <th>addr_state_NY</th>\n",
       "      <th>addr_state_OH</th>\n",
       "      <th>addr_state_OK</th>\n",
       "      <th>addr_state_OR</th>\n",
       "      <th>addr_state_PA</th>\n",
       "      <th>addr_state_RI</th>\n",
       "      <th>addr_state_SC</th>\n",
       "      <th>addr_state_SD</th>\n",
       "      <th>addr_state_TN</th>\n",
       "      <th>addr_state_TX</th>\n",
       "      <th>addr_state_UT</th>\n",
       "      <th>addr_state_VA</th>\n",
       "      <th>addr_state_VT</th>\n",
       "      <th>addr_state_WA</th>\n",
       "      <th>addr_state_WI</th>\n",
       "      <th>addr_state_WV</th>\n",
       "      <th>addr_state_WY</th>\n",
       "      <th>initial_list_status_f</th>\n",
       "      <th>initial_list_status_w</th>\n",
       "      <th>application_type_Individual</th>\n",
       "      <th>application_type_Joint App</th>\n",
       "      <th>disbursement_method_Cash</th>\n",
       "      <th>disbursement_method_DirectPay</th>\n",
       "      <th>debt_settlement_flag_N</th>\n",
       "      <th>debt_settlement_flag_Y</th>\n",
       "      <th>mths_since_recent_bc_cat_50+</th>\n",
       "      <th>mths_since_recent_bc_cat_&lt;50</th>\n",
       "      <th>mths_since_recent_bc_cat_Not Reported</th>\n",
       "      <th>mo_sin_old_il_acct_cat_0-150</th>\n",
       "      <th>mo_sin_old_il_acct_cat_150-200</th>\n",
       "      <th>mo_sin_old_il_acct_cat_200+</th>\n",
       "      <th>mo_sin_old_il_acct_cat_Not Reported</th>\n",
       "      <th>mths_since_recent_inq_cat_0-5</th>\n",
       "      <th>mths_since_recent_inq_cat_15+</th>\n",
       "      <th>mths_since_recent_inq_cat_5-15</th>\n",
       "      <th>mths_since_recent_inq_cat_Not Reported</th>\n",
       "      <th>mths_since_last_delinq_cat_0-50</th>\n",
       "      <th>mths_since_last_delinq_cat_50-75</th>\n",
       "      <th>mths_since_last_delinq_cat_75+</th>\n",
       "      <th>mths_since_last_delinq_cat_Not Reported</th>\n",
       "      <th>mths_since_rcnt_il_cat_50+</th>\n",
       "      <th>mths_since_rcnt_il_cat_&lt;50</th>\n",
       "      <th>mths_since_rcnt_il_cat_Not Reported</th>\n",
       "      <th>mths_since_recent_revol_delinq_cat_0-50</th>\n",
       "      <th>mths_since_recent_revol_delinq_cat_50-75</th>\n",
       "      <th>mths_since_recent_revol_delinq_cat_75+</th>\n",
       "      <th>mths_since_recent_revol_delinq_cat_Not Reported</th>\n",
       "      <th>mths_since_last_major_derog_cat_0-50</th>\n",
       "      <th>mths_since_last_major_derog_cat_50-75</th>\n",
       "      <th>mths_since_last_major_derog_cat_75+</th>\n",
       "      <th>mths_since_last_major_derog_cat_Not Reported</th>\n",
       "      <th>mths_since_recent_bc_dlq_cat_0-50</th>\n",
       "      <th>mths_since_recent_bc_dlq_cat_50-75</th>\n",
       "      <th>mths_since_recent_bc_dlq_cat_75+</th>\n",
       "      <th>mths_since_recent_bc_dlq_cat_Not Reported</th>\n",
       "      <th>mths_since_last_record_cat_0-40</th>\n",
       "      <th>mths_since_last_record_cat_40-80</th>\n",
       "      <th>mths_since_last_record_cat_80+</th>\n",
       "      <th>mths_since_last_record_cat_Not Reported</th>\n",
       "      <th>sec_app_flag_0</th>\n",
       "      <th>sec_app_flag_1</th>\n",
       "      <th>last_credit_pull_month_1</th>\n",
       "      <th>last_credit_pull_month_10</th>\n",
       "      <th>last_credit_pull_month_11</th>\n",
       "      <th>last_credit_pull_month_12</th>\n",
       "      <th>last_credit_pull_month_2</th>\n",
       "      <th>last_credit_pull_month_3</th>\n",
       "      <th>last_credit_pull_month_4</th>\n",
       "      <th>last_credit_pull_month_5</th>\n",
       "      <th>last_credit_pull_month_6</th>\n",
       "      <th>last_credit_pull_month_7</th>\n",
       "      <th>last_credit_pull_month_8</th>\n",
       "      <th>last_credit_pull_month_9</th>\n",
       "      <th>last_credit_pull_year_2012</th>\n",
       "      <th>last_credit_pull_year_2013</th>\n",
       "      <th>last_credit_pull_year_2014</th>\n",
       "      <th>last_credit_pull_year_2015</th>\n",
       "      <th>last_credit_pull_year_2016</th>\n",
       "      <th>last_credit_pull_year_2017</th>\n",
       "      <th>earliest_cr_line_year_1933</th>\n",
       "      <th>earliest_cr_line_year_1934</th>\n",
       "      <th>earliest_cr_line_year_1944</th>\n",
       "      <th>earliest_cr_line_year_1945</th>\n",
       "      <th>earliest_cr_line_year_1946</th>\n",
       "      <th>earliest_cr_line_year_1948</th>\n",
       "      <th>earliest_cr_line_year_1949</th>\n",
       "      <th>earliest_cr_line_year_1950</th>\n",
       "      <th>earliest_cr_line_year_1951</th>\n",
       "      <th>earliest_cr_line_year_1952</th>\n",
       "      <th>earliest_cr_line_year_1953</th>\n",
       "      <th>earliest_cr_line_year_1954</th>\n",
       "      <th>earliest_cr_line_year_1955</th>\n",
       "      <th>earliest_cr_line_year_1956</th>\n",
       "      <th>earliest_cr_line_year_1957</th>\n",
       "      <th>earliest_cr_line_year_1958</th>\n",
       "      <th>earliest_cr_line_year_1959</th>\n",
       "      <th>earliest_cr_line_year_1960</th>\n",
       "      <th>earliest_cr_line_year_1961</th>\n",
       "      <th>earliest_cr_line_year_1962</th>\n",
       "      <th>earliest_cr_line_year_1963</th>\n",
       "      <th>earliest_cr_line_year_1964</th>\n",
       "      <th>earliest_cr_line_year_1965</th>\n",
       "      <th>earliest_cr_line_year_1966</th>\n",
       "      <th>earliest_cr_line_year_1967</th>\n",
       "      <th>earliest_cr_line_year_1968</th>\n",
       "      <th>earliest_cr_line_year_1969</th>\n",
       "      <th>earliest_cr_line_year_1970</th>\n",
       "      <th>earliest_cr_line_year_1971</th>\n",
       "      <th>earliest_cr_line_year_1972</th>\n",
       "      <th>earliest_cr_line_year_1973</th>\n",
       "      <th>earliest_cr_line_year_1974</th>\n",
       "      <th>earliest_cr_line_year_1975</th>\n",
       "      <th>earliest_cr_line_year_1976</th>\n",
       "      <th>earliest_cr_line_year_1977</th>\n",
       "      <th>earliest_cr_line_year_1978</th>\n",
       "      <th>earliest_cr_line_year_1979</th>\n",
       "      <th>earliest_cr_line_year_1980</th>\n",
       "      <th>earliest_cr_line_year_1981</th>\n",
       "      <th>earliest_cr_line_year_1982</th>\n",
       "      <th>earliest_cr_line_year_1983</th>\n",
       "      <th>earliest_cr_line_year_1984</th>\n",
       "      <th>earliest_cr_line_year_1985</th>\n",
       "      <th>earliest_cr_line_year_1986</th>\n",
       "      <th>earliest_cr_line_year_1987</th>\n",
       "      <th>earliest_cr_line_year_1988</th>\n",
       "      <th>earliest_cr_line_year_1989</th>\n",
       "      <th>earliest_cr_line_year_1990</th>\n",
       "      <th>earliest_cr_line_year_1991</th>\n",
       "      <th>earliest_cr_line_year_1992</th>\n",
       "      <th>earliest_cr_line_year_1993</th>\n",
       "      <th>earliest_cr_line_year_1994</th>\n",
       "      <th>earliest_cr_line_year_1995</th>\n",
       "      <th>earliest_cr_line_year_1996</th>\n",
       "      <th>earliest_cr_line_year_1997</th>\n",
       "      <th>earliest_cr_line_year_1998</th>\n",
       "      <th>earliest_cr_line_year_1999</th>\n",
       "      <th>earliest_cr_line_year_2000</th>\n",
       "      <th>earliest_cr_line_year_2001</th>\n",
       "      <th>earliest_cr_line_year_2002</th>\n",
       "      <th>earliest_cr_line_year_2003</th>\n",
       "      <th>earliest_cr_line_year_2004</th>\n",
       "      <th>earliest_cr_line_year_2005</th>\n",
       "      <th>earliest_cr_line_year_2006</th>\n",
       "      <th>earliest_cr_line_year_2007</th>\n",
       "      <th>earliest_cr_line_year_2008</th>\n",
       "      <th>earliest_cr_line_year_2009</th>\n",
       "      <th>earliest_cr_line_year_2010</th>\n",
       "      <th>earliest_cr_line_year_2011</th>\n",
       "      <th>earliest_cr_line_year_2012</th>\n",
       "      <th>earliest_cr_line_year_2013</th>\n",
       "      <th>earliest_cr_line_year_2014</th>\n",
       "      <th>earliest_cr_line_month_01</th>\n",
       "      <th>earliest_cr_line_month_02</th>\n",
       "      <th>earliest_cr_line_month_03</th>\n",
       "      <th>earliest_cr_line_month_04</th>\n",
       "      <th>earliest_cr_line_month_05</th>\n",
       "      <th>earliest_cr_line_month_06</th>\n",
       "      <th>earliest_cr_line_month_07</th>\n",
       "      <th>earliest_cr_line_month_08</th>\n",
       "      <th>earliest_cr_line_month_09</th>\n",
       "      <th>earliest_cr_line_month_10</th>\n",
       "      <th>earliest_cr_line_month_11</th>\n",
       "      <th>earliest_cr_line_month_12</th>\n",
       "      <th>term_36</th>\n",
       "      <th>term_60</th>\n",
       "      <th>sub_grade_1</th>\n",
       "      <th>sub_grade_2</th>\n",
       "      <th>sub_grade_3</th>\n",
       "      <th>sub_grade_4</th>\n",
       "      <th>sub_grade_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.292763</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.01795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.114301</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.440452</td>\n",
       "      <td>0.440452</td>\n",
       "      <td>0.156415</td>\n",
       "      <td>0.156415</td>\n",
       "      <td>0.159548</td>\n",
       "      <td>9.489372e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012029</td>\n",
       "      <td>0.787059</td>\n",
       "      <td>0.786982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054889</td>\n",
       "      <td>0.00429</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.025601</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.224971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.359577</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.330986</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038038</td>\n",
       "      <td>0.018045</td>\n",
       "      <td>0.007870</td>\n",
       "      <td>0.032351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.293756</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.350404</td>\n",
       "      <td>0.352237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.248545</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.01649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008047</td>\n",
       "      <td>0.185265</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.348805</td>\n",
       "      <td>0.348805</td>\n",
       "      <td>0.126255</td>\n",
       "      <td>0.126255</td>\n",
       "      <td>0.101195</td>\n",
       "      <td>9.489372e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010230</td>\n",
       "      <td>0.828235</td>\n",
       "      <td>0.828402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>0.00682</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.008980</td>\n",
       "      <td>0.214664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376028</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.288732</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007755</td>\n",
       "      <td>0.009087</td>\n",
       "      <td>0.015108</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.306039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.053908</td>\n",
       "      <td>0.323587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.162167</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.03496</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008816</td>\n",
       "      <td>0.170639</td>\n",
       "      <td>0.212644</td>\n",
       "      <td>0.185478</td>\n",
       "      <td>0.185478</td>\n",
       "      <td>0.032080</td>\n",
       "      <td>0.032080</td>\n",
       "      <td>0.039522</td>\n",
       "      <td>9.489372e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006715</td>\n",
       "      <td>0.869412</td>\n",
       "      <td>0.869822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020735</td>\n",
       "      <td>0.00812</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.008830</td>\n",
       "      <td>0.089982</td>\n",
       "      <td>0.105713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.319624</td>\n",
       "      <td>0.139269</td>\n",
       "      <td>0.049470</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.31250</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.190141</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020632</td>\n",
       "      <td>0.038644</td>\n",
       "      <td>0.064321</td>\n",
       "      <td>0.059569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.245650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.242588</td>\n",
       "      <td>0.686028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.454155</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.02512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011699</td>\n",
       "      <td>0.503250</td>\n",
       "      <td>0.178161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.456444</td>\n",
       "      <td>0.456444</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>9.489372e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108174</td>\n",
       "      <td>0.798824</td>\n",
       "      <td>0.798817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035253</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.098361</td>\n",
       "      <td>0.024663</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.292697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.327850</td>\n",
       "      <td>0.034247</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.18750</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020802</td>\n",
       "      <td>0.018923</td>\n",
       "      <td>0.023069</td>\n",
       "      <td>0.013198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.295803</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.134771</td>\n",
       "      <td>0.492936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.087142</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.02180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.119177</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.082842</td>\n",
       "      <td>0.082842</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.042158</td>\n",
       "      <td>9.489372e-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.834118</td>\n",
       "      <td>0.834320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023604</td>\n",
       "      <td>0.00320</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.019265</td>\n",
       "      <td>0.034050</td>\n",
       "      <td>0.065371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149236</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>0.007067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.119565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.043992</td>\n",
       "      <td>0.019993</td>\n",
       "      <td>0.052425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.111566</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.404313</td>\n",
       "      <td>0.427786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt  installment  annual_inc      dti  delinq_2yrs  inq_last_6mths  \\\n",
       "0   0.589744     0.292763    0.001359  0.01795     0.000000           0.000   \n",
       "1   0.435897     0.248545    0.000714  0.01649     0.000000           0.125   \n",
       "2   0.205128     0.162167    0.001114  0.03496     0.051282           0.000   \n",
       "3   0.512821     0.454155    0.000796  0.02512     0.000000           0.250   \n",
       "4   0.102564     0.087142    0.000541  0.02180     0.000000           0.000   \n",
       "\n",
       "   open_acc  pub_rec  revol_bal  revol_util  total_acc  out_prncp  \\\n",
       "0  0.250000      0.0   0.003134    0.114301   0.333333   0.440452   \n",
       "1  0.369565      0.0   0.008047    0.185265   0.258621   0.348805   \n",
       "2  0.239130      0.0   0.008816    0.170639   0.212644   0.185478   \n",
       "3  0.141304      0.0   0.011699    0.503250   0.178161   0.000000   \n",
       "4  0.119565      0.0   0.002434    0.119177   0.086207   0.082842   \n",
       "\n",
       "   out_prncp_inv  total_pymnt  total_pymnt_inv  total_rec_prncp  \\\n",
       "0       0.440452     0.156415         0.156415         0.159548   \n",
       "1       0.348805     0.126255         0.126255         0.101195   \n",
       "2       0.185478     0.032080         0.032080         0.039522   \n",
       "3       0.000000     0.456444         0.456444         0.525000   \n",
       "4       0.082842     0.032599         0.032599         0.042158   \n",
       "\n",
       "   total_rec_late_fee  recoveries  collection_recovery_fee  last_pymnt_amnt  \\\n",
       "0        9.489372e-12         0.0                      0.0         0.012029   \n",
       "1        9.489372e-12         0.0                      0.0         0.010230   \n",
       "2        9.489372e-12         0.0                      0.0         0.006715   \n",
       "3        9.489372e-12         0.0                      0.0         0.108174   \n",
       "4        9.489372e-12         0.0                      0.0         0.003663   \n",
       "\n",
       "   last_fico_range_high  last_fico_range_low  collections_12_mths_ex_med  \\\n",
       "0              0.787059             0.786982                         0.0   \n",
       "1              0.828235             0.828402                         0.0   \n",
       "2              0.869412             0.869822                         0.0   \n",
       "3              0.798824             0.798817                         0.0   \n",
       "4              0.834118             0.834320                         0.0   \n",
       "\n",
       "   acc_now_delinq  tot_coll_amt  tot_cur_bal  total_rev_hi_lim  \\\n",
       "0             0.0           0.0     0.054889           0.00429   \n",
       "1             0.0           0.0     0.004876           0.00682   \n",
       "2             0.0           0.0     0.020735           0.00812   \n",
       "3             0.0           0.0     0.035253           0.00364   \n",
       "4             0.0           0.0     0.023604           0.00320   \n",
       "\n",
       "   acc_open_past_24mths  avg_cur_bal  bc_open_to_buy   bc_util  \\\n",
       "0              0.147541     0.025601        0.004100  0.224971   \n",
       "1              0.377049     0.001405        0.008980  0.214664   \n",
       "2              0.032787     0.008830        0.089982  0.105713   \n",
       "3              0.098361     0.024663        0.000315  0.292697   \n",
       "4              0.065574     0.019265        0.034050  0.065371   \n",
       "\n",
       "   chargeoff_within_12_mths  delinq_amnt  mo_sin_old_rev_tl_op  \\\n",
       "0                       0.0          0.0              0.359577   \n",
       "1                       0.0          0.0              0.376028   \n",
       "2                       0.0          0.0              0.319624   \n",
       "3                       0.0          0.0              0.327850   \n",
       "4                       0.0          0.0              0.149236   \n",
       "\n",
       "   mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  num_accts_ever_120_pd  \\\n",
       "0               0.002283        0.003534  0.031915               0.000000   \n",
       "1               0.009132        0.014134  0.021277               0.039216   \n",
       "2               0.139269        0.049470  0.021277               0.039216   \n",
       "3               0.034247        0.007067  0.042553               0.000000   \n",
       "4               0.004566        0.007067  0.000000               0.000000   \n",
       "\n",
       "   num_actv_bc_tl  num_actv_rev_tl  num_bc_sats  num_bc_tl  num_il_tl  \\\n",
       "0         0.09375         0.181818         0.10   0.263158   0.060000   \n",
       "1         0.12500         0.318182         0.16   0.157895   0.020000   \n",
       "2         0.31250         0.272727         0.32   0.236842   0.046667   \n",
       "3         0.18750         0.250000         0.12   0.157895   0.053333   \n",
       "4         0.03125         0.045455         0.06   0.026316   0.060000   \n",
       "\n",
       "   num_op_rev_tl  num_rev_accts  num_rev_tl_bal_gt_0  num_sats  \\\n",
       "0       0.211111       0.330986             0.181818  0.250000   \n",
       "1       0.366667       0.288732             0.318182  0.369565   \n",
       "2       0.200000       0.190141             0.272727  0.239130   \n",
       "3       0.111111       0.140845             0.250000  0.141304   \n",
       "4       0.033333       0.049296             0.045455  0.119565   \n",
       "\n",
       "   num_tl_120dpd_2m  num_tl_30dpd  num_tl_90g_dpd_24m  num_tl_op_past_12m  \\\n",
       "0               0.0           0.0            0.000000            0.064516   \n",
       "1               0.0           0.0            0.000000            0.258065   \n",
       "2               0.0           0.0            0.051282            0.000000   \n",
       "3               0.0           0.0            0.000000            0.064516   \n",
       "4               0.0           0.0            0.000000            0.064516   \n",
       "\n",
       "   pct_tl_nvr_dlq  percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  \\\n",
       "0           1.000             0.600                   0.0        0.0   \n",
       "1           0.936             0.375                   0.0        0.0   \n",
       "2           0.923             0.250                   0.0        0.0   \n",
       "3           1.000             1.000                   0.0        0.0   \n",
       "4           1.000             0.000                   0.0        0.0   \n",
       "\n",
       "   tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n",
       "0         0.038038           0.018045        0.007870   \n",
       "1         0.007755           0.009087        0.015108   \n",
       "2         0.020632           0.038644        0.064321   \n",
       "3         0.020802           0.018923        0.023069   \n",
       "4         0.014210           0.043992        0.019993   \n",
       "\n",
       "   total_il_high_credit_limit  sec_app_fico_best  annual_inc_final  \\\n",
       "0                    0.032351                0.0          0.001327   \n",
       "1                    0.004495                0.0          0.000682   \n",
       "2                    0.059569                0.0          0.001082   \n",
       "3                    0.013198                0.0          0.000764   \n",
       "4                    0.052425                0.0          0.000509   \n",
       "\n",
       "   earliest_cr_line_months  emp_length_floats  average_fico  dti_final  \\\n",
       "0                 0.293756           1.000000      0.350404   0.352237   \n",
       "1                 0.306039           1.000000      0.053908   0.323587   \n",
       "2                 0.245650           1.000000      0.242588   0.686028   \n",
       "3                 0.295803           1.000000      0.134771   0.492936   \n",
       "4                 0.111566           0.222222      0.404313   0.427786   \n",
       "\n",
       "   grade_A  grade_B  grade_C  grade_D  grade_E  grade_F  grade_G  \\\n",
       "0      0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      1.0      0.0      0.0      0.0   \n",
       "4      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   home_ownership_ANY  home_ownership_MORTGAGE  home_ownership_NONE  \\\n",
       "0                 0.0                      1.0                  0.0   \n",
       "1                 0.0                      0.0                  0.0   \n",
       "2                 0.0                      0.0                  0.0   \n",
       "3                 0.0                      1.0                  0.0   \n",
       "4                 0.0                      0.0                  0.0   \n",
       "\n",
       "   home_ownership_OTHER  home_ownership_OWN  home_ownership_RENT  \\\n",
       "0                   0.0                 0.0                  0.0   \n",
       "1                   0.0                 0.0                  1.0   \n",
       "2                   0.0                 0.0                  1.0   \n",
       "3                   0.0                 0.0                  0.0   \n",
       "4                   0.0                 0.0                  1.0   \n",
       "\n",
       "   verification_status_Not Verified  verification_status_Source Verified  \\\n",
       "0                               0.0                                  0.0   \n",
       "1                               0.0                                  0.0   \n",
       "2                               1.0                                  0.0   \n",
       "3                               0.0                                  0.0   \n",
       "4                               1.0                                  0.0   \n",
       "\n",
       "   verification_status_Verified  issue_d_Apr-2013  issue_d_Apr-2014  \\\n",
       "0                           1.0               0.0               0.0   \n",
       "1                           1.0               0.0               0.0   \n",
       "2                           0.0               0.0               0.0   \n",
       "3                           1.0               0.0               0.0   \n",
       "4                           0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Apr-2015  issue_d_Apr-2016  issue_d_Apr-2017  issue_d_Aug-2012  \\\n",
       "0               0.0               1.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Aug-2013  issue_d_Aug-2014  issue_d_Aug-2015  issue_d_Aug-2016  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               1.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Aug-2017  issue_d_Dec-2012  issue_d_Dec-2013  issue_d_Dec-2014  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Dec-2015  issue_d_Dec-2016  issue_d_Feb-2013  issue_d_Feb-2014  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Feb-2015  issue_d_Feb-2016  issue_d_Feb-2017  issue_d_Jan-2013  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Jan-2014  issue_d_Jan-2015  issue_d_Jan-2016  issue_d_Jan-2017  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Jul-2013  issue_d_Jul-2014  issue_d_Jul-2015  issue_d_Jul-2016  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Jul-2017  issue_d_Jun-2013  issue_d_Jun-2014  issue_d_Jun-2015  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Jun-2016  issue_d_Jun-2017  issue_d_Mar-2013  issue_d_Mar-2014  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               1.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Mar-2015  issue_d_Mar-2016  issue_d_Mar-2017  issue_d_May-2013  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_May-2014  issue_d_May-2015  issue_d_May-2016  issue_d_May-2017  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               1.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Nov-2012  issue_d_Nov-2013  issue_d_Nov-2014  issue_d_Nov-2015  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Nov-2016  issue_d_Oct-2012  issue_d_Oct-2013  issue_d_Oct-2014  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               1.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Oct-2015  issue_d_Oct-2016  issue_d_Sep-2012  issue_d_Sep-2013  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   issue_d_Sep-2014  issue_d_Sep-2015  issue_d_Sep-2016  issue_d_Sep-2017  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   loan_status_Charged Off  loan_status_Current  loan_status_Default  \\\n",
       "0                      0.0                  1.0                  0.0   \n",
       "1                      0.0                  1.0                  0.0   \n",
       "2                      0.0                  1.0                  0.0   \n",
       "3                      0.0                  0.0                  0.0   \n",
       "4                      0.0                  1.0                  0.0   \n",
       "\n",
       "   loan_status_Fully Paid  loan_status_In Grace Period  \\\n",
       "0                     0.0                          0.0   \n",
       "1                     0.0                          0.0   \n",
       "2                     0.0                          0.0   \n",
       "3                     1.0                          0.0   \n",
       "4                     0.0                          0.0   \n",
       "\n",
       "   loan_status_Late (16-30 days)  loan_status_Late (31-120 days)  purpose_car  \\\n",
       "0                            0.0                             0.0          0.0   \n",
       "1                            0.0                             0.0          0.0   \n",
       "2                            0.0                             0.0          0.0   \n",
       "3                            0.0                             0.0          0.0   \n",
       "4                            0.0                             0.0          0.0   \n",
       "\n",
       "   purpose_credit_card  purpose_debt_consolidation  purpose_educational  \\\n",
       "0                  0.0                         1.0                  0.0   \n",
       "1                  0.0                         1.0                  0.0   \n",
       "2                  1.0                         0.0                  0.0   \n",
       "3                  0.0                         1.0                  0.0   \n",
       "4                  1.0                         0.0                  0.0   \n",
       "\n",
       "   purpose_home_improvement  purpose_house  purpose_major_purchase  \\\n",
       "0                       0.0            0.0                     0.0   \n",
       "1                       0.0            0.0                     0.0   \n",
       "2                       0.0            0.0                     0.0   \n",
       "3                       0.0            0.0                     0.0   \n",
       "4                       0.0            0.0                     0.0   \n",
       "\n",
       "   purpose_medical  purpose_moving  purpose_other  purpose_renewable_energy  \\\n",
       "0              0.0             0.0            0.0                       0.0   \n",
       "1              0.0             0.0            0.0                       0.0   \n",
       "2              0.0             0.0            0.0                       0.0   \n",
       "3              0.0             0.0            0.0                       0.0   \n",
       "4              0.0             0.0            0.0                       0.0   \n",
       "\n",
       "   purpose_small_business  purpose_vacation  purpose_wedding  addr_state_AK  \\\n",
       "0                     0.0               0.0              0.0            0.0   \n",
       "1                     0.0               0.0              0.0            0.0   \n",
       "2                     0.0               0.0              0.0            0.0   \n",
       "3                     0.0               0.0              0.0            0.0   \n",
       "4                     0.0               0.0              0.0            0.0   \n",
       "\n",
       "   addr_state_AL  addr_state_AR  addr_state_AZ  addr_state_CA  addr_state_CO  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            1.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            1.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   addr_state_CT  addr_state_DC  addr_state_DE  addr_state_FL  addr_state_GA  \\\n",
       "0            0.0            0.0            0.0            1.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   addr_state_HI  addr_state_IA  addr_state_ID  addr_state_IL  addr_state_IN  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   addr_state_KS  addr_state_KY  addr_state_LA  addr_state_MA  addr_state_MD  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            1.0            0.0   \n",
       "\n",
       "   addr_state_ME  addr_state_MI  addr_state_MN  addr_state_MO  addr_state_MS  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   addr_state_MT  addr_state_NC  addr_state_ND  addr_state_NE  addr_state_NH  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   addr_state_NJ  addr_state_NM  addr_state_NV  addr_state_NY  addr_state_OH  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   addr_state_OK  addr_state_OR  addr_state_PA  addr_state_RI  addr_state_SC  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   addr_state_SD  addr_state_TN  addr_state_TX  addr_state_UT  addr_state_VA  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            1.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   addr_state_VT  addr_state_WA  addr_state_WI  addr_state_WV  addr_state_WY  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   initial_list_status_f  initial_list_status_w  application_type_Individual  \\\n",
       "0                    0.0                    1.0                          1.0   \n",
       "1                    0.0                    1.0                          1.0   \n",
       "2                    1.0                    0.0                          1.0   \n",
       "3                    1.0                    0.0                          1.0   \n",
       "4                    0.0                    1.0                          1.0   \n",
       "\n",
       "   application_type_Joint App  disbursement_method_Cash  \\\n",
       "0                         0.0                       1.0   \n",
       "1                         0.0                       1.0   \n",
       "2                         0.0                       1.0   \n",
       "3                         0.0                       1.0   \n",
       "4                         0.0                       1.0   \n",
       "\n",
       "   disbursement_method_DirectPay  debt_settlement_flag_N  \\\n",
       "0                            0.0                     1.0   \n",
       "1                            0.0                     1.0   \n",
       "2                            0.0                     1.0   \n",
       "3                            0.0                     1.0   \n",
       "4                            0.0                     1.0   \n",
       "\n",
       "   debt_settlement_flag_Y  mths_since_recent_bc_cat_50+  \\\n",
       "0                     0.0                           0.0   \n",
       "1                     0.0                           0.0   \n",
       "2                     0.0                           1.0   \n",
       "3                     0.0                           0.0   \n",
       "4                     0.0                           0.0   \n",
       "\n",
       "   mths_since_recent_bc_cat_<50  mths_since_recent_bc_cat_Not Reported  \\\n",
       "0                           1.0                                    0.0   \n",
       "1                           1.0                                    0.0   \n",
       "2                           0.0                                    0.0   \n",
       "3                           1.0                                    0.0   \n",
       "4                           1.0                                    0.0   \n",
       "\n",
       "   mo_sin_old_il_acct_cat_0-150  mo_sin_old_il_acct_cat_150-200  \\\n",
       "0                           1.0                             0.0   \n",
       "1                           1.0                             0.0   \n",
       "2                           0.0                             1.0   \n",
       "3                           0.0                             1.0   \n",
       "4                           1.0                             0.0   \n",
       "\n",
       "   mo_sin_old_il_acct_cat_200+  mo_sin_old_il_acct_cat_Not Reported  \\\n",
       "0                          0.0                                  0.0   \n",
       "1                          0.0                                  0.0   \n",
       "2                          0.0                                  0.0   \n",
       "3                          0.0                                  0.0   \n",
       "4                          0.0                                  0.0   \n",
       "\n",
       "   mths_since_recent_inq_cat_0-5  mths_since_recent_inq_cat_15+  \\\n",
       "0                            0.0                            0.0   \n",
       "1                            1.0                            0.0   \n",
       "2                            0.0                            0.0   \n",
       "3                            1.0                            0.0   \n",
       "4                            0.0                            0.0   \n",
       "\n",
       "   mths_since_recent_inq_cat_5-15  mths_since_recent_inq_cat_Not Reported  \\\n",
       "0                             1.0                                     0.0   \n",
       "1                             0.0                                     0.0   \n",
       "2                             1.0                                     0.0   \n",
       "3                             0.0                                     0.0   \n",
       "4                             1.0                                     0.0   \n",
       "\n",
       "   mths_since_last_delinq_cat_0-50  mths_since_last_delinq_cat_50-75  \\\n",
       "0                              0.0                               0.0   \n",
       "1                              1.0                               0.0   \n",
       "2                              1.0                               0.0   \n",
       "3                              0.0                               0.0   \n",
       "4                              0.0                               0.0   \n",
       "\n",
       "   mths_since_last_delinq_cat_75+  mths_since_last_delinq_cat_Not Reported  \\\n",
       "0                             0.0                                      1.0   \n",
       "1                             0.0                                      0.0   \n",
       "2                             0.0                                      0.0   \n",
       "3                             0.0                                      1.0   \n",
       "4                             0.0                                      1.0   \n",
       "\n",
       "   mths_since_rcnt_il_cat_50+  mths_since_rcnt_il_cat_<50  \\\n",
       "0                         0.0                         1.0   \n",
       "1                         0.0                         1.0   \n",
       "2                         0.0                         1.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         1.0   \n",
       "\n",
       "   mths_since_rcnt_il_cat_Not Reported  \\\n",
       "0                                  0.0   \n",
       "1                                  0.0   \n",
       "2                                  0.0   \n",
       "3                                  1.0   \n",
       "4                                  0.0   \n",
       "\n",
       "   mths_since_recent_revol_delinq_cat_0-50  \\\n",
       "0                                      0.0   \n",
       "1                                      1.0   \n",
       "2                                      1.0   \n",
       "3                                      0.0   \n",
       "4                                      0.0   \n",
       "\n",
       "   mths_since_recent_revol_delinq_cat_50-75  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       0.0   \n",
       "\n",
       "   mths_since_recent_revol_delinq_cat_75+  \\\n",
       "0                                     0.0   \n",
       "1                                     0.0   \n",
       "2                                     0.0   \n",
       "3                                     0.0   \n",
       "4                                     0.0   \n",
       "\n",
       "   mths_since_recent_revol_delinq_cat_Not Reported  \\\n",
       "0                                              1.0   \n",
       "1                                              0.0   \n",
       "2                                              0.0   \n",
       "3                                              1.0   \n",
       "4                                              1.0   \n",
       "\n",
       "   mths_since_last_major_derog_cat_0-50  \\\n",
       "0                                   0.0   \n",
       "1                                   1.0   \n",
       "2                                   1.0   \n",
       "3                                   0.0   \n",
       "4                                   0.0   \n",
       "\n",
       "   mths_since_last_major_derog_cat_50-75  mths_since_last_major_derog_cat_75+  \\\n",
       "0                                    0.0                                  0.0   \n",
       "1                                    0.0                                  0.0   \n",
       "2                                    0.0                                  0.0   \n",
       "3                                    0.0                                  0.0   \n",
       "4                                    0.0                                  0.0   \n",
       "\n",
       "   mths_since_last_major_derog_cat_Not Reported  \\\n",
       "0                                           1.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           1.0   \n",
       "4                                           1.0   \n",
       "\n",
       "   mths_since_recent_bc_dlq_cat_0-50  mths_since_recent_bc_dlq_cat_50-75  \\\n",
       "0                                0.0                                 0.0   \n",
       "1                                0.0                                 0.0   \n",
       "2                                0.0                                 0.0   \n",
       "3                                0.0                                 0.0   \n",
       "4                                0.0                                 0.0   \n",
       "\n",
       "   mths_since_recent_bc_dlq_cat_75+  \\\n",
       "0                               0.0   \n",
       "1                               0.0   \n",
       "2                               0.0   \n",
       "3                               0.0   \n",
       "4                               0.0   \n",
       "\n",
       "   mths_since_recent_bc_dlq_cat_Not Reported  mths_since_last_record_cat_0-40  \\\n",
       "0                                        1.0                              0.0   \n",
       "1                                        1.0                              0.0   \n",
       "2                                        1.0                              0.0   \n",
       "3                                        1.0                              0.0   \n",
       "4                                        1.0                              0.0   \n",
       "\n",
       "   mths_since_last_record_cat_40-80  mths_since_last_record_cat_80+  \\\n",
       "0                               0.0                             0.0   \n",
       "1                               0.0                             0.0   \n",
       "2                               0.0                             0.0   \n",
       "3                               0.0                             0.0   \n",
       "4                               0.0                             0.0   \n",
       "\n",
       "   mths_since_last_record_cat_Not Reported  sec_app_flag_0  sec_app_flag_1  \\\n",
       "0                                      1.0             1.0             0.0   \n",
       "1                                      1.0             1.0             0.0   \n",
       "2                                      1.0             1.0             0.0   \n",
       "3                                      1.0             1.0             0.0   \n",
       "4                                      1.0             1.0             0.0   \n",
       "\n",
       "   last_credit_pull_month_1  last_credit_pull_month_10  \\\n",
       "0                       0.0                        0.0   \n",
       "1                       0.0                        0.0   \n",
       "2                       0.0                        0.0   \n",
       "3                       0.0                        0.0   \n",
       "4                       0.0                        0.0   \n",
       "\n",
       "   last_credit_pull_month_11  last_credit_pull_month_12  \\\n",
       "0                        0.0                        1.0   \n",
       "1                        0.0                        1.0   \n",
       "2                        0.0                        1.0   \n",
       "3                        0.0                        1.0   \n",
       "4                        0.0                        1.0   \n",
       "\n",
       "   last_credit_pull_month_2  last_credit_pull_month_3  \\\n",
       "0                       0.0                       0.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   last_credit_pull_month_4  last_credit_pull_month_5  \\\n",
       "0                       0.0                       0.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   last_credit_pull_month_6  last_credit_pull_month_7  \\\n",
       "0                       0.0                       0.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   last_credit_pull_month_8  last_credit_pull_month_9  \\\n",
       "0                       0.0                       0.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   last_credit_pull_year_2012  last_credit_pull_year_2013  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   last_credit_pull_year_2014  last_credit_pull_year_2015  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   last_credit_pull_year_2016  last_credit_pull_year_2017  \\\n",
       "0                         0.0                         1.0   \n",
       "1                         0.0                         1.0   \n",
       "2                         0.0                         1.0   \n",
       "3                         0.0                         1.0   \n",
       "4                         0.0                         1.0   \n",
       "\n",
       "   earliest_cr_line_year_1933  earliest_cr_line_year_1934  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1944  earliest_cr_line_year_1945  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1946  earliest_cr_line_year_1948  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1949  earliest_cr_line_year_1950  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1951  earliest_cr_line_year_1952  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1953  earliest_cr_line_year_1954  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1955  earliest_cr_line_year_1956  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1957  earliest_cr_line_year_1958  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1959  earliest_cr_line_year_1960  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1961  earliest_cr_line_year_1962  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1963  earliest_cr_line_year_1964  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1965  earliest_cr_line_year_1966  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1967  earliest_cr_line_year_1968  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1969  earliest_cr_line_year_1970  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1971  earliest_cr_line_year_1972  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1973  earliest_cr_line_year_1974  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1975  earliest_cr_line_year_1976  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1977  earliest_cr_line_year_1978  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1979  earliest_cr_line_year_1980  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1981  earliest_cr_line_year_1982  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1983  earliest_cr_line_year_1984  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1985  earliest_cr_line_year_1986  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1987  earliest_cr_line_year_1988  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1989  earliest_cr_line_year_1990  \\\n",
       "0                         0.0                         1.0   \n",
       "1                         1.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         1.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1991  earliest_cr_line_year_1992  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1993  earliest_cr_line_year_1994  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         1.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1995  earliest_cr_line_year_1996  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1997  earliest_cr_line_year_1998  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_1999  earliest_cr_line_year_2000  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_2001  earliest_cr_line_year_2002  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_2003  earliest_cr_line_year_2004  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_2005  earliest_cr_line_year_2006  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         1.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_2007  earliest_cr_line_year_2008  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_2009  earliest_cr_line_year_2010  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_2011  earliest_cr_line_year_2012  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_year_2013  earliest_cr_line_year_2014  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   earliest_cr_line_month_01  earliest_cr_line_month_02  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        0.0                        0.0   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   earliest_cr_line_month_03  earliest_cr_line_month_04  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        0.0                        0.0   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   earliest_cr_line_month_05  earliest_cr_line_month_06  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        0.0                        0.0   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   earliest_cr_line_month_07  earliest_cr_line_month_08  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        1.0   \n",
       "3                        1.0                        0.0   \n",
       "4                        1.0                        0.0   \n",
       "\n",
       "   earliest_cr_line_month_09  earliest_cr_line_month_10  \\\n",
       "0                        1.0                        0.0   \n",
       "1                        1.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        0.0                        0.0   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   earliest_cr_line_month_11  earliest_cr_line_month_12  term_36  term_60  \\\n",
       "0                        0.0                        0.0      0.0      1.0   \n",
       "1                        0.0                        0.0      0.0      1.0   \n",
       "2                        0.0                        0.0      1.0      0.0   \n",
       "3                        0.0                        0.0      1.0      0.0   \n",
       "4                        0.0                        0.0      1.0      0.0   \n",
       "\n",
       "   sub_grade_1  sub_grade_2  sub_grade_3  sub_grade_4  sub_grade_5  \n",
       "0          0.0          0.0          1.0          0.0          0.0  \n",
       "1          0.0          0.0          0.0          0.0          1.0  \n",
       "2          1.0          0.0          0.0          0.0          0.0  \n",
       "3          0.0          0.0          0.0          0.0          1.0  \n",
       "4          0.0          1.0          0.0          0.0          0.0  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make dataframe of the X space:\n",
    "X_train_df = pd.DataFrame(X_train, columns = col_names)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make sure that int_rate is not in the Xs. \n",
    "[print(i) for i in col_names if 'int_rate' in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import keras\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_overfit_plt(model, plt_param = 'loss'):\n",
    "    \"\"\"\n",
    "    The following function will create a plot comparing training and validation\n",
    "    for a neural network. It will plot either the accuracy or the loss as indicated\n",
    "    in by the plt_param argument. \n",
    "    \n",
    "    \"\"\"\n",
    "    if plt_param == 'loss':\n",
    "        trn = 'loss'\n",
    "        val = 'val_loss'\n",
    "    else:\n",
    "        trn = 'loss'\n",
    "        val = 'val_loss'\n",
    "    \n",
    "    # Get Training and Validation Loss\n",
    "    training = model.history[trn]\n",
    "    validation = model.history[val]\n",
    "    \n",
    "    # Determine the number of epochs\n",
    "    epochs = range(1, len(training) + 1)\n",
    "    \n",
    "    print(epochs)\n",
    "    \n",
    "    # create the plot\n",
    "    plt.plot(epochs, training, 'bo', label='Training '+ plt_param)\n",
    "    plt.plot(epochs, validation, 'b', label='Validation '+ plt_param)\n",
    "    plt.title('Training and validation '+ plt_param)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(plt_param)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def validation_plots(model):\n",
    "    plt.subplots(figsize=(10,4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    NN_overfit_plt(model, plt_param = 'loss')\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     NN_overfit_plt(model, plt_param = 'mean_squared_error')\n",
    "#     plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#New function to create the model. \n",
    "from keras.layers.advanced_activations import PReLU\n",
    "counter=0\n",
    "def iter_ANN_prelu(inputs = X_train.shape[1],\n",
    "             lasso_p = 0,\n",
    "             ridge_p = 0, #0\n",
    "             nodes = 20, #25 best so far\n",
    "             do = [False, 0], #.1 best so far\n",
    "             prelu_layers = 2,\n",
    "             non_prelu_layers = 0, #3 best so far\n",
    "             layer_activation = 'relu',\n",
    "             wgt_init = 'uniform',\n",
    "             output_activation = None, \n",
    "             optimizer = 'adam',\n",
    "             loss = 'mean_squared_error',\n",
    "             metrics = None):\n",
    "#     counter+=1\n",
    "    # Set up Sequential ANN and Input Layer\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nodes, input_shape=(inputs,)))\n",
    "    #Define the 'prelu'\n",
    "    act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\n",
    "    \n",
    "    for i in range(prelu_layers):\n",
    "        model.add(Dense(nodes, kernel_regularizer=regularizers.l1_l2(l1 = lasso_p, l2 = ridge_p), kernel_initializer=wgt_init))\n",
    "        #add PreLU\n",
    "        model.add(act)\n",
    "        if do[0] == True:\n",
    "            model.add(Dropout(do[1]))\n",
    "        \n",
    "    for i in range(non_prelu_layers):                 \n",
    "        model.add(Dense(nodes, kernel_regularizer=regularizers.l1_l2(l1 = lasso_p, l2 = ridge_p), kernel_initializer=wgt_init,\n",
    "                 activation = layer_activation))\n",
    "        if do[0] == True:\n",
    "            model.add(Dropout(do[1]))\n",
    "    \n",
    "    # Set Up Output Layer \n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(1, activation = output_activation))\n",
    "    \n",
    "    model.compile(optimizer = optimizer,\n",
    "                           loss = loss)\n",
    "    print(\"Counter = \"+ str(counter)+\"\\nlasso_p = \"+ str(lasso_p) +\"\\nridge_p = \"+str(ridge_p) + \"\\nnodes = \"+ str(nodes)+ \"\\ndo = \"+ str(do)+\"\\nprelu_layers = \"+str(prelu_layers))\n",
    "    print(\"non_prelu_layers = \" + str(non_prelu_layers) + \"\\nlayer_activation = \" +str(layer_activation))         \n",
    "    print(\"\\nwgt_init = \"+str(wgt_init) + \"\\noutput_activation = \"+str(output_activation)+ \"\\nloss = \"+str(loss)) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thomb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(weights=None, alpha_initializer=\"zero\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter = 0\n",
      "lasso_p = 0\n",
      "ridge_p = 0\n",
      "nodes = 20\n",
      "do = [False, 0]\n",
      "prelu_layers = 2\n",
      "non_prelu_layers = 0\n",
      "layer_activation = relu\n",
      "layer_activation = relu\n",
      "wgt_init = uniform\n",
      "output_activation = None\n",
      "loss = mean_squared_error\n",
      "Train on 1157398 samples, validate on 128600 samples\n",
      "Epoch 1/15\n",
      "1157398/1157398 [==============================] - 51s 44us/step - loss: 7.9767 - val_loss: 4.1397\n",
      "Epoch 2/15\n",
      "1157398/1157398 [==============================] - 41s 35us/step - loss: 2.3353 - val_loss: 1.4005\n",
      "Epoch 3/15\n",
      "1157398/1157398 [==============================] - 38s 33us/step - loss: 1.3237 - val_loss: 1.3560\n",
      "Epoch 4/15\n",
      "1157398/1157398 [==============================] - 37s 32us/step - loss: 0.9407 - val_loss: 0.9502\n",
      "Epoch 5/15\n",
      "1157398/1157398 [==============================] - 50s 43us/step - loss: 0.7772 - val_loss: 0.7535\n",
      "Epoch 6/15\n",
      "1157398/1157398 [==============================] - 50s 44us/step - loss: 0.6967 - val_loss: 0.6639\n",
      "Epoch 7/15\n",
      "1157398/1157398 [==============================] - 50s 43us/step - loss: 0.6234 - val_loss: 0.5274\n",
      "Epoch 8/15\n",
      "1157398/1157398 [==============================] - 57s 49us/step - loss: 0.5806 - val_loss: 0.3866\n",
      "Epoch 9/15\n",
      "1157398/1157398 [==============================] - 61s 53us/step - loss: 0.5460 - val_loss: 0.6078\n",
      "Epoch 10/15\n",
      "1157398/1157398 [==============================] - 59s 51us/step - loss: 0.5069 - val_loss: 0.7893\n",
      "Epoch 11/15\n",
      "1157398/1157398 [==============================] - 49s 42us/step - loss: 0.4855 - val_loss: 0.3311\n",
      "Epoch 12/15\n",
      "1157398/1157398 [==============================] - 49s 42us/step - loss: 0.4681 - val_loss: 0.3931\n",
      "Epoch 13/15\n",
      "1157398/1157398 [==============================] - 59s 51us/step - loss: 0.4419 - val_loss: 0.4567\n",
      "Epoch 14/15\n",
      "1157398/1157398 [==============================] - 55s 48us/step - loss: 0.4173 - val_loss: 0.3039\n",
      "Epoch 15/15\n",
      "1157398/1157398 [==============================] - 46s 40us/step - loss: 0.4045 - val_loss: 0.3606\n"
     ]
    }
   ],
   "source": [
    "#Apply the model. \n",
    "import tensorflow as tf\n",
    "from keras.backend import tensorflow_backend as K\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(\n",
    "                    intra_op_parallelism_threads=6)) as sess:\n",
    "    K.set_session(sess)\n",
    "\n",
    "model_prelu = iter_ANN_prelu()\n",
    "model_prelu_hist = model_prelu.fit(X_train, \n",
    "                       y_train,\n",
    "                       epochs=15,\n",
    "                       batch_size = 128,\n",
    "                       validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98394015031319637"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracing r-squared value for a given model. \n",
    "y_prelu_pred=model_prelu.predict(X_test)\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_prelu_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1157398, 356)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni4jEhBK+iEhMKOGLiMTE/wcppRvQc0NS4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2756a319470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN_overfit_plt(model_prelu_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAEWCAYAAADo5WuiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e8c85970f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_plots(model_prelu_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linreg_preds=linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.56273177874\n",
      "0.752221335886\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_test, linreg_preds))\n",
    "print(r2_score(y_test, linreg_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear regression with grade and sub-grade\n",
    "linreg1 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linreg_preds=linreg1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.477341075079\n",
      "0.978737976481\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_test, linreg_preds))\n",
    "print(r2_score(y_test, linreg_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding iterable functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thomb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(weights=None, alpha_initializer=\"zero\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter = 0\n",
      "lasso_p = 0\n",
      "ridge_p = 0\n",
      "nodes = 20\n",
      "do = [False, 0]\n",
      "prelu_layers = 0\n",
      "non_prelu_layers = 2\n",
      "layer_activation = relu\n",
      "layer_activation = relu\n",
      "wgt_init = uniform\n",
      "output_activation = None\n",
      "loss = mean_squared_error\n",
      "Model, L1 =  64\n",
      "Train on 1157398 samples, validate on 128600 samples\n",
      "Epoch 1/15\n",
      "1157398/1157398 [==============================] - 49s 42us/step - loss: 6.2573 - val_loss: 2.0590\n",
      "Epoch 2/15\n",
      "1157398/1157398 [==============================] - 47s 41us/step - loss: 1.4648 - val_loss: 1.6352\n",
      "Epoch 3/15\n",
      "1157398/1157398 [==============================] - 42s 36us/step - loss: 1.0821 - val_loss: 0.8724\n",
      "Epoch 4/15\n",
      "1157398/1157398 [==============================] - 44s 38us/step - loss: 0.8865 - val_loss: 0.7868\n",
      "Epoch 5/15\n",
      "1157398/1157398 [==============================] - 44s 38us/step - loss: 0.7878 - val_loss: 0.7473\n",
      "Epoch 6/15\n",
      "1157398/1157398 [==============================] - 44s 38us/step - loss: 0.6959 - val_loss: 0.7243\n",
      "Epoch 7/15\n",
      "1157398/1157398 [==============================] - 42s 36us/step - loss: 0.6346 - val_loss: 0.4693\n",
      "Epoch 8/15\n",
      "1157398/1157398 [==============================] - 42s 37us/step - loss: 0.5870 - val_loss: 0.6128\n",
      "Epoch 9/15\n",
      "1157398/1157398 [==============================] - 41s 36us/step - loss: 0.5388 - val_loss: 0.5960\n",
      "Epoch 10/15\n",
      "1157398/1157398 [==============================] - 41s 35us/step - loss: 0.5106 - val_loss: 0.4716\n",
      "Epoch 11/15\n",
      "1157398/1157398 [==============================] - 42s 37us/step - loss: 0.4851 - val_loss: 0.3228\n",
      "Epoch 12/15\n",
      "1157398/1157398 [==============================] - 47s 41us/step - loss: 0.4581 - val_loss: 0.4283\n",
      "Epoch 13/15\n",
      "1157398/1157398 [==============================] - 44s 38us/step - loss: 0.4336 - val_loss: 0.2782\n",
      "Epoch 14/15\n",
      "1157398/1157398 [==============================] - 48s 41us/step - loss: 0.4155 - val_loss: 0.3643\n",
      "Epoch 15/15\n",
      "1157398/1157398 [==============================] - 42s 37us/step - loss: 0.3973 - val_loss: 0.2824\n",
      "0.987419863009\n",
      "range(1, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAEWCAYAAADRgtr5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x275756ab0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Counter = 0\n",
      "lasso_p = 0\n",
      "ridge_p = 0\n",
      "nodes = 20\n",
      "do = [False, 0]\n",
      "prelu_layers = 0\n",
      "non_prelu_layers = 2\n",
      "layer_activation = relu\n",
      "layer_activation = relu\n",
      "wgt_init = uniform\n",
      "output_activation = None\n",
      "loss = mean_squared_error\n",
      "Model, L1 =  128\n",
      "Train on 1157398 samples, validate on 128600 samples\n",
      "Epoch 1/15\n",
      "1157398/1157398 [==============================] - 36s 31us/step - loss: 7.9526 - val_loss: 4.0237\n",
      "Epoch 2/15\n",
      "1157398/1157398 [==============================] - 34s 29us/step - loss: 3.0150 - val_loss: 2.7504\n",
      "Epoch 3/15\n",
      "1157398/1157398 [==============================] - 28s 24us/step - loss: 2.6722 - val_loss: 2.8494\n",
      "Epoch 4/15\n",
      "1157398/1157398 [==============================] - 26s 22us/step - loss: 2.5741 - val_loss: 2.4204\n",
      "Epoch 5/15\n",
      "1157398/1157398 [==============================] - 26s 23us/step - loss: 1.8820 - val_loss: 1.5177\n",
      "Epoch 6/15\n",
      "1157398/1157398 [==============================] - 25s 22us/step - loss: 1.0607 - val_loss: 1.0846\n",
      "Epoch 7/15\n",
      "1157398/1157398 [==============================] - 24s 21us/step - loss: 0.9590 - val_loss: 0.9514\n",
      "Epoch 8/15\n",
      "1157398/1157398 [==============================] - 25s 21us/step - loss: 0.9164 - val_loss: 0.8328\n",
      "Epoch 9/15\n",
      "1157398/1157398 [==============================] - 23s 20us/step - loss: 0.8929 - val_loss: 0.8010\n",
      "Epoch 10/15\n",
      "1157398/1157398 [==============================] - 23s 20us/step - loss: 0.8750 - val_loss: 0.8053\n",
      "Epoch 11/15\n",
      "1157398/1157398 [==============================] - 23s 20us/step - loss: 0.8505 - val_loss: 1.3944\n",
      "Epoch 12/15\n",
      "1157398/1157398 [==============================] - 23s 20us/step - loss: 0.8382 - val_loss: 0.8478\n",
      "Epoch 13/15\n",
      "1157398/1157398 [==============================] - 23s 20us/step - loss: 0.8181 - val_loss: 0.9092\n",
      "Epoch 14/15\n",
      "1157398/1157398 [==============================] - 23s 20us/step - loss: 0.7259 - val_loss: 0.5680\n",
      "Epoch 15/15\n",
      "1157398/1157398 [==============================] - 23s 20us/step - loss: 0.6221 - val_loss: 0.6357\n",
      "0.971682588603\n",
      "range(1, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAEWCAYAAADRgtr5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27566aeb208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Counter = 0\n",
      "lasso_p = 0\n",
      "ridge_p = 0\n",
      "nodes = 20\n",
      "do = [False, 0]\n",
      "prelu_layers = 0\n",
      "non_prelu_layers = 2\n",
      "layer_activation = relu\n",
      "layer_activation = relu\n",
      "wgt_init = uniform\n",
      "output_activation = None\n",
      "loss = mean_squared_error\n",
      "Model, L1 =  256\n",
      "Train on 1157398 samples, validate on 128600 samples\n",
      "Epoch 1/15\n",
      "1157398/1157398 [==============================] - 15s 13us/step - loss: 9.9445 - val_loss: 4.8434\n",
      "Epoch 2/15\n",
      "1157398/1157398 [==============================] - 20s 17us/step - loss: 2.8585 - val_loss: 2.3414\n",
      "Epoch 3/15\n",
      "1157398/1157398 [==============================] - 18s 16us/step - loss: 1.2792 - val_loss: 1.8257\n",
      "Epoch 4/15\n",
      "1157398/1157398 [==============================] - 19s 16us/step - loss: 0.8964 - val_loss: 0.7637\n",
      "Epoch 5/15\n",
      "1157398/1157398 [==============================] - 19s 16us/step - loss: 0.7301 - val_loss: 1.8205\n",
      "Epoch 6/15\n",
      "1157398/1157398 [==============================] - 19s 16us/step - loss: 0.6546 - val_loss: 0.4694\n",
      "Epoch 7/15\n",
      "1157398/1157398 [==============================] - 19s 16us/step - loss: 0.6068 - val_loss: 0.5338\n",
      "Epoch 8/15\n",
      "1157398/1157398 [==============================] - 19s 17us/step - loss: 0.5629 - val_loss: 0.4956\n",
      "Epoch 9/15\n",
      "1157398/1157398 [==============================] - 19s 17us/step - loss: 0.5238 - val_loss: 0.6366\n",
      "Epoch 10/15\n",
      "1157398/1157398 [==============================] - 20s 17us/step - loss: 0.5161 - val_loss: 0.6141\n",
      "Epoch 11/15\n",
      "1157398/1157398 [==============================] - 20s 18us/step - loss: 0.4935 - val_loss: 0.3883\n",
      "Epoch 12/15\n",
      "1157398/1157398 [==============================] - 21s 18us/step - loss: 0.4693 - val_loss: 0.3320\n",
      "Epoch 13/15\n",
      "1157398/1157398 [==============================] - 20s 17us/step - loss: 0.4516 - val_loss: 0.5537\n",
      "Epoch 14/15\n",
      "1157398/1157398 [==============================] - 15s 13us/step - loss: 0.4445 - val_loss: 0.5062\n",
      "Epoch 15/15\n",
      "1157398/1157398 [==============================] - 15s 13us/step - loss: 0.4216 - val_loss: 0.4023\n",
      "0.982078398186\n",
      "range(1, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEWCAYAAAAOzKDmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNEtixphAsyRmjAm0/weyQZ6o6uhumgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2756a22f780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Counter = 0\n",
      "lasso_p = 0\n",
      "ridge_p = 0\n",
      "nodes = 20\n",
      "do = [False, 0]\n",
      "prelu_layers = 0\n",
      "non_prelu_layers = 2\n",
      "layer_activation = relu\n",
      "layer_activation = relu\n",
      "wgt_init = uniform\n",
      "output_activation = None\n",
      "loss = mean_squared_error\n",
      "Model, L1 =  512\n",
      "Train on 1157398 samples, validate on 128600 samples\n",
      "Epoch 1/15\n",
      "1157398/1157398 [==============================] - 15s 13us/step - loss: 13.8073 - val_loss: 7.7252\n",
      "Epoch 2/15\n",
      "1157398/1157398 [==============================] - 15s 13us/step - loss: 5.5986 - val_loss: 4.0683\n",
      "Epoch 3/15\n",
      "1157398/1157398 [==============================] - 17s 15us/step - loss: 2.7895 - val_loss: 1.5087\n",
      "Epoch 4/15\n",
      "1157398/1157398 [==============================] - 19s 16us/step - loss: 1.2491 - val_loss: 1.0202\n",
      "Epoch 5/15\n",
      "1157398/1157398 [==============================] - 19s 16us/step - loss: 0.9296 - val_loss: 0.8497\n",
      "Epoch 6/15\n",
      "1157398/1157398 [==============================] - 19s 17us/step - loss: 0.7852 - val_loss: 0.6620\n",
      "Epoch 7/15\n",
      "1157398/1157398 [==============================] - 18s 15us/step - loss: 0.6651 - val_loss: 0.6944\n",
      "Epoch 8/15\n",
      "1157398/1157398 [==============================] - 17s 15us/step - loss: 0.5829 - val_loss: 0.5570\n",
      "Epoch 9/15\n",
      "1157398/1157398 [==============================] - 21s 18us/step - loss: 0.5373 - val_loss: 0.4714\n",
      "Epoch 10/15\n",
      "1157398/1157398 [==============================] - 22s 19us/step - loss: 0.4852 - val_loss: 0.4242\n",
      "Epoch 11/15\n",
      "1157398/1157398 [==============================] - 17s 15us/step - loss: 0.4513 - val_loss: 0.3443\n",
      "Epoch 12/15\n",
      "1157398/1157398 [==============================] - 17s 15us/step - loss: 0.4265 - val_loss: 0.3745\n",
      "Epoch 13/15\n",
      "1157398/1157398 [==============================] - 19s 17us/step - loss: 0.4210 - val_loss: 0.3525\n",
      "Epoch 14/15\n",
      "1157398/1157398 [==============================] - 19s 17us/step - loss: 0.3960 - val_loss: 1.2085\n",
      "Epoch 15/15\n",
      "1157398/1157398 [==============================] - 23s 20us/step - loss: 0.3812 - val_loss: 0.3189\n",
      "0.985795857652\n",
      "range(1, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEWCAYAAAAOzKDmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/X8Ab7wC46URcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2756a30a208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adding Steve's iterable thingy:\n",
    "\n",
    "model_dict = {}\n",
    "\n",
    "l1 = [0, 0, 0, 0]\n",
    "l2 = [0, 0, 0, 0]\n",
    "DO = [[False, 0],[False, 0],[False, 0],[False, 0]]\n",
    "batch_size = [64, 128, 256, 512]\n",
    "\n",
    "model_list = []\n",
    "for i, value in enumerate(batch_size):\n",
    "    model = iter_ANN_prelu(inputs = X_train.shape[1],\n",
    "             lasso_p = 0,\n",
    "             ridge_p = 0,\n",
    "             nodes = 20,\n",
    "             do = [False, 0],\n",
    "             prelu_layers = 0,\n",
    "             non_prelu_layers = 2,\n",
    "             layer_activation = 'relu',\n",
    "             wgt_init = 'uniform',\n",
    "             output_activation = None,\n",
    "             optimizer = 'adam',\n",
    "             loss = 'mean_squared_error',\n",
    "             metrics = None)\n",
    "    \n",
    "    print('Model, L1 = ',value)\n",
    "    model_dict['model' + str(i)] = model.fit(X_train, \n",
    "                                             y_train,\n",
    "                                             epochs=15,\n",
    "                                             batch_size = value,  #128 is the best so far...\n",
    "                                             validation_data=(X_test, y_test))\n",
    "    from sklearn.metrics import r2_score\n",
    "    y_prelu_pred=model.predict(X_test)\n",
    "    model_dict['model'+ str(i)+'_R2'] = r2_score(y_test, y_prelu_pred)\n",
    "    model_list.append('model'+str(i))\n",
    "    print(r2_score(y_test, y_prelu_pred))\n",
    "    validation_plots(model_dict['model' + str(i)])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How to save a model\n",
    "# model.save('model1.h5')\n",
    "# from keras.models import load_model\n",
    "# model = load_model('model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a dataframe of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>DO</th>\n",
       "      <th>MIN_Loss</th>\n",
       "      <th>MAX_loss</th>\n",
       "      <th>DIFF</th>\n",
       "      <th>LOW_EPOCH</th>\n",
       "      <th>R_Squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.278204</td>\n",
       "      <td>2.059027</td>\n",
       "      <td>1.780823</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.987420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.568007</td>\n",
       "      <td>4.023720</td>\n",
       "      <td>3.455713</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.971683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.332025</td>\n",
       "      <td>4.843430</td>\n",
       "      <td>4.511406</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.982078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318889</td>\n",
       "      <td>7.725208</td>\n",
       "      <td>7.406319</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.985796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    l1   l2   DO  MIN_Loss  MAX_loss      DIFF  LOW_EPOCH  R_Squared\n",
       "0  0.0  0.0  0.0  0.278204  2.059027  1.780823       12.0   0.987420\n",
       "1  0.0  0.0  0.0  0.568007  4.023720  3.455713       13.0   0.971683\n",
       "2  0.0  0.0  0.0  0.332025  4.843430  4.511406       11.0   0.982078\n",
       "3  0.0  0.0  0.0  0.318889  7.725208  7.406319       14.0   0.985796"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out the best model:\n",
    "\n",
    "for i, key in enumerate(model_list):\n",
    "    l1_val =  l1[i]\n",
    "    l2_val =  l2[i]\n",
    "    do_val =  DO[i][1]\n",
    "    \n",
    "    #  Validation Accuracy\n",
    "    min_val = min(model_dict[key].history['val_loss'])\n",
    "    max_val = max(model_dict[key].history['val_loss'])\n",
    "    \n",
    "    dif = max_val - min_val\n",
    "    high_epoch = np.argmin(np.array(model_dict[key].history['val_loss']))\n",
    "\n",
    "    if i == 0:\n",
    "        vals = np.array([[l1_val, l2_val, do_val,min_val, max_val, dif, high_epoch, model_dict['model'+ str(i)+'_R2']]])\n",
    "    else:\n",
    "        vals = np.vstack((vals,np.array([[l1_val, l2_val, do_val,min_val, max_val, dif, high_epoch, model_dict['model'+ str(i)+'_R2']]])))\n",
    "    \n",
    "cols = ['l1', 'l2','DO','MIN_Loss','MAX_loss','DIFF','LOW_EPOCH', 'R_Squared']\n",
    "nn_metrics = pd.DataFrame(vals, columns = cols)\n",
    "nn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>DO</th>\n",
       "      <th>MIN_Loss</th>\n",
       "      <th>MAX_loss</th>\n",
       "      <th>DIFF</th>\n",
       "      <th>LOW_EPOCH</th>\n",
       "      <th>R_Squared</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.278204</td>\n",
       "      <td>2.059027</td>\n",
       "      <td>1.780823</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.987420</td>\n",
       "      <td>0.527450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568007</td>\n",
       "      <td>4.023720</td>\n",
       "      <td>3.455713</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.971683</td>\n",
       "      <td>0.753663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332025</td>\n",
       "      <td>4.843430</td>\n",
       "      <td>4.511406</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.982078</td>\n",
       "      <td>0.576216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.318889</td>\n",
       "      <td>7.725208</td>\n",
       "      <td>7.406319</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.985796</td>\n",
       "      <td>0.564702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    l1   l2  DO  MIN_Loss  MAX_loss      DIFF  LOW_EPOCH  R_Squared      RMSE\n",
       "0  0.0  0.0   0  0.278204  2.059027  1.780823       12.0   0.987420  0.527450\n",
       "1  0.0  0.0   0  0.568007  4.023720  3.455713       13.0   0.971683  0.753663\n",
       "2  0.0  0.0   0  0.332025  4.843430  4.511406       11.0   0.982078  0.576216\n",
       "3  0.0  0.0   0  0.318889  7.725208  7.406319       14.0   0.985796  0.564702"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_metrics=pd.DataFrame(nn_metrics)\n",
    "nn_metrics['RMSE']=[np.sqrt(nn_metrics['MIN_Loss'][0]), np.sqrt(nn_metrics['MIN_Loss'][1]), np.sqrt(nn_metrics['MIN_Loss'][2]), np.sqrt(nn_metrics['MIN_Loss'][3])]\n",
    "nn_metrics['DO'] = 0\n",
    "nn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>DO</th>\n",
       "      <th>MIN_Loss</th>\n",
       "      <th>MAX_loss</th>\n",
       "      <th>DIFF</th>\n",
       "      <th>LOW_EPOCH</th>\n",
       "      <th>R_Squared</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Batch_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.278204</td>\n",
       "      <td>2.059027</td>\n",
       "      <td>1.780823</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.987420</td>\n",
       "      <td>0.527450</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568007</td>\n",
       "      <td>4.023720</td>\n",
       "      <td>3.455713</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.971683</td>\n",
       "      <td>0.753663</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.332025</td>\n",
       "      <td>4.843430</td>\n",
       "      <td>4.511406</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.982078</td>\n",
       "      <td>0.576216</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.318889</td>\n",
       "      <td>7.725208</td>\n",
       "      <td>7.406319</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.985796</td>\n",
       "      <td>0.564702</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    l1   l2  DO  MIN_Loss  MAX_loss      DIFF  LOW_EPOCH  R_Squared      RMSE  \\\n",
       "0  0.0  0.0   0  0.278204  2.059027  1.780823       12.0   0.987420  0.527450   \n",
       "1  0.0  0.0   0  0.568007  4.023720  3.455713       13.0   0.971683  0.753663   \n",
       "2  0.0  0.0   0  0.332025  4.843430  4.511406       11.0   0.982078  0.576216   \n",
       "3  0.0  0.0   0  0.318889  7.725208  7.406319       14.0   0.985796  0.564702   \n",
       "\n",
       "   Batch_Size  \n",
       "0          64  \n",
       "1         128  \n",
       "2         256  \n",
       "3         512  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_metrics=pd.DataFrame(nn_metrics)\n",
    "nn_metrics['Batch_Size']=[64, 128, 256, 512]\n",
    "# nn_metrics['DO'] = 0\n",
    "nn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "l1              0.000000\n",
       "l2              0.000000\n",
       "DO              0.000000\n",
       "MIN_Loss        0.310210\n",
       "MAX_loss        7.668468\n",
       "DIFF            7.358258\n",
       "LOW_EPOCH      13.000000\n",
       "R_Squared       0.982506\n",
       "RMSE            0.556965\n",
       "Batch_Size    512.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEWCAYAAAAOzKDmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27566c50e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________________________________________________________________________________\n",
      "\n",
      "Model 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "l1              0.000000\n",
       "l2              0.000000\n",
       "DO              0.100000\n",
       "MIN_Loss        2.524531\n",
       "MAX_loss        7.854281\n",
       "DIFF            5.329749\n",
       "LOW_EPOCH       9.000000\n",
       "R_Squared       0.884302\n",
       "RMSE            1.588877\n",
       "Batch_Size    512.000000\n",
       "Name: 1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEWCAYAAAAOzKDmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27568b81c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________________________________________________________________________________\n",
      "\n",
      "Model 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "l1              0.000000\n",
       "l2              0.000000\n",
       "DO              0.250000\n",
       "MIN_Loss        1.639495\n",
       "MAX_loss        8.192676\n",
       "DIFF            6.553180\n",
       "LOW_EPOCH      12.000000\n",
       "R_Squared       0.921853\n",
       "RMSE            1.280428\n",
       "Batch_Size    512.000000\n",
       "Name: 2, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEWCAYAAAAOzKDmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1hRiIhJr/x9APdnfnMx/mAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27566f56cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________________________________________________________________________________\n",
      "\n",
      "Model 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "l1              0.000000\n",
       "l2              0.000000\n",
       "DO              0.500000\n",
       "MIN_Loss        4.822556\n",
       "MAX_loss       10.591718\n",
       "DIFF            5.769162\n",
       "LOW_EPOCH       5.000000\n",
       "R_Squared       0.770980\n",
       "RMSE            2.196032\n",
       "Batch_Size    512.000000\n",
       "Name: 3, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEWCAYAAAAOzKDmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27569f61b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_____________________________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(model_list):\n",
    "    print('Model '+str(i))\n",
    "    display(nn_metrics.loc[i])\n",
    "    validation_plots(model_dict['model' + str(i)])\n",
    "    print('\\n_____________________________________________________________________________________\\n')\n",
    "\n",
    "# validation_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Grid Search for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting up the grid search#\n",
    "regressor = KerasRegressor(build_fn = iter_ANN_prelu, batch_size=512, epochs=10)\n",
    "\n",
    "layer_activation=['relu']\n",
    "prelu_layers = [1,2,3]\n",
    "non_prelu_layers = [0,1,2]\n",
    "do=[[False, 0], [True, 0.1],[True, 0.3]]\n",
    "\n",
    "# wgt_constraint=[1,2,3,4,5]\n",
    "nodes=[20, 30]#1\n",
    "wgt_init=['uniform','normal', 'zero']\n",
    "#loss\n",
    "optimizer=['adam']\n",
    "loss=['mean_squared_error'] \n",
    "\n",
    "\n",
    "\n",
    "param_grid=dict(layer_activation=layer_activation, prelu_layers = prelu_layers, non_prelu_layers = non_prelu_layers, do=do, \n",
    "             nodes=nodes, wgt_init=wgt_init, \n",
    "               optimizer=optimizer, loss=loss)\n",
    "parameters = param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thomb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `PReLU` call to the Keras 2 API: `PReLU(weights=None, alpha_initializer=\"zero\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 26s 33us/step - loss: 12.4573 - val_loss: 2.3378\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 12s 15us/step - loss: 1.7799 - val_loss: 1.4031\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 12s 15us/step - loss: 1.1870 - val_loss: 1.3501\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 12s 15us/step - loss: 0.9418 - val_loss: 0.8732\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 12s 15us/step - loss: 0.8143 - val_loss: 1.1038\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 12s 15us/step - loss: 0.7469 - val_loss: 0.7325\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.7028 - val_loss: 0.7163\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 0.6734 - val_loss: 0.6665\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 16s 20us/step - loss: 0.6520 - val_loss: 0.6447\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 12s 15us/step - loss: 0.6362 - val_loss: 0.6301\n",
      "385800/385800 [==============================] - 4s 11us/step\n",
      "771598/771598 [==============================] - 10s 13us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 10.4204 - val_loss: 2.3024\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 1.7741 - val_loss: 1.4156\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 10s 14us/step - loss: 1.1923 - val_loss: 1.0252\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.9249 - val_loss: 0.8702\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 12s 15us/step - loss: 0.7999 - val_loss: 0.8274\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.7344 - val_loss: 0.7295\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.6932 - val_loss: 0.7135\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.6660 - val_loss: 0.6682\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 11s 15us/step - loss: 0.6453 - val_loss: 0.6440\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.6312 - val_loss: 0.6337\n",
      "385799/385799 [==============================] - 3s 9us/step\n",
      "771599/771599 [==============================] - 7s 9us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 11.0706 - val_loss: 2.1942\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 1.6839 - val_loss: 1.3347\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 1.0918 - val_loss: 0.8981\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.7342 - val_loss: 0.6504\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.5592 - val_loss: 0.5357\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 0.4690 - val_loss: 0.4688\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.4151 - val_loss: 0.4042\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.3795 - val_loss: 0.3694\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 12s 15us/step - loss: 0.3553 - val_loss: 0.3579\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.3357 - val_loss: 0.4259\n",
      "385799/385799 [==============================] - 4s 10us/step\n",
      "771599/771599 [==============================] - 7s 9us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 9.9968 - val_loss: 2.0843\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 11s 14us/step - loss: 1.4664 - val_loss: 1.0870\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 11s 14us/step - loss: 0.9222 - val_loss: 0.8629\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 11s 14us/step - loss: 0.7953 - val_loss: 0.8275\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 11s 14us/step - loss: 0.7295 - val_loss: 0.7204\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 11s 15us/step - loss: 0.6881 - val_loss: 0.7098\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.6602 - val_loss: 0.6512\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 11s 14us/step - loss: 0.6404 - val_loss: 0.6380\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 11s 15us/step - loss: 0.6251 - val_loss: 0.6894\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.6129 - val_loss: 0.6588\n",
      "385800/385800 [==============================] - 4s 9us/step\n",
      "771598/771598 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 11.6604 - val_loss: 2.2485\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 1.7207 - val_loss: 1.3574\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 1.1530 - val_loss: 1.0001\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.9075 - val_loss: 0.8377\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 12s 15us/step - loss: 0.7875 - val_loss: 0.7509\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.7232 - val_loss: 0.7074\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.6834 - val_loss: 0.6709\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.6578 - val_loss: 0.6600\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.6388 - val_loss: 0.6503\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.6244 - val_loss: 0.6287\n",
      "385799/385799 [==============================] - 4s 9us/step\n",
      "771599/771599 [==============================] - 7s 9us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 10.8869 - val_loss: 2.2900\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 1.6563 - val_loss: 1.2455\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 1.0474 - val_loss: 0.9250\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.8490 - val_loss: 0.8034\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.7549 - val_loss: 0.7360\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.7032 - val_loss: 0.7848\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 10s 14us/step - loss: 0.6706 - val_loss: 0.6603\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 10s 14us/step - loss: 0.6469 - val_loss: 0.6733\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 10s 14us/step - loss: 0.6302 - val_loss: 0.6496\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 0.6179 - val_loss: 0.6220\n",
      "385799/385799 [==============================] - 3s 9us/step\n",
      "771599/771599 [==============================] - 7s 9us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 177.8556 - val_loss: 160.0051\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 11s 14us/step - loss: 143.9772 - val_loss: 128.7587\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 11s 14us/step - loss: 115.0952 - val_loss: 102.1523\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 90.6171 - val_loss: 79.7690\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 70.2588 - val_loss: 61.4209\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 53.8585 - val_loss: 46.9499\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 41.2591 - val_loss: 36.1934\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 32.2526 - val_loss: 28.8940\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 26.5326 - val_loss: 24.6784\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 23.5996 - val_loss: 22.8915\n",
      "385800/385800 [==============================] - 4s 11us/step\n",
      "771598/771598 [==============================] - 8s 10us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 177.7884 - val_loss: 160.0013\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 143.9129 - val_loss: 128.7580\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 115.0302 - val_loss: 102.1515\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 90.5520 - val_loss: 79.7697\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 70.1930 - val_loss: 61.4174\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 53.7906 - val_loss: 46.9462\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 41.1902 - val_loss: 36.1899\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 11s 15us/step - loss: 32.1875 - val_loss: 28.8957\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 26.4709 - val_loss: 24.6826\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 11s 14us/step - loss: 23.5395 - val_loss: 22.8965\n",
      "385799/385799 [==============================] - 4s 9us/step\n",
      "771599/771599 [==============================] - 7s 9us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 178.1275 - val_loss: 160.0076\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 144.2158 - val_loss: 128.7602\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 115.3037 - val_loss: 102.1472\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 90.7909 - val_loss: 79.7617\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 70.4103 - val_loss: 61.4152\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 53.9847 - val_loss: 46.9466\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 11s 15us/step - loss: 41.3607 - val_loss: 36.1872\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 11s 15us/step - loss: 32.3246 - val_loss: 28.8841\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 11s 15us/step - loss: 26.5838 - val_loss: 24.6704\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 11s 15us/step - loss: 23.6316 - val_loss: 22.8850\n",
      "385799/385799 [==============================] - 3s 9us/step\n",
      "771599/771599 [==============================] - 7s 9us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 13.2552 - val_loss: 3.4467\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 2.5176 - val_loss: 1.7918\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 1.4375 - val_loss: 1.2350\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 1.0985 - val_loss: 1.0331\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 0.9906 - val_loss: 0.9615\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 0.9260 - val_loss: 0.9083\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.8781 - val_loss: 0.8611\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.8423 - val_loss: 0.8314\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 0.8151 - val_loss: 0.8060\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.7926 - val_loss: 0.7928\n",
      "385800/385800 [==============================] - 4s 10us/step\n",
      "771598/771598 [==============================] - 8s 10us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 12.9967 - val_loss: 3.6409\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 2.6000 - val_loss: 1.6743\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 1.3046 - val_loss: 1.1502\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 1.0806 - val_loss: 1.0343\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.9838 - val_loss: 0.9530\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 0.9170 - val_loss: 0.8965\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 0.8687 - val_loss: 0.8566\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 18s 24us/step - loss: 0.8326 - val_loss: 0.8206\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 18s 24us/step - loss: 0.8053 - val_loss: 0.8347\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.7833 - val_loss: 0.7775\n",
      "385799/385799 [==============================] - 6s 15us/step\n",
      "771599/771599 [==============================] - 9s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 13.8601 - val_loss: 3.7213\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 2.7071 - val_loss: 1.8371\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 1.4185 - val_loss: 1.1910\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 1.0875 - val_loss: 1.0384\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 0.9925 - val_loss: 0.9670\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 0.9242 - val_loss: 0.9009\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8754 - val_loss: 0.8792\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 0.8386 - val_loss: 0.8277\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8096 - val_loss: 0.8156\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.7879 - val_loss: 0.7826\n",
      "385799/385799 [==============================] - 4s 10us/step\n",
      "771599/771599 [==============================] - 8s 10us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 11.4839 - val_loss: 2.7937\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 1.9015 - val_loss: 1.4471\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 1.2893 - val_loss: 1.1976\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 1.1145 - val_loss: 1.0809\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 16s 21us/step - loss: 0.9940 - val_loss: 0.9424\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 0.8704 - val_loss: 0.8452\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771598/771598 [==============================] - 13s 17us/step - loss: 0.8282 - val_loss: 0.8209\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 0.8014 - val_loss: 0.7943\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 0.7802 - val_loss: 0.7778\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 0.7620 - val_loss: 0.7733\n",
      "385800/385800 [==============================] - 4s 12us/step\n",
      "771598/771598 [==============================] - 9s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 12.3258 - val_loss: 3.4561\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 2.4035 - val_loss: 1.7573\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.3799 - val_loss: 1.1857\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.1171 - val_loss: 1.0758\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.0165 - val_loss: 0.9823\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 0.9452 - val_loss: 0.9584\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 0.8932 - val_loss: 0.9000\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 0.8530 - val_loss: 0.8404\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 0.8227 - val_loss: 0.8207\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 0.7981 - val_loss: 0.9006\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 9s 12us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 19s 24us/step - loss: 12.3349 - val_loss: 3.1853\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 2.4355 - val_loss: 1.8812\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.4358 - val_loss: 1.1557\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.0926 - val_loss: 1.0834\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 0.9955 - val_loss: 0.9722\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 0.9286 - val_loss: 0.9060\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 0.8820 - val_loss: 0.8658\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8449 - val_loss: 0.8340\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8169 - val_loss: 0.8128\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 0.7939 - val_loss: 0.8045\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 8s 10us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 19s 24us/step - loss: 177.8457 - val_loss: 159.9931\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 143.9673 - val_loss: 128.7530\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 115.0837 - val_loss: 102.1391\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 90.6125 - val_loss: 79.7690\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 70.2617 - val_loss: 61.4211\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 53.8583 - val_loss: 46.9478\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 41.2544 - val_loss: 36.1887\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 32.2508 - val_loss: 28.8943\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 26.5326 - val_loss: 24.6786\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 23.5943 - val_loss: 22.8871\n",
      "385800/385800 [==============================] - 4s 11us/step\n",
      "771598/771598 [==============================] - 9s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 177.7803 - val_loss: 159.9899\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 143.9028 - val_loss: 128.7505\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 115.0208 - val_loss: 102.1417\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 90.5461 - val_loss: 79.7638\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 70.1928 - val_loss: 61.4157\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 53.7947 - val_loss: 46.9502\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 41.1939 - val_loss: 36.1912\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 32.1832 - val_loss: 28.8913\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 26.4661 - val_loss: 24.6775\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 23.5332 - val_loss: 22.8915\n",
      "385799/385799 [==============================] - 4s 10us/step\n",
      "771599/771599 [==============================] - 8s 10us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 178.1282 - val_loss: 160.0109\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 144.2253 - val_loss: 128.7688\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 115.3083 - val_loss: 102.1505\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 90.8013 - val_loss: 79.7703\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 70.4160 - val_loss: 61.4168\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 53.9824 - val_loss: 46.9414\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 41.3550 - val_loss: 36.1827\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 32.3247 - val_loss: 28.8866\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 26.5863 - val_loss: 24.6743\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 23.6371 - val_loss: 22.8894\n",
      "385799/385799 [==============================] - 4s 10us/step\n",
      "771599/771599 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 19s 24us/step - loss: 15.7247 - val_loss: 4.0286\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 3.0838 - val_loss: 2.3464\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 1.9089 - val_loss: 1.5927\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 16s 21us/step - loss: 1.4435 - val_loss: 1.3514\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 1.2644 - val_loss: 1.2161\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 1.1352 - val_loss: 1.1599\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 1.0462 - val_loss: 1.0391\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 1.0069 - val_loss: 0.9966\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 0.9775 - val_loss: 0.9767\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 16s 20us/step - loss: 0.9526 - val_loss: 0.9606\n",
      "385800/385800 [==============================] - 5s 14us/step\n",
      "771598/771598 [==============================] - 10s 13us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 20.7178 - val_loss: 4.0768\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 3.4201 - val_loss: 2.6938\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 2.1050 - val_loss: 1.7988\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.6252 - val_loss: 1.4550\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.3085 - val_loss: 1.2453\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 1.1729 - val_loss: 1.1128\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 1.0464 - val_loss: 1.0228\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 0.9995 - val_loss: 0.9882\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 0.9709 - val_loss: 0.9812\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 0.9471 - val_loss: 0.9701\n",
      "385799/385799 [==============================] - 4s 10us/step\n",
      "771599/771599 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 15.4320 - val_loss: 4.3380\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 3.2291 - val_loss: 2.2465\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.7832 - val_loss: 1.5492\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.4024 - val_loss: 1.3144\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.2624 - val_loss: 1.2230\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.1823 - val_loss: 1.1552\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.1139 - val_loss: 1.1856\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.0413 - val_loss: 0.9990\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 0.9733 - val_loss: 0.9864\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 0.9470 - val_loss: 0.9405\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 16s 21us/step - loss: 16.0627 - val_loss: 3.6555\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 2.9193 - val_loss: 2.2934\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 1.9616 - val_loss: 1.7780\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 1.5975 - val_loss: 1.4728\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 1.3520 - val_loss: 1.2859\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 1.1845 - val_loss: 1.1463\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 1.1097 - val_loss: 1.0819\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 1.0415 - val_loss: 1.0188\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 0.9685 - val_loss: 0.9561\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 0.9393 - val_loss: 0.9375\n",
      "385800/385800 [==============================] - 4s 11us/step\n",
      "771598/771598 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 17s 21us/step - loss: 13.6658 - val_loss: 3.9424\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 3.1509 - val_loss: 2.4610\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.9583 - val_loss: 1.5716\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.3928 - val_loss: 1.2573\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.1687 - val_loss: 1.1216\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.0897 - val_loss: 1.0788\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.0441 - val_loss: 1.0292\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.0071 - val_loss: 0.9954\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 0.9765 - val_loss: 0.9696\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 0.9525 - val_loss: 0.9442\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 18.7265 - val_loss: 4.2477\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 3.3654 - val_loss: 2.6500\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 2.1339 - val_loss: 1.7616\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.5822 - val_loss: 1.4321\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.3286 - val_loss: 1.2657\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.2131 - val_loss: 1.1829\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.1467 - val_loss: 1.1338\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.0905 - val_loss: 1.0669\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.0334 - val_loss: 1.0045\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 0.9630 - val_loss: 0.9768\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 177.8564 - val_loss: 160.0010\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 143.9798 - val_loss: 128.7634\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 115.0944 - val_loss: 102.1521\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 90.6177 - val_loss: 79.7718\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 70.2609 - val_loss: 61.4210\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 53.8625 - val_loss: 46.9529\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 41.2561 - val_loss: 36.1865\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 32.2515 - val_loss: 28.8981\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 26.5365 - val_loss: 24.6808\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 23.5982 - val_loss: 22.8899\n",
      "385800/385800 [==============================] - 4s 11us/step\n",
      "771598/771598 [==============================] - 9s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 177.7896 - val_loss: 160.0002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 143.9115 - val_loss: 128.7597\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 115.0265 - val_loss: 102.1459\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 90.5479 - val_loss: 79.7656\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 70.1931 - val_loss: 61.4211\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 53.7941 - val_loss: 46.9487\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 41.1932 - val_loss: 36.1921\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 32.1894 - val_loss: 28.8969\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 26.4710 - val_loss: 24.6823\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 23.5323 - val_loss: 22.8891\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 9s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 178.1272 - val_loss: 160.0142\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 144.2229 - val_loss: 128.7655\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 115.3022 - val_loss: 102.1447\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 90.7969 - val_loss: 79.7682\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 70.4139 - val_loss: 61.4183\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 53.9878 - val_loss: 46.9496\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 41.3603 - val_loss: 36.1850\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 32.3225 - val_loss: 28.8837\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 26.5814 - val_loss: 24.6691\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 23.6318 - val_loss: 22.8866\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 9s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 12.0124 - val_loss: 3.3654\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 2.5244 - val_loss: 1.8852\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 1.4307 - val_loss: 1.1452\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 1.0608 - val_loss: 1.0225\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.9769 - val_loss: 0.9510\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.9158 - val_loss: 0.8925\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.8706 - val_loss: 0.8561\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.8360 - val_loss: 0.8559\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.8095 - val_loss: 0.8029\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.7880 - val_loss: 0.8165\n",
      "385800/385800 [==============================] - 4s 10us/step\n",
      "771598/771598 [==============================] - 8s 10us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 12.2573 - val_loss: 3.6143\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 2.6843 - val_loss: 1.8290\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 1.3662 - val_loss: 1.1280\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 1.0674 - val_loss: 1.0297\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.9799 - val_loss: 0.9600\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.9157 - val_loss: 0.9035\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8697 - val_loss: 0.8666\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8352 - val_loss: 0.8480\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8089 - val_loss: 0.8122\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.7873 - val_loss: 0.7897\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 12.6881 - val_loss: 3.1484\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 2.4170 - val_loss: 1.7619\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 1.3299 - val_loss: 1.1056\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 1.0420 - val_loss: 1.0079\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.9579 - val_loss: 0.9271\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8975 - val_loss: 0.8860\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8541 - val_loss: 0.8687\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8225 - val_loss: 0.8167\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.7969 - val_loss: 0.8982\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.7790 - val_loss: 0.7731\n",
      "385799/385799 [==============================] - 4s 10us/step\n",
      "771599/771599 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 12.9351 - val_loss: 2.7724\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 1.9658 - val_loss: 1.4174\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 1.1435 - val_loss: 0.9521\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.8479 - val_loss: 0.7791\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.7180 - val_loss: 0.6819\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.6398 - val_loss: 0.6124\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.5855 - val_loss: 0.5649\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.5450 - val_loss: 0.5291\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.5134 - val_loss: 0.5220\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 12s 16us/step - loss: 0.4873 - val_loss: 0.4774\n",
      "385800/385800 [==============================] - 4s 11us/step\n",
      "771598/771598 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 13.2683 - val_loss: 3.6557\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 2.6739 - val_loss: 1.9601\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 1.5637 - val_loss: 1.3158\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 1.1520 - val_loss: 1.0407\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 13s 17us/step - loss: 0.9920 - val_loss: 0.9752\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.9245 - val_loss: 0.9109\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8766 - val_loss: 0.8671\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8405 - val_loss: 0.8785\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8125 - val_loss: 0.8058\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.7895 - val_loss: 0.7893\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 12.4652 - val_loss: 2.9634\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 2.1231 - val_loss: 1.4369\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 1.1970 - val_loss: 1.1035\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 1.0447 - val_loss: 1.0068\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.9591 - val_loss: 0.9286\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8999 - val_loss: 0.8787\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8573 - val_loss: 0.8623\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.8255 - val_loss: 0.8220\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.7999 - val_loss: 0.7944\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 0.7795 - val_loss: 0.7915\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 16s 20us/step - loss: 177.8461 - val_loss: 159.9932\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 143.9647 - val_loss: 128.7460\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 115.0810 - val_loss: 102.1428\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 90.6083 - val_loss: 79.7618\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 13s 17us/step - loss: 70.2507 - val_loss: 61.4119\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 53.8524 - val_loss: 46.9427\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 41.2521 - val_loss: 36.1869\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 32.2480 - val_loss: 28.8947\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 26.5340 - val_loss: 24.6795\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 13s 16us/step - loss: 23.5992 - val_loss: 22.8911\n",
      "385800/385800 [==============================] - 4s 11us/step\n",
      "771598/771598 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 177.7939 - val_loss: 160.0031\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 143.9132 - val_loss: 128.7584\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 115.0219 - val_loss: 102.1444\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 90.5508 - val_loss: 79.7694\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 70.1981 - val_loss: 61.4221\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 53.7979 - val_loss: 46.9539\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 41.1979 - val_loss: 36.1954\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 32.1925 - val_loss: 28.9010\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 26.4731 - val_loss: 24.6820\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 23.5361 - val_loss: 22.8914\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 178.1139 - val_loss: 159.9957\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 144.2067 - val_loss: 128.7525\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 115.2939 - val_loss: 102.1376\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 90.7878 - val_loss: 79.7587\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 70.4077 - val_loss: 61.4112\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 53.9789 - val_loss: 46.9372\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 41.3502 - val_loss: 36.1794\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 13s 16us/step - loss: 32.3251 - val_loss: 28.8872\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 26.5861 - val_loss: 24.6743\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 12s 16us/step - loss: 23.6333 - val_loss: 22.8856\n",
      "385799/385799 [==============================] - 4s 11us/step\n",
      "771599/771599 [==============================] - 8s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 17.3450 - val_loss: 4.0245\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 3.2695 - val_loss: 2.6360\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 2.0659 - val_loss: 1.6750\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 1.5364 - val_loss: 1.4391\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 1.3543 - val_loss: 1.2980\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 1.2240 - val_loss: 1.1663\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 1.1176 - val_loss: 1.0850\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 1.0436 - val_loss: 1.0074\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 0.9736 - val_loss: 0.9820\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 14s 18us/step - loss: 0.9476 - val_loss: 0.9458\n",
      "385800/385800 [==============================] - 4s 11us/step\n",
      "771598/771598 [==============================] - 9s 11us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 17.9678 - val_loss: 4.1871\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 3.3149 - val_loss: 2.4862\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.9747 - val_loss: 1.6950\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 14s 18us/step - loss: 1.5293 - val_loss: 1.3796\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 1.2857 - val_loss: 1.2491\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.1773 - val_loss: 1.1620\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.1227 - val_loss: 1.1136\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 18s 23us/step - loss: 1.0800 - val_loss: 1.0811\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.0445 - val_loss: 1.0329\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.0135 - val_loss: 1.0367\n",
      "385799/385799 [==============================] - 6s 14us/step\n",
      "771599/771599 [==============================] - 11s 15us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 21s 28us/step - loss: 18.1144 - val_loss: 3.8035\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 3.0725 - val_loss: 2.4782\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 2.0164 - val_loss: 1.6629\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.4836 - val_loss: 1.3778\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.2963 - val_loss: 1.2286\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.1865 - val_loss: 1.1577\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.1280 - val_loss: 1.1063\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.0796 - val_loss: 1.0605\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.0329 - val_loss: 1.0235\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 0.9729 - val_loss: 0.9415\n",
      "385799/385799 [==============================] - 5s 14us/step\n",
      "771599/771599 [==============================] - 11s 14us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 14.7695 - val_loss: 4.1359\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 3.2122 - val_loss: 2.3558\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 1.8632 - val_loss: 1.6154\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 19s 24us/step - loss: 1.4676 - val_loss: 1.3508\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.2520 - val_loss: 1.1564\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 16s 21us/step - loss: 1.1038 - val_loss: 1.0833\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 16s 21us/step - loss: 1.0503 - val_loss: 1.0396\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 16s 21us/step - loss: 1.0117 - val_loss: 1.0028\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 16s 21us/step - loss: 0.9818 - val_loss: 0.9723\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 16s 21us/step - loss: 0.9564 - val_loss: 0.9494\n",
      "385800/385800 [==============================] - 5s 14us/step\n",
      "771598/771598 [==============================] - 11s 14us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 15.1443 - val_loss: 3.8788\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 3.0549 - val_loss: 2.4496\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.9478 - val_loss: 1.6019\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.4374 - val_loss: 1.3622\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 17s 23us/step - loss: 1.2993 - val_loss: 1.2632\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 19s 24us/step - loss: 1.2163 - val_loss: 1.1968\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 19s 24us/step - loss: 1.1530 - val_loss: 1.1288\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.1031 - val_loss: 1.1385\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 18s 24us/step - loss: 1.0634 - val_loss: 1.0486\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 1.0298 - val_loss: 1.0211\n",
      "385799/385799 [==============================] - 5s 14us/step\n",
      "771599/771599 [==============================] - 13s 16us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 14.2068 - val_loss: 4.2642\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 3.1887 - val_loss: 2.3509\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.8591 - val_loss: 1.5778\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.4757 - val_loss: 1.4182\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.3459 - val_loss: 1.2983\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 19s 24us/step - loss: 1.2396 - val_loss: 1.1877\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.1314 - val_loss: 1.1002\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.0590 - val_loss: 1.0263\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 19s 24us/step - loss: 0.9857 - val_loss: 0.9732\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 0.9484 - val_loss: 0.9581\n",
      "385799/385799 [==============================] - 6s 16us/step\n",
      "771599/771599 [==============================] - 13s 16us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 25s 32us/step - loss: 177.8503 - val_loss: 159.9985\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 143.9747 - val_loss: 128.7591\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 115.0914 - val_loss: 102.1462\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 90.6143 - val_loss: 79.7701\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 70.2610 - val_loss: 61.4182\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 53.8569 - val_loss: 46.9505\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 41.2611 - val_loss: 36.1956\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 32.2567 - val_loss: 28.8989\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 26.5365 - val_loss: 24.6810\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 23.5992 - val_loss: 22.8918\n",
      "385800/385800 [==============================] - 7s 19us/step\n",
      "771598/771598 [==============================] - 14s 18us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 177.7793 - val_loss: 159.9882\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 21s 28us/step - loss: 143.9030 - val_loss: 128.7505\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 115.0173 - val_loss: 102.1366\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 90.5462 - val_loss: 79.7665\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 21s 28us/step - loss: 70.1959 - val_loss: 61.4181\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 53.7923 - val_loss: 46.9475\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 41.1950 - val_loss: 36.1946\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 32.1897 - val_loss: 28.8999\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 26.4705 - val_loss: 24.6792\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 23.5334 - val_loss: 22.8903\n",
      "385799/385799 [==============================] - 7s 18us/step\n",
      "771599/771599 [==============================] - 12s 16us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 178.1198 - val_loss: 160.0011\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 144.2132 - val_loss: 128.7584\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 115.3036 - val_loss: 102.1475\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 90.7949 - val_loss: 79.7620\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 70.4066 - val_loss: 61.4103\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 53.9825 - val_loss: 46.9438\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 41.3528 - val_loss: 36.1806\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 32.3277 - val_loss: 28.8914\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 26.5865 - val_loss: 24.6717\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 23.6330 - val_loss: 22.8851\n",
      "385799/385799 [==============================] - 7s 17us/step\n",
      "771599/771599 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 20.6638 - val_loss: 4.6944\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 4.0231 - val_loss: 3.3571\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 2.6050 - val_loss: 2.0052\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 1.8328 - val_loss: 1.6929\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 1.5458 - val_loss: 1.4464\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 1.3932 - val_loss: 1.3502\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 1.3149 - val_loss: 1.2928\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 1.2458 - val_loss: 1.2164\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 1.1898 - val_loss: 1.1819\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 1.1545 - val_loss: 1.1426\n",
      "385800/385800 [==============================] - 7s 18us/step\n",
      "771598/771598 [==============================] - 14s 18us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 26s 34us/step - loss: 20.8799 - val_loss: 5.0903\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 22s 28us/step - loss: 4.1672 - val_loss: 3.3481\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 2.6463 - val_loss: 2.1476\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 21s 28us/step - loss: 1.9058 - val_loss: 1.7153\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 1.6156 - val_loss: 1.5548\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 21s 28us/step - loss: 1.4764 - val_loss: 1.4059\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 21s 28us/step - loss: 1.3621 - val_loss: 1.3328\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 22s 28us/step - loss: 1.2889 - val_loss: 1.2717\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 1.2380 - val_loss: 1.2398\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 22s 28us/step - loss: 1.1966 - val_loss: 1.1778\n",
      "385799/385799 [==============================] - 7s 18us/step\n",
      "771599/771599 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 24.1647 - val_loss: 4.0220\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 22s 29us/step - loss: 3.6049 - val_loss: 3.1415\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 22s 28us/step - loss: 2.7037 - val_loss: 2.2977\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 22s 29us/step - loss: 1.9818 - val_loss: 1.6978\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 21s 28us/step - loss: 1.5453 - val_loss: 1.4761\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 22s 28us/step - loss: 1.4281 - val_loss: 1.3978\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 22s 28us/step - loss: 1.3444 - val_loss: 1.3277\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 21s 28us/step - loss: 1.2864 - val_loss: 1.2889\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 21s 28us/step - loss: 1.2427 - val_loss: 1.2337\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 21s 28us/step - loss: 1.2076 - val_loss: 1.1973\n",
      "385799/385799 [==============================] - 7s 18us/step\n",
      "771599/771599 [==============================] - 14s 18us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 17.0636 - val_loss: 4.9090\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 3.5754 - val_loss: 2.7347\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 22s 29us/step - loss: 2.3438 - val_loss: 2.2365\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 1.9595 - val_loss: 1.8249\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 1.6424 - val_loss: 1.4652\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 1.4153 - val_loss: 1.3866\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 1.3521 - val_loss: 1.3311\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 1.3022 - val_loss: 1.2806\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 23s 29us/step - loss: 1.2572 - val_loss: 1.2413\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 23s 30us/step - loss: 1.2080 - val_loss: 1.2065\n",
      "385800/385800 [==============================] - 7s 19us/step\n",
      "771598/771598 [==============================] - 15s 19us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 27s 34us/step - loss: 16.6158 - val_loss: 4.3853\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 3.2212 - val_loss: 2.5098\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 2.1577 - val_loss: 1.9057\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 1.7366 - val_loss: 1.6392\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.5296 - val_loss: 1.4590\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 1.3730 - val_loss: 1.3130\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 1.2677 - val_loss: 1.2435\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 1.2162 - val_loss: 1.2096\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.1709 - val_loss: 1.1507\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.1168 - val_loss: 1.1353\n",
      "385799/385799 [==============================] - 6s 16us/step\n",
      "771599/771599 [==============================] - 12s 16us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 18.8783 - val_loss: 4.6503\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 3.9586 - val_loss: 3.3061\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 20s 26us/step - loss: 2.6226 - val_loss: 2.0619\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.8335 - val_loss: 1.6401\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 1.5111 - val_loss: 1.4390\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.3886 - val_loss: 1.3551\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.3001 - val_loss: 1.2701\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.2296 - val_loss: 1.2137\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.1649 - val_loss: 1.1454\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.1252 - val_loss: 1.1172\n",
      "385799/385799 [==============================] - 7s 17us/step\n",
      "771599/771599 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 25s 33us/step - loss: 177.8672 - val_loss: 160.0188\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 143.9897 - val_loss: 128.7698\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 115.1029 - val_loss: 102.1603\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 90.6245 - val_loss: 79.7745\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 70.2642 - val_loss: 61.4241\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 53.8623 - val_loss: 46.9521\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 41.2629 - val_loss: 36.1948\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 32.2578 - val_loss: 28.9005\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 26.5344 - val_loss: 24.6771\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 23.5969 - val_loss: 22.8890\n",
      "385800/385800 [==============================] - 6s 15us/step\n",
      "771598/771598 [==============================] - 11s 15us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 22s 29us/step - loss: 177.7923 - val_loss: 160.0025\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 143.9109 - val_loss: 128.7578\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 18s 24us/step - loss: 115.0286 - val_loss: 102.1453\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 90.5492 - val_loss: 79.7680\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 18s 24us/step - loss: 70.1962 - val_loss: 61.4214\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 18s 24us/step - loss: 53.8001 - val_loss: 46.9554\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 18s 24us/step - loss: 41.1960 - val_loss: 36.1920\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 32.1913 - val_loss: 28.8985\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 26.4693 - val_loss: 24.6802\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 23.5365 - val_loss: 22.8920\n",
      "385799/385799 [==============================] - 5s 14us/step\n",
      "771599/771599 [==============================] - 11s 14us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 178.1196 - val_loss: 160.0020\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 144.2110 - val_loss: 128.7567\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 115.2967 - val_loss: 102.1413\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 90.7906 - val_loss: 79.7613\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 70.4099 - val_loss: 61.4136\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 53.9810 - val_loss: 46.9414\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 41.3531 - val_loss: 36.1832\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 32.3225 - val_loss: 28.8845\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 26.5841 - val_loss: 24.6715\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 23.6313 - val_loss: 22.8832\n",
      "385799/385799 [==============================] - 5s 13us/step\n",
      "771599/771599 [==============================] - 10s 13us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 16.4112 - val_loss: 3.8680\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 3.2278 - val_loss: 2.6075\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 2.0039 - val_loss: 1.6280\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 1.4473 - val_loss: 1.3132\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 1.2126 - val_loss: 1.1401\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 1.0962 - val_loss: 1.0802\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 1.0453 - val_loss: 1.0312\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 1.0083 - val_loss: 1.0150\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 0.9787 - val_loss: 0.9774\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 15s 19us/step - loss: 0.9540 - val_loss: 0.9450\n",
      "385800/385800 [==============================] - 5s 13us/step\n",
      "771598/771598 [==============================] - 11s 14us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 16.5280 - val_loss: 3.9930\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 3.2069 - val_loss: 2.5686\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 2.1108 - val_loss: 1.7366\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 1.5269 - val_loss: 1.3884\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 1.3105 - val_loss: 1.3335\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 1.2132 - val_loss: 1.1657\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 1.1211 - val_loss: 1.0986\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 1.0742 - val_loss: 1.0573\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.0364 - val_loss: 1.0913\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 15s 19us/step - loss: 1.0026 - val_loss: 0.9887\n",
      "385799/385799 [==============================] - 5s 13us/step\n",
      "771599/771599 [==============================] - 10s 13us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 16.8106 - val_loss: 3.9510\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 3.2063 - val_loss: 2.4823\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 2.0528 - val_loss: 1.8307\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.4853 - val_loss: 1.2808\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.1931 - val_loss: 1.1744\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.0884 - val_loss: 1.0898\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.0411 - val_loss: 1.0256\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.0051 - val_loss: 1.0014\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 0.9751 - val_loss: 0.9685\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 0.9505 - val_loss: 0.9629\n",
      "385799/385799 [==============================] - 5s 13us/step\n",
      "771599/771599 [==============================] - 10s 13us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 20s 25us/step - loss: 17.1295 - val_loss: 3.6748\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 2.9537 - val_loss: 2.3944\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 2.0458 - val_loss: 1.7785\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 1.6650 - val_loss: 1.5645\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 1.4007 - val_loss: 1.3197\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 1.2546 - val_loss: 1.1918\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 1.1369 - val_loss: 1.0984\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 1.0688 - val_loss: 1.0519\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 1.0295 - val_loss: 1.0130\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 0.9894 - val_loss: 1.0079\n",
      "385800/385800 [==============================] - 5s 13us/step\n",
      "771598/771598 [==============================] - 10s 13us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 16.5827 - val_loss: 3.9639\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 3.0856 - val_loss: 2.4007\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 2.0979 - val_loss: 1.7763\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 1.5423 - val_loss: 1.4070\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 1.3330 - val_loss: 1.2849\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 1.2285 - val_loss: 1.1796\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 1.1371 - val_loss: 1.1269\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 1.0914 - val_loss: 1.0822\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 1.0552 - val_loss: 1.0447\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 1.0233 - val_loss: 1.0140\n",
      "385799/385799 [==============================] - 5s 14us/step\n",
      "771599/771599 [==============================] - 11s 14us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 14.5125 - val_loss: 3.7216\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 2.8571 - val_loss: 2.0910\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.6954 - val_loss: 1.5514\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.4662 - val_loss: 1.4183\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.3203 - val_loss: 1.3121\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.2215 - val_loss: 1.2312\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.1451 - val_loss: 1.1146\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.0744 - val_loss: 1.0552\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 1.0269 - val_loss: 1.0045\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 0.9702 - val_loss: 0.9808\n",
      "385799/385799 [==============================] - 5s 13us/step\n",
      "771599/771599 [==============================] - 10s 14us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 177.8557 - val_loss: 160.0011\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 143.9778 - val_loss: 128.7630\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 115.0929 - val_loss: 102.1494\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 90.6153 - val_loss: 79.7694\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 70.2621 - val_loss: 61.4236\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 53.8616 - val_loss: 46.9499\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 41.2535 - val_loss: 36.1867\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 32.2505 - val_loss: 28.8966\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 26.5346 - val_loss: 24.6799\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 15s 20us/step - loss: 23.5991 - val_loss: 22.8911\n",
      "385800/385800 [==============================] - 5s 13us/step\n",
      "771598/771598 [==============================] - 10s 13us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 177.8153 - val_loss: 160.0287\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 143.9330 - val_loss: 128.7755\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 115.0395 - val_loss: 102.1582\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 90.5614 - val_loss: 79.7798\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 70.2056 - val_loss: 61.4274\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 53.8032 - val_loss: 46.9574\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 41.1963 - val_loss: 36.1921\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 32.1887 - val_loss: 28.8977\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 26.4713 - val_loss: 24.6793\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 23.5336 - val_loss: 22.8921\n",
      "385799/385799 [==============================] - 5s 14us/step\n",
      "771599/771599 [==============================] - 11s 14us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 178.1309 - val_loss: 160.0109\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 144.2259 - val_loss: 128.7715\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 115.3140 - val_loss: 102.1541\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 90.8077 - val_loss: 79.7814\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 70.4227 - val_loss: 61.4210\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 53.9887 - val_loss: 46.9483\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 41.3565 - val_loss: 36.1806\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 15s 20us/step - loss: 32.3226 - val_loss: 28.8838\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 15s 20us/step - loss: 26.5851 - val_loss: 24.6730\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 16s 20us/step - loss: 23.6340 - val_loss: 22.8861\n",
      "385799/385799 [==============================] - 5s 14us/step\n",
      "771599/771599 [==============================] - 11s 14us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 20.4865 - val_loss: 5.3773\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 4.3340 - val_loss: 3.1130\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 2.3381 - val_loss: 1.8887\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.6872 - val_loss: 1.6018\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.5359 - val_loss: 1.4856\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.4462 - val_loss: 1.4230\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.3785 - val_loss: 1.3481\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.3078 - val_loss: 1.2711\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.2464 - val_loss: 1.2324\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.2086 - val_loss: 1.1953\n",
      "385800/385800 [==============================] - 5s 14us/step\n",
      "771598/771598 [==============================] - 11s 14us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 22s 28us/step - loss: 22.8191 - val_loss: 4.7229\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 4.0566 - val_loss: 3.3995\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 2.8725 - val_loss: 2.3774\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.9386 - val_loss: 1.5968\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.5077 - val_loss: 1.4785\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.4187 - val_loss: 1.3867\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.3273 - val_loss: 1.3419\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.2519 - val_loss: 1.2501\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.1962 - val_loss: 1.1870\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.1640 - val_loss: 1.1569\n",
      "385799/385799 [==============================] - 6s 15us/step\n",
      "771599/771599 [==============================] - 11s 15us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 23s 29us/step - loss: 20.4513 - val_loss: 5.0708\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 18s 24us/step - loss: 4.2338 - val_loss: 3.2988\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 2.3712 - val_loss: 1.9227\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 1.7383 - val_loss: 1.7091\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 17s 23us/step - loss: 1.6234 - val_loss: 1.5858\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 1.5180 - val_loss: 1.4618\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 17s 23us/step - loss: 1.3822 - val_loss: 1.3344\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 17s 23us/step - loss: 1.3014 - val_loss: 1.2929\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 17s 23us/step - loss: 1.2275 - val_loss: 1.1941\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 17s 23us/step - loss: 1.1483 - val_loss: 1.1377\n",
      "385799/385799 [==============================] - 6s 15us/step\n",
      "771599/771599 [==============================] - 12s 15us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 23s 29us/step - loss: 20.9444 - val_loss: 4.6926\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 4.0306 - val_loss: 3.2275\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 2.4508 - val_loss: 1.9969\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.7162 - val_loss: 1.5952\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.5207 - val_loss: 1.4650\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.4090 - val_loss: 1.3627\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.3137 - val_loss: 1.2809\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.2514 - val_loss: 1.2330\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.2116 - val_loss: 1.2301\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 17s 22us/step - loss: 1.1774 - val_loss: 1.1653\n",
      "385800/385800 [==============================] - 6s 15us/step\n",
      "771598/771598 [==============================] - 11s 15us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 22s 28us/step - loss: 19.1726 - val_loss: 4.8456\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 3.8931 - val_loss: 3.1140\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 2.3712 - val_loss: 1.9684\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.7409 - val_loss: 1.5559\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.4881 - val_loss: 1.4485\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.4121 - val_loss: 1.3831\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.3504 - val_loss: 1.3246\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.2962 - val_loss: 1.2722\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.2420 - val_loss: 1.2220\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 16s 21us/step - loss: 1.1856 - val_loss: 1.1719\n",
      "385799/385799 [==============================] - 5s 13us/step\n",
      "771599/771599 [==============================] - 10s 13us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 23s 29us/step - loss: 18.3236 - val_loss: 4.9347\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 3.7937 - val_loss: 2.9247\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 2.3491 - val_loss: 1.9191\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.6941 - val_loss: 1.5330\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 17s 22us/step - loss: 1.4459 - val_loss: 1.3895\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 17s 23us/step - loss: 1.3456 - val_loss: 1.3253\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 17s 23us/step - loss: 1.2657 - val_loss: 1.2604\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 17s 23us/step - loss: 1.2023 - val_loss: 1.1862\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 17s 23us/step - loss: 1.1649 - val_loss: 1.1603\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 17s 23us/step - loss: 1.1328 - val_loss: 1.1209\n",
      "385799/385799 [==============================] - 6s 15us/step\n",
      "771599/771599 [==============================] - 11s 15us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 23s 30us/step - loss: 177.8541 - val_loss: 160.0022\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 18s 23us/step - loss: 143.9802 - val_loss: 128.7644\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 18s 23us/step - loss: 115.1003 - val_loss: 102.1551\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 18s 23us/step - loss: 90.6171 - val_loss: 79.7664\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 18s 23us/step - loss: 70.2583 - val_loss: 61.4220\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 18s 23us/step - loss: 53.8610 - val_loss: 46.9537\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 18s 23us/step - loss: 41.2587 - val_loss: 36.1887\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 18s 23us/step - loss: 32.2526 - val_loss: 28.8970\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 18s 23us/step - loss: 26.5375 - val_loss: 24.6818\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 18s 23us/step - loss: 23.5997 - val_loss: 22.8909\n",
      "385800/385800 [==============================] - 6s 15us/step\n",
      "771598/771598 [==============================] - 12s 15us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 177.7986 - val_loss: 160.0093\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 143.9201 - val_loss: 128.7659\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 18s 24us/step - loss: 115.0327 - val_loss: 102.1505\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 90.5502 - val_loss: 79.7670\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 70.1921 - val_loss: 61.4160\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 53.7922 - val_loss: 46.9503\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 41.1956 - val_loss: 36.1931\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 32.1893 - val_loss: 28.8991\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 26.4726 - val_loss: 24.6837\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 23.5386 - val_loss: 22.8945\n",
      "385799/385799 [==============================] - 6s 15us/step\n",
      "771599/771599 [==============================] - 12s 15us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 178.1131 - val_loss: 159.9904\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 144.2101 - val_loss: 128.7588\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 115.2971 - val_loss: 102.1410\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 90.7887 - val_loss: 79.7591\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 70.4084 - val_loss: 61.4118\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 53.9819 - val_loss: 46.9420\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 41.3511 - val_loss: 36.1787\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 32.3217 - val_loss: 28.8830\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 26.5829 - val_loss: 24.6711\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 18s 23us/step - loss: 23.6326 - val_loss: 22.8852\n",
      "385799/385799 [==============================] - 6s 15us/step\n",
      "771599/771599 [==============================] - 12s 15us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 25s 33us/step - loss: 26.2007 - val_loss: 4.7935\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 20s 25us/step - loss: 4.0124 - val_loss: 3.3952\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 3.1512 - val_loss: 2.9618\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 2.6680 - val_loss: 2.2840\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 2.0544 - val_loss: 1.9320\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 1.8238 - val_loss: 1.7277\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 1.5932 - val_loss: 1.4877\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 20s 25us/step - loss: 1.4445 - val_loss: 1.4245\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 1.3824 - val_loss: 1.3491\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 20s 25us/step - loss: 1.3221 - val_loss: 1.3110\n",
      "385800/385800 [==============================] - 6s 16us/step\n",
      "771598/771598 [==============================] - 12s 16us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 24.5437 - val_loss: 4.4205\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 3.7917 - val_loss: 3.3068\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 3.0358 - val_loss: 2.7908\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 2.4176 - val_loss: 2.1699\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 1.9842 - val_loss: 1.8461\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.7154 - val_loss: 1.7420\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.5470 - val_loss: 1.4868\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.4383 - val_loss: 1.4152\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.3903 - val_loss: 1.3717\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.3463 - val_loss: 1.3355\n",
      "385799/385799 [==============================] - 6s 16us/step\n",
      "771599/771599 [==============================] - 12s 16us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 23.2649 - val_loss: 4.7292\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 4.1735 - val_loss: 3.7492\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 3.2314 - val_loss: 2.6713\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 2.2521 - val_loss: 2.0414\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.9450 - val_loss: 1.8530\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 1.7207 - val_loss: 1.6209\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 1.5724 - val_loss: 1.5803\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.4860 - val_loss: 1.4429\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 1.4036 - val_loss: 1.4011\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 20s 25us/step - loss: 1.3367 - val_loss: 1.3017\n",
      "385799/385799 [==============================] - 6s 16us/step\n",
      "771599/771599 [==============================] - 13s 16us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 26s 33us/step - loss: 23.4801 - val_loss: 5.2168\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 4.3129 - val_loss: 3.4645\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 20s 25us/step - loss: 2.7525 - val_loss: 2.2859\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771598/771598 [==============================] - 20s 26us/step - loss: 2.0477 - val_loss: 1.8379\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 1.6879 - val_loss: 1.6399\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 1.5663 - val_loss: 1.5264\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 1.4817 - val_loss: 1.4884\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 1.4217 - val_loss: 1.4027\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 1.3620 - val_loss: 1.3733\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 1.3018 - val_loss: 1.2869\n",
      "385800/385800 [==============================] - 6s 17us/step\n",
      "771598/771598 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 26s 33us/step - loss: 23.6051 - val_loss: 5.2045\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 4.4880 - val_loss: 3.6712\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 2.7774 - val_loss: 2.1364\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.9489 - val_loss: 1.9939\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.7427 - val_loss: 1.7808\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.6136 - val_loss: 1.5802\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.5233 - val_loss: 1.5038\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.4289 - val_loss: 1.3779\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.3425 - val_loss: 1.3401\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.3068 - val_loss: 1.3012\n",
      "385799/385799 [==============================] - 6s 17us/step\n",
      "771599/771599 [==============================] - 13s 16us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 26s 34us/step - loss: 22.4418 - val_loss: 5.3959\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 4.4357 - val_loss: 3.5581\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 2.8740 - val_loss: 2.3953\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 2.1008 - val_loss: 1.9769\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.8790 - val_loss: 1.7863\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.7013 - val_loss: 1.6491\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.5800 - val_loss: 1.5481\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.4761 - val_loss: 1.4354\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.3810 - val_loss: 1.3546\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.3206 - val_loss: 1.3031\n",
      "385799/385799 [==============================] - 6s 17us/step\n",
      "771599/771599 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 27s 34us/step - loss: 177.8520 - val_loss: 159.9961\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 143.9725 - val_loss: 128.7555\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 115.0916 - val_loss: 102.1477\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 90.6161 - val_loss: 79.7689\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 70.2570 - val_loss: 61.4164\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 53.8555 - val_loss: 46.9488\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 41.2517 - val_loss: 36.1846\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 32.2480 - val_loss: 28.8914\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 26.5324 - val_loss: 24.6794\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 21s 27us/step - loss: 23.5974 - val_loss: 22.8877\n",
      "385800/385800 [==============================] - 7s 17us/step\n",
      "771598/771598 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 26s 34us/step - loss: 177.7910 - val_loss: 160.0054\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 143.9163 - val_loss: 128.7632\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 115.0315 - val_loss: 102.1503\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 90.5477 - val_loss: 79.7655\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 70.1932 - val_loss: 61.4182\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 53.7952 - val_loss: 46.9522\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 41.1982 - val_loss: 36.1968\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 32.1937 - val_loss: 28.9007\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 26.4735 - val_loss: 24.6826\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 23.5332 - val_loss: 22.8893\n",
      "385799/385799 [==============================] - 7s 17us/step\n",
      "771599/771599 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 26s 34us/step - loss: 178.1273 - val_loss: 160.0122\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 144.2218 - val_loss: 128.7673\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 115.3106 - val_loss: 102.1563\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 90.8014 - val_loss: 79.7726\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 70.4143 - val_loss: 61.4174\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 53.9841 - val_loss: 46.9434\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 41.3558 - val_loss: 36.1813\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 32.3246 - val_loss: 28.8881\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 26.5847 - val_loss: 24.6697\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 23.6338 - val_loss: 22.8881\n",
      "385799/385799 [==============================] - 7s 17us/step\n",
      "771599/771599 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 25s 33us/step - loss: 10.3286 - val_loss: 2.4807\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 1.8082 - val_loss: 1.3483\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 1.1276 - val_loss: 0.9768\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 0.8856 - val_loss: 0.9033\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 0.7857 - val_loss: 0.7723\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 0.7294 - val_loss: 0.7095\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 0.6940 - val_loss: 0.6929\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 0.6674 - val_loss: 0.6695\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 0.6482 - val_loss: 0.6894\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 19s 25us/step - loss: 0.6334 - val_loss: 0.6487\n",
      "385800/385800 [==============================] - 7s 17us/step\n",
      "771598/771598 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 9.6019 - val_loss: 1.9769\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.3206 - val_loss: 1.0348\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.9293 - val_loss: 0.8725\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.8056 - val_loss: 0.8054\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.7399 - val_loss: 0.7219\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.6979 - val_loss: 0.6838\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.6691 - val_loss: 0.6753\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.6481 - val_loss: 0.6540\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.6329 - val_loss: 0.6287\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.6203 - val_loss: 0.6430\n",
      "385799/385799 [==============================] - 7s 17us/step\n",
      "771599/771599 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 26s 33us/step - loss: 10.6211 - val_loss: 2.4857\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.7422 - val_loss: 1.2897\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 1.0851 - val_loss: 0.9748\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.8729 - val_loss: 0.8161\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.7776 - val_loss: 0.7517\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.7238 - val_loss: 0.7115\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.6892 - val_loss: 0.6789\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.6645 - val_loss: 0.6616\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.6459 - val_loss: 0.6456\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 19s 25us/step - loss: 0.6306 - val_loss: 0.6299\n",
      "385799/385799 [==============================] - 7s 17us/step\n",
      "771599/771599 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 10.1467 - val_loss: 2.4591\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 1.7703 - val_loss: 1.3187\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 1.0964 - val_loss: 0.9589\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 0.8834 - val_loss: 0.8630\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 0.7890 - val_loss: 0.7794\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 0.7328 - val_loss: 0.7306\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 0.6946 - val_loss: 0.6810\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 0.6672 - val_loss: 0.6688\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 0.6483 - val_loss: 0.6479\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 0.6325 - val_loss: 0.6392\n",
      "385800/385800 [==============================] - 7s 17us/step\n",
      "771598/771598 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 26s 34us/step - loss: 9.2796 - val_loss: 2.2919\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.5600 - val_loss: 1.0981\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.8278 - val_loss: 0.6569\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.5652 - val_loss: 0.5175\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.4620 - val_loss: 0.4506\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.4059 - val_loss: 0.4197\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.3689 - val_loss: 0.3623\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.3435 - val_loss: 0.3376\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.3251 - val_loss: 0.3238\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.3099 - val_loss: 0.3069\n",
      "385799/385799 [==============================] - 7s 17us/step\n",
      "771599/771599 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 26s 34us/step - loss: 8.9462 - val_loss: 2.2769\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 1.5127 - val_loss: 1.0199\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.8162 - val_loss: 0.6826\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.5959 - val_loss: 0.5671\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.4932 - val_loss: 0.4980\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.4290 - val_loss: 0.4198\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.3862 - val_loss: 0.3777\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.3567 - val_loss: 0.3580\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.3352 - val_loss: 0.3534\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 0.3188 - val_loss: 0.3237\n",
      "385799/385799 [==============================] - 7s 18us/step\n",
      "771599/771599 [==============================] - 14s 18us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 177.8584 - val_loss: 160.0004\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 143.9772 - val_loss: 128.7583\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 115.0910 - val_loss: 102.1474\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 90.6129 - val_loss: 79.7687\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 70.2583 - val_loss: 61.4189\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 53.8600 - val_loss: 46.9525\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 41.2595 - val_loss: 36.1935\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 32.2552 - val_loss: 28.8962\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 26.5360 - val_loss: 24.6815\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 20s 26us/step - loss: 23.6014 - val_loss: 22.8930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385800/385800 [==============================] - 7s 17us/step\n",
      "771598/771598 [==============================] - 13s 17us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 26s 34us/step - loss: 177.7852 - val_loss: 159.9972\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 143.9080 - val_loss: 128.7536\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 115.0218 - val_loss: 102.1429\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 90.5446 - val_loss: 79.7648\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 70.1920 - val_loss: 61.4158\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 53.7934 - val_loss: 46.9525\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 41.1918 - val_loss: 36.1865\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 32.1864 - val_loss: 28.8969\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 26.4694 - val_loss: 24.6797\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 20s 26us/step - loss: 23.5336 - val_loss: 22.8909\n",
      "385799/385799 [==============================] - 7s 18us/step\n",
      "771599/771599 [==============================] - 14s 18us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 27s 36us/step - loss: 178.1251 - val_loss: 160.0089\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 144.2169 - val_loss: 128.7607\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 115.2975 - val_loss: 102.1430\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 90.7952 - val_loss: 79.7674\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 70.4089 - val_loss: 61.4112\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 53.9816 - val_loss: 46.9428\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 41.3521 - val_loss: 36.1785\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 32.3265 - val_loss: 28.8893\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 26.5858 - val_loss: 24.6711\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 21s 27us/step - loss: 23.6297 - val_loss: 22.8829\n",
      "385799/385799 [==============================] - 7s 18us/step\n",
      "771599/771599 [==============================] - 14s 18us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 29s 38us/step - loss: 12.4005 - val_loss: 3.0214\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 22s 29us/step - loss: 2.0367 - val_loss: 1.5101\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 22s 29us/step - loss: 1.2359 - val_loss: 1.1017\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 22s 29us/step - loss: 1.0225 - val_loss: 0.9805\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 22s 29us/step - loss: 0.9484 - val_loss: 0.9225\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 22s 29us/step - loss: 0.8981 - val_loss: 0.8784\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 22s 29us/step - loss: 0.8612 - val_loss: 0.9385\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 22s 29us/step - loss: 0.8334 - val_loss: 0.8246\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 22s 29us/step - loss: 0.8086 - val_loss: 0.8039\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 22s 29us/step - loss: 0.7895 - val_loss: 0.7828\n",
      "385800/385800 [==============================] - 7s 19us/step\n",
      "771598/771598 [==============================] - 15s 19us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 12.5585 - val_loss: 3.4184\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 2.2854 - val_loss: 1.5036\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 1.3184 - val_loss: 1.2171\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 1.1383 - val_loss: 1.0769\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 1.0238 - val_loss: 0.9843\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 0.9499 - val_loss: 0.9235\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 0.8974 - val_loss: 0.8805\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 0.8594 - val_loss: 0.8467\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 0.8299 - val_loss: 0.8227\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 0.8070 - val_loss: 0.8077\n",
      "385799/385799 [==============================] - 8s 20us/step\n",
      "771599/771599 [==============================] - 15s 19us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 29s 38us/step - loss: 12.6325 - val_loss: 3.4764\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 23s 29us/step - loss: 2.4457 - val_loss: 1.6738\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 23s 29us/step - loss: 1.3404 - val_loss: 1.2229\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 23s 29us/step - loss: 1.1156 - val_loss: 1.0697\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 23s 29us/step - loss: 1.0170 - val_loss: 0.9814\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 23s 29us/step - loss: 0.9448 - val_loss: 0.9224\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 23s 29us/step - loss: 0.8947 - val_loss: 0.8847\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 23s 29us/step - loss: 0.8569 - val_loss: 0.8450\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 0.8278 - val_loss: 0.8659\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 23s 29us/step - loss: 0.8039 - val_loss: 0.8457\n",
      "385799/385799 [==============================] - 7s 19us/step\n",
      "771599/771599 [==============================] - 15s 19us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 11.5763 - val_loss: 2.5110\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 1.7594 - val_loss: 1.3504\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 1.1641 - val_loss: 1.0771\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 1.0109 - val_loss: 0.9902\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 0.9413 - val_loss: 0.9154\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 0.8923 - val_loss: 0.8966\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 0.8553 - val_loss: 0.8502\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 0.8265 - val_loss: 0.8203\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 0.8035 - val_loss: 0.8002\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 0.7841 - val_loss: 0.7782\n",
      "385800/385800 [==============================] - 8s 20us/step\n",
      "771598/771598 [==============================] - 15s 20us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 12.7280 - val_loss: 2.7900\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.9661 - val_loss: 1.5134\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.3414 - val_loss: 1.2298\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.0929 - val_loss: 1.0073\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.9570 - val_loss: 0.9314\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.9022 - val_loss: 0.8903\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8620 - val_loss: 0.8501\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8317 - val_loss: 0.8391\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8079 - val_loss: 0.8194\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.7882 - val_loss: 0.7850\n",
      "385799/385799 [==============================] - 8s 20us/step\n",
      "771599/771599 [==============================] - 15s 20us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 12.0621 - val_loss: 3.4758\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 2.2828 - val_loss: 1.5743\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.2867 - val_loss: 1.1731\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.1014 - val_loss: 1.0963\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.0059 - val_loss: 0.9922\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.9374 - val_loss: 0.9133\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8880 - val_loss: 0.8716\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8512 - val_loss: 0.8455\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8233 - val_loss: 0.8158\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8009 - val_loss: 0.7973\n",
      "385799/385799 [==============================] - 8s 20us/step\n",
      "771599/771599 [==============================] - 15s 20us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 177.8664 - val_loss: 160.0129\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 143.9923 - val_loss: 128.7778\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 115.1109 - val_loss: 102.1646\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 90.6298 - val_loss: 79.7820\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 70.2703 - val_loss: 61.4297\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 25s 32us/step - loss: 53.8653 - val_loss: 46.9557\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 41.2629 - val_loss: 36.1978\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 32.2576 - val_loss: 28.8986\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 26.5334 - val_loss: 24.6781\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 24s 31us/step - loss: 23.5979 - val_loss: 22.8912\n",
      "385800/385800 [==============================] - 8s 20us/step\n",
      "771598/771598 [==============================] - 16s 20us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 177.8097 - val_loss: 160.0244\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 143.9325 - val_loss: 128.7793\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 115.0425 - val_loss: 102.1584\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 90.5569 - val_loss: 79.7747\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 70.1986 - val_loss: 61.4208\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 53.7990 - val_loss: 46.9562\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 41.1994 - val_loss: 36.1978\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 32.1908 - val_loss: 28.8983\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 26.4718 - val_loss: 24.6811\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 23.5329 - val_loss: 22.8903\n",
      "385799/385799 [==============================] - 8s 20us/step\n",
      "771599/771599 [==============================] - 15s 20us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 178.1139 - val_loss: 159.9938\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 144.2044 - val_loss: 128.7502\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 115.2898 - val_loss: 102.1356\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 90.7859 - val_loss: 79.7598\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 70.4047 - val_loss: 61.4066\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 53.9774 - val_loss: 46.9365\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 41.3478 - val_loss: 36.1738\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 32.3196 - val_loss: 28.8859\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 26.5831 - val_loss: 24.6698\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 23.6339 - val_loss: 22.8875\n",
      "385799/385799 [==============================] - 8s 20us/step\n",
      "771599/771599 [==============================] - 15s 20us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 32s 42us/step - loss: 17.6599 - val_loss: 4.0958\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 3.0106 - val_loss: 2.2280\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 1.9125 - val_loss: 1.7166\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 26s 33us/step - loss: 1.5793 - val_loss: 1.5180\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 1.3634 - val_loss: 1.3039\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 1.2521 - val_loss: 1.2044\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 1.1539 - val_loss: 1.1454\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 1.1007 - val_loss: 1.0844\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 1.0591 - val_loss: 1.0524\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 26s 34us/step - loss: 1.0167 - val_loss: 0.9930\n",
      "385800/385800 [==============================] - 8s 21us/step\n",
      "771598/771598 [==============================] - 16s 21us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 15.3165 - val_loss: 4.1433\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 3.0538 - val_loss: 2.2533\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.8476 - val_loss: 1.6197\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.4712 - val_loss: 1.4255\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.3404 - val_loss: 1.2953\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.2417 - val_loss: 1.1890\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.1407 - val_loss: 1.2075\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.0854 - val_loss: 1.0666\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.0233 - val_loss: 1.0026\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 0.9596 - val_loss: 0.9752\n",
      "385799/385799 [==============================] - 9s 23us/step\n",
      "771599/771599 [==============================] - 17s 22us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 14.9811 - val_loss: 4.3198\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 3.2281 - val_loss: 2.3388\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.8961 - val_loss: 1.6275\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.4534 - val_loss: 1.3410\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.2746 - val_loss: 1.2351\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.1910 - val_loss: 1.1980\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.1075 - val_loss: 1.0694\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.0230 - val_loss: 1.0151\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 0.9879 - val_loss: 0.9775\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 0.9630 - val_loss: 0.9572\n",
      "385799/385799 [==============================] - 8s 22us/step\n",
      "771599/771599 [==============================] - 17s 21us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 34s 44us/step - loss: 13.2393 - val_loss: 4.3099\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 27s 35us/step - loss: 3.0955 - val_loss: 2.2410\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 27s 35us/step - loss: 1.8399 - val_loss: 1.6048\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 27s 35us/step - loss: 1.3624 - val_loss: 1.2584\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 27s 35us/step - loss: 1.1665 - val_loss: 1.1565\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 27s 35us/step - loss: 1.1014 - val_loss: 1.1320\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 27s 35us/step - loss: 1.0576 - val_loss: 1.0654\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 27s 35us/step - loss: 1.0214 - val_loss: 1.0092\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 27s 36us/step - loss: 0.9920 - val_loss: 1.0476\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 27s 35us/step - loss: 0.9682 - val_loss: 0.9716\n",
      "385800/385800 [==============================] - 8s 22us/step\n",
      "771598/771598 [==============================] - 17s 22us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 35s 45us/step - loss: 14.3780 - val_loss: 4.1723\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 27s 36us/step - loss: 3.0053 - val_loss: 2.1343\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.7495 - val_loss: 1.5555\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.4698 - val_loss: 1.3943\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 27s 36us/step - loss: 1.3140 - val_loss: 1.2426\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.1938 - val_loss: 1.1789\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.1088 - val_loss: 1.0877\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 1.0233 - val_loss: 1.0156\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 0.9888 - val_loss: 0.9794\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 27s 35us/step - loss: 0.9639 - val_loss: 0.9834\n",
      "385799/385799 [==============================] - 8s 22us/step\n",
      "771599/771599 [==============================] - 17s 22us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 35s 45us/step - loss: 13.7023 - val_loss: 3.6005\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 2.4311 - val_loss: 1.8270\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.5920 - val_loss: 1.4657\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.3981 - val_loss: 1.3464\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.2780 - val_loss: 1.2376\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.2003 - val_loss: 1.2058\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.1457 - val_loss: 1.1274\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.0931 - val_loss: 1.0680\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.0368 - val_loss: 1.0339\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.0072 - val_loss: 1.0101\n",
      "385799/385799 [==============================] - 9s 22us/step\n",
      "771599/771599 [==============================] - 17s 22us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 35s 45us/step - loss: 177.8542 - val_loss: 160.0055\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 143.9806 - val_loss: 128.7610\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 115.0910 - val_loss: 102.1444\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 90.6094 - val_loss: 79.7640\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 70.2513 - val_loss: 61.4096\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 53.8573 - val_loss: 46.9513\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 41.2587 - val_loss: 36.1902\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 32.2522 - val_loss: 28.8966\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 26.5366 - val_loss: 24.6807\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 23.6002 - val_loss: 22.8927\n",
      "385800/385800 [==============================] - 9s 22us/step\n",
      "771598/771598 [==============================] - 17s 22us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 33s 43us/step - loss: 177.7916 - val_loss: 160.0005\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 26s 33us/step - loss: 143.9141 - val_loss: 128.7611\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 26s 33us/step - loss: 115.0282 - val_loss: 102.1489\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 26s 34us/step - loss: 90.5560 - val_loss: 79.7747\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 26s 33us/step - loss: 70.1990 - val_loss: 61.4222\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 26s 33us/step - loss: 53.7935 - val_loss: 46.9460\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 26s 33us/step - loss: 41.1899 - val_loss: 36.1901\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 26s 33us/step - loss: 32.1891 - val_loss: 28.8993\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 26s 33us/step - loss: 26.4701 - val_loss: 24.6784\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 23.5316 - val_loss: 22.8890\n",
      "385799/385799 [==============================] - 8s 20us/step\n",
      "771599/771599 [==============================] - 15s 20us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 35s 46us/step - loss: 178.1117 - val_loss: 159.9933\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 144.2050 - val_loss: 128.7487\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 115.2898 - val_loss: 102.1385\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 90.7915 - val_loss: 79.7601\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 70.4062 - val_loss: 61.4107\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 53.9760 - val_loss: 46.9335\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 41.3494 - val_loss: 36.1771\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 32.3230 - val_loss: 28.8868\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 26.5829 - val_loss: 24.6700\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 23.6315 - val_loss: 22.8860\n",
      "385799/385799 [==============================] - 9s 22us/step\n",
      "771599/771599 [==============================] - 17s 22us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 12.8876 - val_loss: 3.5328\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 25s 32us/step - loss: 2.3693 - val_loss: 1.4977\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 25s 32us/step - loss: 1.3049 - val_loss: 1.2144\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 25s 32us/step - loss: 1.1393 - val_loss: 1.0794\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 25s 32us/step - loss: 1.0299 - val_loss: 0.9893\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 25s 32us/step - loss: 0.9546 - val_loss: 0.9273\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 24s 32us/step - loss: 0.9021 - val_loss: 0.8837\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 25s 32us/step - loss: 0.8626 - val_loss: 0.8479\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 25s 32us/step - loss: 0.8322 - val_loss: 0.8691\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 24s 32us/step - loss: 0.8083 - val_loss: 0.8001\n",
      "385800/385800 [==============================] - 8s 21us/step\n",
      "771598/771598 [==============================] - 16s 21us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 11.2453 - val_loss: 3.0328\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 1.9117 - val_loss: 1.3323\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 1.1323 - val_loss: 1.0383\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 0.9919 - val_loss: 0.9639\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 0.9279 - val_loss: 0.9198\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 0.8838 - val_loss: 0.8702\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 0.8488 - val_loss: 0.8397\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 0.8209 - val_loss: 0.8167\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 0.7987 - val_loss: 0.7986\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 0.7807 - val_loss: 0.7896\n",
      "385799/385799 [==============================] - 8s 21us/step\n",
      "771599/771599 [==============================] - 16s 21us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 12.1444 - val_loss: 3.3421\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 2.1422 - val_loss: 1.4060\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.2402 - val_loss: 1.1788\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.0893 - val_loss: 1.0374\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.9916 - val_loss: 0.9573\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.9266 - val_loss: 0.9334\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8804 - val_loss: 0.8769\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8453 - val_loss: 0.8345\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8180 - val_loss: 0.8115\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.7961 - val_loss: 0.7944\n",
      "385799/385799 [==============================] - 8s 20us/step\n",
      "771599/771599 [==============================] - 16s 20us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 29s 38us/step - loss: 11.0017 - val_loss: 2.5719\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 1.6908 - val_loss: 1.3136\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 1.1727 - val_loss: 1.1252\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 1.0401 - val_loss: 1.0006\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 0.9631 - val_loss: 0.9404\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 0.9101 - val_loss: 0.9067\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 0.8692 - val_loss: 0.9035\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 0.8390 - val_loss: 0.8272\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 0.8139 - val_loss: 0.8099\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 21s 28us/step - loss: 0.7941 - val_loss: 0.7895\n",
      "385800/385800 [==============================] - 7s 18us/step\n",
      "771598/771598 [==============================] - 14s 18us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 11.8697 - val_loss: 3.5002\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 2.3071 - val_loss: 1.4592\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 1.3019 - val_loss: 1.2385\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 1.1357 - val_loss: 1.0788\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.0273 - val_loss: 1.0174\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 24s 30us/step - loss: 0.9522 - val_loss: 0.9257\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 0.9001 - val_loss: 0.8869\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 0.8623 - val_loss: 0.8523\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 0.8316 - val_loss: 0.8267\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 23s 30us/step - loss: 0.8087 - val_loss: 0.8236\n",
      "385799/385799 [==============================] - 8s 20us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 15s 20us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 32s 41us/step - loss: 12.4029 - val_loss: 3.4099\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 2.2411 - val_loss: 1.5143\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.3006 - val_loss: 1.2041\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.1178 - val_loss: 1.0605\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 1.0107 - val_loss: 0.9832\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.9403 - val_loss: 0.9160\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8903 - val_loss: 0.8729\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8528 - val_loss: 0.8409\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8231 - val_loss: 0.8160\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 24s 31us/step - loss: 0.8006 - val_loss: 0.7948\n",
      "385799/385799 [==============================] - 8s 20us/step\n",
      "771599/771599 [==============================] - 16s 20us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 177.8502 - val_loss: 160.0010\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 143.9767 - val_loss: 128.7592\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 115.0879 - val_loss: 102.1426\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 90.6122 - val_loss: 79.7698\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 70.2577 - val_loss: 61.4174\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 53.8559 - val_loss: 46.9484\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 41.2584 - val_loss: 36.1927\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 32.2551 - val_loss: 28.8995\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 26.5321 - val_loss: 24.6750\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 22s 28us/step - loss: 23.5989 - val_loss: 22.8932\n",
      "385800/385800 [==============================] - 7s 18us/step\n",
      "771598/771598 [==============================] - 14s 18us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 33s 43us/step - loss: 177.7885 - val_loss: 160.0000\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 143.9142 - val_loss: 128.7599\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 115.0281 - val_loss: 102.1462\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 90.5501 - val_loss: 79.7666\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 70.1922 - val_loss: 61.4180\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 53.7940 - val_loss: 46.9511\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 25s 32us/step - loss: 41.1956 - val_loss: 36.1950\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 32.1909 - val_loss: 28.8981\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 26s 34us/step - loss: 26.4747 - val_loss: 24.6852\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 25s 33us/step - loss: 23.5378 - val_loss: 22.8940\n",
      "385799/385799 [==============================] - 8s 21us/step\n",
      "771599/771599 [==============================] - 16s 21us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 178.1220 - val_loss: 160.0050\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 22s 29us/step - loss: 144.2078 - val_loss: 128.7467\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 22s 29us/step - loss: 115.2879 - val_loss: 102.1379\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 22s 28us/step - loss: 90.7913 - val_loss: 79.7635\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 22s 29us/step - loss: 70.4128 - val_loss: 61.4167\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 22s 29us/step - loss: 53.9847 - val_loss: 46.9409\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 22s 29us/step - loss: 41.3573 - val_loss: 36.1825\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 22s 29us/step - loss: 32.3279 - val_loss: 28.8885\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 22s 29us/step - loss: 26.5857 - val_loss: 24.6729\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 22s 29us/step - loss: 23.6334 - val_loss: 22.8872\n",
      "385799/385799 [==============================] - 7s 19us/step\n",
      "771599/771599 [==============================] - 14s 18us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 14.8935 - val_loss: 4.2770\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 3.0454 - val_loss: 2.2444\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 1.8055 - val_loss: 1.5612\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 1.4765 - val_loss: 1.4718\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 1.3451 - val_loss: 1.2843\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 1.2212 - val_loss: 1.1825\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 1.1505 - val_loss: 1.1318\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 1.1047 - val_loss: 1.0905\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 1.0663 - val_loss: 1.0511\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 28s 36us/step - loss: 1.0342 - val_loss: 1.0480\n",
      "385800/385800 [==============================] - 9s 22us/step\n",
      "771598/771598 [==============================] - 17s 22us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 15.6924 - val_loss: 4.2979\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 2.9866 - val_loss: 2.1606\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.8956 - val_loss: 1.7473\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.5880 - val_loss: 1.5598\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.4080 - val_loss: 1.3506\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.2797 - val_loss: 1.2294\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.1720 - val_loss: 1.1274\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.0994 - val_loss: 1.0854\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.0627 - val_loss: 1.0498\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 28s 36us/step - loss: 1.0326 - val_loss: 1.0265\n",
      "385799/385799 [==============================] - 9s 23us/step\n",
      "771599/771599 [==============================] - 17s 22us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 15.1698 - val_loss: 4.0505\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 28s 37us/step - loss: 3.0853 - val_loss: 2.4265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 28s 37us/step - loss: 1.9303 - val_loss: 1.5409\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 28s 37us/step - loss: 1.3888 - val_loss: 1.3250\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 28s 37us/step - loss: 1.2687 - val_loss: 1.2330\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 28s 37us/step - loss: 1.1906 - val_loss: 1.1593\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 28s 37us/step - loss: 1.1198 - val_loss: 1.0817\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 28s 37us/step - loss: 1.0319 - val_loss: 1.0158\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 29s 38us/step - loss: 0.9856 - val_loss: 0.9793\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 0.9592 - val_loss: 0.9544\n",
      "385799/385799 [==============================] - 9s 23us/step\n",
      "771599/771599 [==============================] - 18s 23us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 37s 48us/step - loss: 13.8265 - val_loss: 3.9283\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 28s 37us/step - loss: 2.6175 - val_loss: 1.8601\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 28s 37us/step - loss: 1.6315 - val_loss: 1.5736\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 28s 37us/step - loss: 1.4108 - val_loss: 1.3517\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 28s 37us/step - loss: 1.2927 - val_loss: 1.2566\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 28s 37us/step - loss: 1.1895 - val_loss: 1.2155\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 28s 37us/step - loss: 1.1124 - val_loss: 1.0827\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 1.0209 - val_loss: 0.9883\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 0.9582 - val_loss: 0.9509\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 28s 37us/step - loss: 0.9373 - val_loss: 0.9770\n",
      "385800/385800 [==============================] - 9s 23us/step\n",
      "771598/771598 [==============================] - 18s 23us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 14.1042 - val_loss: 4.2055\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 29s 37us/step - loss: 3.0289 - val_loss: 2.3376\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 29s 37us/step - loss: 2.0177 - val_loss: 1.7534\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 29s 38us/step - loss: 1.5309 - val_loss: 1.3783\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 29s 38us/step - loss: 1.2788 - val_loss: 1.2184\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 29s 37us/step - loss: 1.1596 - val_loss: 1.1109\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 29s 37us/step - loss: 1.0683 - val_loss: 1.2896\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 29s 37us/step - loss: 1.0309 - val_loss: 1.0190\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 29s 37us/step - loss: 1.0005 - val_loss: 0.9956\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 29s 37us/step - loss: 0.9759 - val_loss: 0.9715\n",
      "385799/385799 [==============================] - 9s 23us/step\n",
      "771599/771599 [==============================] - 17s 23us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 14.1849 - val_loss: 3.8634\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 2.5692 - val_loss: 1.8775\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 1.6847 - val_loss: 1.5792\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 1.4885 - val_loss: 1.4491\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 1.3613 - val_loss: 1.3199\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 1.2441 - val_loss: 1.2189\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 1.1369 - val_loss: 1.0761\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 1.0404 - val_loss: 1.0137\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 0.9855 - val_loss: 0.9754\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 0.9619 - val_loss: 0.9581\n",
      "385799/385799 [==============================] - 9s 24us/step\n",
      "771599/771599 [==============================] - 18s 24us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 37s 49us/step - loss: 177.8609 - val_loss: 160.0091\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 29s 38us/step - loss: 143.9824 - val_loss: 128.7627\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 29s 38us/step - loss: 115.0977 - val_loss: 102.1550\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 29s 38us/step - loss: 90.6176 - val_loss: 79.7676\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 29s 38us/step - loss: 70.2587 - val_loss: 61.4198\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 29s 38us/step - loss: 53.8588 - val_loss: 46.9510\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 29s 38us/step - loss: 41.2616 - val_loss: 36.1952\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 29s 38us/step - loss: 32.2549 - val_loss: 28.9010\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 29s 38us/step - loss: 26.5372 - val_loss: 24.6810\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 29s 38us/step - loss: 23.6022 - val_loss: 22.8921\n",
      "385800/385800 [==============================] - 9s 23us/step\n",
      "771598/771598 [==============================] - 18s 23us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 177.7867 - val_loss: 159.9983\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 143.9077 - val_loss: 128.7563\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 115.0244 - val_loss: 102.1428\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 90.5464 - val_loss: 79.7632\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 70.1909 - val_loss: 61.4162\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 53.7923 - val_loss: 46.9494\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 41.1901 - val_loss: 36.1860\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 32.1812 - val_loss: 28.8918\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 26.4678 - val_loss: 24.6792\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 30s 38us/step - loss: 23.5350 - val_loss: 22.8916\n",
      "385799/385799 [==============================] - 9s 24us/step\n",
      "771599/771599 [==============================] - 18s 24us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 178.1076 - val_loss: 159.9849\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 144.2020 - val_loss: 128.7487\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 115.2909 - val_loss: 102.1350\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 90.7847 - val_loss: 79.7538\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 70.4027 - val_loss: 61.4052\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 53.9776 - val_loss: 46.9386\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 41.3515 - val_loss: 36.1776\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 32.3239 - val_loss: 28.8897\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 26.5849 - val_loss: 24.6685\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 30s 39us/step - loss: 23.6289 - val_loss: 22.8839\n",
      "385799/385799 [==============================] - 9s 24us/step\n",
      "771599/771599 [==============================] - 18s 24us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 41s 54us/step - loss: 20.2536 - val_loss: 5.6693\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 33s 42us/step - loss: 3.9153 - val_loss: 2.6738\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 33s 42us/step - loss: 2.2600 - val_loss: 2.0425\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 33s 42us/step - loss: 1.8608 - val_loss: 1.6804\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 33s 42us/step - loss: 1.5770 - val_loss: 1.5271\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 33s 42us/step - loss: 1.4669 - val_loss: 1.5093\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 33s 42us/step - loss: 1.3773 - val_loss: 1.3420\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 33s 43us/step - loss: 1.3157 - val_loss: 1.3223\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 33s 42us/step - loss: 1.2718 - val_loss: 1.3342\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 33s 42us/step - loss: 1.2266 - val_loss: 1.2024\n",
      "385800/385800 [==============================] - 10s 26us/step\n",
      "771598/771598 [==============================] - 19s 25us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 41s 54us/step - loss: 20.2177 - val_loss: 5.4247\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 33s 42us/step - loss: 3.9916 - val_loss: 2.8201\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 33s 42us/step - loss: 2.2707 - val_loss: 1.9340\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 33s 42us/step - loss: 1.7596 - val_loss: 1.6712\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 33s 42us/step - loss: 1.5983 - val_loss: 1.6118\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 33s 42us/step - loss: 1.4727 - val_loss: 1.4203\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 33s 42us/step - loss: 1.3523 - val_loss: 1.3158\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 33s 42us/step - loss: 1.2472 - val_loss: 1.1967\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 33s 42us/step - loss: 1.1603 - val_loss: 1.1453\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 33s 43us/step - loss: 1.1235 - val_loss: 1.1152\n",
      "385799/385799 [==============================] - 10s 25us/step\n",
      "771599/771599 [==============================] - 19s 25us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 43s 56us/step - loss: 19.7819 - val_loss: 6.0096\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 3.7035 - val_loss: 2.3744\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 2.1601 - val_loss: 1.9995\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 1.7884 - val_loss: 1.6462\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 1.5830 - val_loss: 1.5258\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 1.4554 - val_loss: 1.4022\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 34s 45us/step - loss: 1.3686 - val_loss: 1.3772\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 1.3078 - val_loss: 1.2920\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 34s 45us/step - loss: 1.2492 - val_loss: 1.2231\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 1.1762 - val_loss: 1.1456\n",
      "385799/385799 [==============================] - 10s 27us/step\n",
      "771599/771599 [==============================] - 20s 26us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 44s 58us/step - loss: 15.4603 - val_loss: 5.0732\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 3.8091 - val_loss: 2.8278\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 2.3660 - val_loss: 2.0423\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 1.8530 - val_loss: 1.7241\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 1.6030 - val_loss: 1.5410\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 1.4732 - val_loss: 1.4243\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 1.3557 - val_loss: 1.3298\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 1.2713 - val_loss: 1.2636\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 1.2304 - val_loss: 1.2918\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 1.1934 - val_loss: 1.1786\n",
      "385800/385800 [==============================] - 10s 26us/step\n",
      "771598/771598 [==============================] - 20s 26us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 44s 58us/step - loss: 18.1316 - val_loss: 4.3679\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 35s 46us/step - loss: 3.1964 - val_loss: 2.4498\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 35s 46us/step - loss: 2.1472 - val_loss: 1.9522\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 35s 46us/step - loss: 1.8208 - val_loss: 1.7419\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 35s 46us/step - loss: 1.6133 - val_loss: 1.5181\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 35s 46us/step - loss: 1.4490 - val_loss: 1.3891\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 35s 46us/step - loss: 1.3365 - val_loss: 1.3195\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 35s 46us/step - loss: 1.2781 - val_loss: 1.2611\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 35s 46us/step - loss: 1.2254 - val_loss: 1.2204\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 35s 46us/step - loss: 1.1975 - val_loss: 1.2070\n",
      "385799/385799 [==============================] - 10s 26us/step\n",
      "771599/771599 [==============================] - 20s 26us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 18.0561 - val_loss: 4.7027\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 36s 46us/step - loss: 3.4703 - val_loss: 2.5990\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 36s 46us/step - loss: 2.1693 - val_loss: 1.9260\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 1.7733 - val_loss: 1.7101\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 36s 46us/step - loss: 1.5462 - val_loss: 1.5046\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 36s 46us/step - loss: 1.4218 - val_loss: 1.3668\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 36s 46us/step - loss: 1.3282 - val_loss: 1.3053\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 36s 46us/step - loss: 1.2692 - val_loss: 1.3025\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 36s 46us/step - loss: 1.2178 - val_loss: 1.2084\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 36s 46us/step - loss: 1.1897 - val_loss: 1.1860\n",
      "385799/385799 [==============================] - 10s 26us/step\n",
      "771599/771599 [==============================] - 23s 29us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 45s 59us/step - loss: 177.8637 - val_loss: 160.0147\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 143.9835 - val_loss: 128.7626\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 115.0943 - val_loss: 102.1498\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 90.6143 - val_loss: 79.7689\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 70.2613 - val_loss: 61.4211\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 53.8616 - val_loss: 46.9546\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 37s 47us/step - loss: 41.2599 - val_loss: 36.1893\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 32.2530 - val_loss: 28.8980\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 26.5376 - val_loss: 24.6811\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 36s 47us/step - loss: 23.5990 - val_loss: 22.8929\n",
      "385800/385800 [==============================] - 10s 26us/step\n",
      "771598/771598 [==============================] - 20s 26us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 46s 60us/step - loss: 177.7918 - val_loss: 159.9998\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 143.9173 - val_loss: 128.7646\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 115.0317 - val_loss: 102.1512\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 90.5542 - val_loss: 79.7726\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 70.1970 - val_loss: 61.4219\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 53.7980 - val_loss: 46.9512\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 41.1936 - val_loss: 36.1894\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 32.1883 - val_loss: 28.8987\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 26.4678 - val_loss: 24.6770\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 23.5305 - val_loss: 22.8895\n",
      "385799/385799 [==============================] - 11s 27us/step\n",
      "771599/771599 [==============================] - 21s 27us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 46s 59us/step - loss: 178.1242 - val_loss: 160.0058\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 144.2159 - val_loss: 128.7567\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 115.3039 - val_loss: 102.1486\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 90.7959 - val_loss: 79.7665\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 70.4148 - val_loss: 61.4201\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 53.9907 - val_loss: 46.9485\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 41.3588 - val_loss: 36.1842\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 32.3216 - val_loss: 28.8813\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 26.5837 - val_loss: 24.6716\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 23.6304 - val_loss: 22.8833\n",
      "385799/385799 [==============================] - 10s 27us/step\n",
      "771599/771599 [==============================] - 21s 27us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 40s 52us/step - loss: 15.9013 - val_loss: 4.1580\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 3.3273 - val_loss: 2.6785\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 2.0413 - val_loss: 1.7479\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 1.5996 - val_loss: 1.5252\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 1.3941 - val_loss: 1.3400\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 1.2516 - val_loss: 1.1710\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 1.0977 - val_loss: 1.0433\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 1.0153 - val_loss: 1.0082\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 0.9844 - val_loss: 0.9732\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 30s 39us/step - loss: 0.9602 - val_loss: 0.9519\n",
      "385800/385800 [==============================] - 10s 25us/step\n",
      "771598/771598 [==============================] - 19s 24us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 14.1890 - val_loss: 4.0646\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 32s 41us/step - loss: 2.9807 - val_loss: 2.2860\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.7887 - val_loss: 1.4993\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.4189 - val_loss: 1.3576\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.3050 - val_loss: 1.3387\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.2254 - val_loss: 1.1970\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.1638 - val_loss: 1.1391\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.1132 - val_loss: 1.2299\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.0738 - val_loss: 1.0574\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.0358 - val_loss: 1.0205\n",
      "385799/385799 [==============================] - 10s 25us/step\n",
      "771599/771599 [==============================] - 20s 25us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 15.4233 - val_loss: 4.5613\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 3.1679 - val_loss: 2.2612\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.8782 - val_loss: 1.5457\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.3677 - val_loss: 1.2466\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.1886 - val_loss: 1.1539\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.1205 - val_loss: 1.1459\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.0720 - val_loss: 1.0535\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.0352 - val_loss: 1.0213\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.0036 - val_loss: 1.0217\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 0.9777 - val_loss: 0.9833\n",
      "385799/385799 [==============================] - 9s 25us/step\n",
      "771599/771599 [==============================] - 19s 25us/step\n",
      "Loss function: mean_squared_error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 41s 53us/step - loss: 13.8936 - val_loss: 3.8849\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 31s 40us/step - loss: 2.6044 - val_loss: 1.8056\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 31s 40us/step - loss: 1.6086 - val_loss: 1.4981\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 31s 40us/step - loss: 1.4274 - val_loss: 1.3673\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 31s 40us/step - loss: 1.2957 - val_loss: 1.2449\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 31s 40us/step - loss: 1.1716 - val_loss: 1.1275\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 31s 40us/step - loss: 1.0845 - val_loss: 1.0636\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 31s 40us/step - loss: 1.0371 - val_loss: 1.0263\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 31s 40us/step - loss: 0.9841 - val_loss: 0.9630\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 31s 40us/step - loss: 0.9478 - val_loss: 0.9512\n",
      "385800/385800 [==============================] - 10s 25us/step\n",
      "771598/771598 [==============================] - 19s 25us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 41s 54us/step - loss: 13.8800 - val_loss: 3.7592\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 2.7919 - val_loss: 2.0489\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.7184 - val_loss: 1.4893\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.3670 - val_loss: 1.3829\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.2353 - val_loss: 1.1848\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.1555 - val_loss: 1.1558\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.1114 - val_loss: 1.1008\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.0747 - val_loss: 1.0690\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.0440 - val_loss: 1.0409\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.0176 - val_loss: 1.0073\n",
      "385799/385799 [==============================] - 10s 25us/step\n",
      "771599/771599 [==============================] - 19s 25us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 14.5657 - val_loss: 4.4981\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 3.2244 - val_loss: 2.3708\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.9501 - val_loss: 1.6572\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.5091 - val_loss: 1.4373\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.3758 - val_loss: 1.9856\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.2999 - val_loss: 1.3261\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 31s 40us/step - loss: 1.2179 - val_loss: 1.1927\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.1310 - val_loss: 1.2059\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.0890 - val_loss: 1.0757\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 31s 41us/step - loss: 1.0546 - val_loss: 1.0404\n",
      "385799/385799 [==============================] - 10s 25us/step\n",
      "771599/771599 [==============================] - 19s 25us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 42s 55us/step - loss: 177.8545 - val_loss: 160.0063\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 32s 42us/step - loss: 143.9839 - val_loss: 128.7659\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 115.0943 - val_loss: 102.1490\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 90.6133 - val_loss: 79.7679\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 70.2588 - val_loss: 61.4185\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 53.8561 - val_loss: 46.9457\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 41.2569 - val_loss: 36.1914\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 32s 42us/step - loss: 32.2533 - val_loss: 28.8959\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 26.5333 - val_loss: 24.6769\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 23.5979 - val_loss: 22.8930\n",
      "385800/385800 [==============================] - 10s 25us/step\n",
      "771598/771598 [==============================] - 19s 25us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 42s 54us/step - loss: 177.7771 - val_loss: 159.9896\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 143.9040 - val_loss: 128.7492\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 32s 41us/step - loss: 115.0125 - val_loss: 102.1314\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 32s 41us/step - loss: 90.5383 - val_loss: 79.7587\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 32s 41us/step - loss: 70.1887 - val_loss: 61.4148\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 32s 41us/step - loss: 53.7903 - val_loss: 46.9452\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 32s 41us/step - loss: 41.1882 - val_loss: 36.1862\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 32.1858 - val_loss: 28.8972\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 32s 41us/step - loss: 26.4696 - val_loss: 24.6794\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 23.5354 - val_loss: 22.8939\n",
      "385799/385799 [==============================] - 10s 25us/step\n",
      "771599/771599 [==============================] - 19s 25us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 178.1030 - val_loss: 159.9787\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 144.1954 - val_loss: 128.7474\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 115.2935 - val_loss: 102.1399\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 90.7858 - val_loss: 79.7579\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 70.4036 - val_loss: 61.4061\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 33s 42us/step - loss: 53.9792 - val_loss: 46.9391\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 41.3492 - val_loss: 36.1747\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 32.3212 - val_loss: 28.8845\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 26.5816 - val_loss: 24.6669\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 23.6323 - val_loss: 22.8857\n",
      "385799/385799 [==============================] - 10s 26us/step\n",
      "771599/771599 [==============================] - 20s 26us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 48s 62us/step - loss: 21.0901 - val_loss: 5.0039\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 37s 49us/step - loss: 4.0390 - val_loss: 3.1980\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 37s 48us/step - loss: 2.4648 - val_loss: 2.0266\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 37s 48us/step - loss: 1.7941 - val_loss: 1.6872\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 37s 48us/step - loss: 1.5921 - val_loss: 1.5297\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 37s 48us/step - loss: 1.4698 - val_loss: 1.4168\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 37s 49us/step - loss: 1.3775 - val_loss: 1.3484\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 37s 49us/step - loss: 1.3054 - val_loss: 1.2732\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 37s 49us/step - loss: 1.2386 - val_loss: 1.2172\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 37s 49us/step - loss: 1.1776 - val_loss: 1.1546\n",
      "385800/385800 [==============================] - 11s 28us/step\n",
      "771598/771598 [==============================] - 22s 28us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 48s 63us/step - loss: 20.4487 - val_loss: 5.8055\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 4.2549 - val_loss: 3.0418\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 2.3433 - val_loss: 1.9365\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 1.7063 - val_loss: 1.6465\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 38s 50us/step - loss: 1.5490 - val_loss: 1.4931\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 38s 50us/step - loss: 1.4479 - val_loss: 1.4239\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 38s 50us/step - loss: 1.3747 - val_loss: 1.3578\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 1.2900 - val_loss: 1.3151\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 38s 50us/step - loss: 1.2126 - val_loss: 1.1720\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 1.1273 - val_loss: 1.1098\n",
      "385799/385799 [==============================] - 11s 29us/step\n",
      "771599/771599 [==============================] - 22s 29us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 49s 64us/step - loss: 20.0854 - val_loss: 5.7530\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 4.1736 - val_loss: 3.0418\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 38s 50us/step - loss: 2.4040 - val_loss: 2.0965\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 1.9317 - val_loss: 1.8006\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 38s 50us/step - loss: 1.6667 - val_loss: 1.5653\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 38s 50us/step - loss: 1.5161 - val_loss: 1.4906\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 38s 50us/step - loss: 1.4338 - val_loss: 1.3822\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 1.3076 - val_loss: 1.2701\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 1.2437 - val_loss: 1.2303\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 38s 50us/step - loss: 1.1981 - val_loss: 1.1875\n",
      "385799/385799 [==============================] - 11s 29us/step\n",
      "771599/771599 [==============================] - 22s 29us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 50s 64us/step - loss: 16.5784 - val_loss: 5.0287\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 3.5635 - val_loss: 2.5641\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 2.2805 - val_loss: 2.1606\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 1.9083 - val_loss: 1.7345\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 1.6418 - val_loss: 1.6132\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 1.5123 - val_loss: 1.5105\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 1.4338 - val_loss: 1.4160\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 39s 50us/step - loss: 1.3808 - val_loss: 1.3658\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 1.3363 - val_loss: 1.3595\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 1.2937 - val_loss: 1.2788\n",
      "385800/385800 [==============================] - 11s 30us/step\n",
      "771598/771598 [==============================] - 23s 30us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 49s 63us/step - loss: 16.8964 - val_loss: 5.1261\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 3.8643 - val_loss: 2.8975\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 2.3061 - val_loss: 1.9916\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 1.7321 - val_loss: 1.5829\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 1.4815 - val_loss: 1.4241\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 1.3853 - val_loss: 1.4289\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 1.3271 - val_loss: 1.3051\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 1.2764 - val_loss: 1.2894\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 1.2318 - val_loss: 1.2127\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 38s 49us/step - loss: 1.1848 - val_loss: 1.1662\n",
      "385799/385799 [==============================] - 11s 28us/step\n",
      "771599/771599 [==============================] - 22s 28us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 50s 65us/step - loss: 16.9970 - val_loss: 4.9201\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 3.5509 - val_loss: 2.5081\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 2.1023 - val_loss: 1.7829\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 1.6866 - val_loss: 1.6406\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 1.5737 - val_loss: 1.5731\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 1.4789 - val_loss: 1.4449\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 1.3880 - val_loss: 1.3586\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 1.3313 - val_loss: 1.3158\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 40s 51us/step - loss: 1.2766 - val_loss: 1.2516\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 40s 51us/step - loss: 1.2174 - val_loss: 1.2117\n",
      "385799/385799 [==============================] - 12s 30us/step\n",
      "771599/771599 [==============================] - 23s 30us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 50s 65us/step - loss: 177.8437 - val_loss: 159.9856\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 39s 50us/step - loss: 143.9625 - val_loss: 128.7510\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 115.0836 - val_loss: 102.1417\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 90.6076 - val_loss: 79.7630\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 70.2521 - val_loss: 61.4120\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 53.8522 - val_loss: 46.9447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 39s 50us/step - loss: 41.2547 - val_loss: 36.1913\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 39s 50us/step - loss: 32.2565 - val_loss: 28.8993\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 39s 50us/step - loss: 26.5368 - val_loss: 24.6825\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 39s 50us/step - loss: 23.5996 - val_loss: 22.8916\n",
      "385800/385800 [==============================] - 11s 29us/step\n",
      "771598/771598 [==============================] - 23s 29us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 50s 65us/step - loss: 177.7723 - val_loss: 159.9798\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 143.8999 - val_loss: 128.7467\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 115.0198 - val_loss: 102.1408\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 90.5427 - val_loss: 79.7572\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 70.1849 - val_loss: 61.4120\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 53.7866 - val_loss: 46.9424\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 41.1861 - val_loss: 36.1826\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 32.1818 - val_loss: 28.8915\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 26.4675 - val_loss: 24.6769\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 23.5300 - val_loss: 22.8891\n",
      "385799/385799 [==============================] - 12s 30us/step\n",
      "771599/771599 [==============================] - 23s 29us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 50s 65us/step - loss: 178.1009 - val_loss: 159.9808\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 144.2012 - val_loss: 128.7470\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 115.2827 - val_loss: 102.1252\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 90.7809 - val_loss: 79.7542\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 70.4010 - val_loss: 61.4042\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 53.9727 - val_loss: 46.9309\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 41.3479 - val_loss: 36.1759\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 32.3210 - val_loss: 28.8850\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 26.5804 - val_loss: 24.6656\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 23.6286 - val_loss: 22.8843\n",
      "385799/385799 [==============================] - 11s 29us/step\n",
      "771599/771599 [==============================] - 23s 30us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 56s 72us/step - loss: 22.2150 - val_loss: 6.1527\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 4.2582 - val_loss: 2.9358\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 2.4129 - val_loss: 2.0818\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 1.8815 - val_loss: 1.7941\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 1.7333 - val_loss: 1.6865\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 45s 58us/step - loss: 1.6369 - val_loss: 1.5925\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 1.5407 - val_loss: 1.5507\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 1.4519 - val_loss: 1.4237\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 1.3858 - val_loss: 1.3544\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 45s 58us/step - loss: 1.3148 - val_loss: 1.3692\n",
      "385800/385800 [==============================] - 12s 32us/step\n",
      "771598/771598 [==============================] - 25s 32us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 21.4124 - val_loss: 5.8058\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 3.8869 - val_loss: 2.9045\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 2.4856 - val_loss: 2.2647\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 2.0292 - val_loss: 1.9181\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.8530 - val_loss: 1.8597\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.7553 - val_loss: 1.7259\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.6479 - val_loss: 1.5944\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.5384 - val_loss: 1.4959\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.4334 - val_loss: 1.4045\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.3510 - val_loss: 1.3434\n",
      "385799/385799 [==============================] - 13s 32us/step\n",
      "771599/771599 [==============================] - 25s 33us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 23.3914 - val_loss: 5.7185\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 4.6239 - val_loss: 3.5394\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 2.7579 - val_loss: 2.4610\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 2.1444 - val_loss: 1.9707\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.7954 - val_loss: 1.6706\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.5931 - val_loss: 1.5420\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.4944 - val_loss: 1.4701\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.4423 - val_loss: 1.4250\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.3915 - val_loss: 1.3782\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 1.3513 - val_loss: 1.3394\n",
      "385799/385799 [==============================] - 12s 32us/step\n",
      "771599/771599 [==============================] - 25s 32us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 58s 75us/step - loss: 21.6881 - val_loss: 5.8086\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 4.4313 - val_loss: 3.2733\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 2.6887 - val_loss: 2.6367\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 53s 68us/step - loss: 2.1810 - val_loss: 2.0971\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 62s 81us/step - loss: 1.9164 - val_loss: 1.8012\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 61s 79us/step - loss: 1.7004 - val_loss: 1.6349\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 54s 70us/step - loss: 1.5666 - val_loss: 1.5208\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 54s 70us/step - loss: 1.4806 - val_loss: 1.4489\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 63s 81us/step - loss: 1.3984 - val_loss: 1.3673\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 1.3384 - val_loss: 1.3134\n",
      "385800/385800 [==============================] - 14s 37us/step\n",
      "771598/771598 [==============================] - 29s 37us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 20.8203 - val_loss: 6.3299\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 4.4528 - val_loss: 3.2218\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 2.7325 - val_loss: 2.4568\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 2.2452 - val_loss: 2.0829\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 50s 65us/step - loss: 1.8784 - val_loss: 1.7531\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 1.7036 - val_loss: 1.6684\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 1.6147 - val_loss: 1.5660\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 1.4909 - val_loss: 1.4366\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 1.3797 - val_loss: 1.3572\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 1.3158 - val_loss: 1.3117\n",
      "385799/385799 [==============================] - 16s 41us/step\n",
      "771599/771599 [==============================] - 31s 40us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 20.8055 - val_loss: 5.9925\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 55s 72us/step - loss: 4.2800 - val_loss: 3.2938\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 55s 72us/step - loss: 2.7909 - val_loss: 2.3547\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 2.1704 - val_loss: 2.0465\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 1.8805 - val_loss: 1.7602\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 55s 71us/step - loss: 1.7146 - val_loss: 1.6850\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 55s 71us/step - loss: 1.6470 - val_loss: 1.6210\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 1.5860 - val_loss: 1.6548\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 55s 71us/step - loss: 1.5391 - val_loss: 1.5292\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 55s 72us/step - loss: 1.4922 - val_loss: 1.4734\n",
      "385799/385799 [==============================] - 16s 41us/step\n",
      "771599/771599 [==============================] - 31s 40us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 66s 85us/step - loss: 177.8706 - val_loss: 160.0223\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 52s 67us/step - loss: 143.9963 - val_loss: 128.7747\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 51s 66us/step - loss: 115.1051 - val_loss: 102.1592\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 50s 65us/step - loss: 90.6220 - val_loss: 79.7746\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 50s 65us/step - loss: 70.2690 - val_loss: 61.4268\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 53s 68us/step - loss: 53.8612 - val_loss: 46.9491\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 52s 67us/step - loss: 41.2601 - val_loss: 36.1942\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 54s 70us/step - loss: 32.2563 - val_loss: 28.8990\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 54s 70us/step - loss: 26.5382 - val_loss: 24.6843\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 23.6015 - val_loss: 22.8933\n",
      "385800/385800 [==============================] - 15s 39us/step\n",
      "771598/771598 [==============================] - 32s 41us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 81s 105us/step - loss: 177.7979 - val_loss: 160.0107\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 58s 76us/step - loss: 143.9251 - val_loss: 128.7740\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 50s 65us/step - loss: 115.0370 - val_loss: 102.1557\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 50s 65us/step - loss: 90.5508 - val_loss: 79.7647\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 70.1958 - val_loss: 61.4238\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 53.8000 - val_loss: 46.9550\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 41.1964 - val_loss: 36.1940\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 32.1889 - val_loss: 28.8981\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 26.4687 - val_loss: 24.6804\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 49s 64us/step - loss: 23.5347 - val_loss: 22.8927\n",
      "385799/385799 [==============================] - 15s 38us/step\n",
      "771599/771599 [==============================] - 29s 38us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 69s 90us/step - loss: 178.1184 - val_loss: 159.9967\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 55s 72us/step - loss: 144.2111 - val_loss: 128.7571\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 115.3002 - val_loss: 102.1448\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 51s 66us/step - loss: 90.7928 - val_loss: 79.7624\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 50s 64us/step - loss: 70.4079 - val_loss: 61.4090\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 54s 69us/step - loss: 53.9821 - val_loss: 46.9423\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 41.3510 - val_loss: 36.1775\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 54s 69us/step - loss: 32.3243 - val_loss: 28.8883\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 51s 67us/step - loss: 26.5868 - val_loss: 24.6726\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 23.6323 - val_loss: 22.8863\n",
      "385799/385799 [==============================] - 14s 37us/step\n",
      "771599/771599 [==============================] - 28s 36us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 47s 60us/step - loss: 13.2963 - val_loss: 2.4723\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 34s 43us/step - loss: 3.4060 - val_loss: 1.6920\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 33s 42us/step - loss: 2.2668 - val_loss: 1.4285\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 32s 42us/step - loss: 1.9584 - val_loss: 0.8870\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 33s 43us/step - loss: 1.7612 - val_loss: 0.8232\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 34s 43us/step - loss: 1.6074 - val_loss: 0.7945\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 32s 42us/step - loss: 1.4659 - val_loss: 0.7112\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 1.3513 - val_loss: 0.6785\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 31s 41us/step - loss: 1.2424 - val_loss: 0.6204\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 1.1127 - val_loss: 0.4870\n",
      "385800/385800 [==============================] - 11s 27us/step\n",
      "771598/771598 [==============================] - 21s 27us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 44s 57us/step - loss: 11.9813 - val_loss: 2.6210\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 32s 42us/step - loss: 3.2595 - val_loss: 1.4771\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 33s 42us/step - loss: 2.2919 - val_loss: 1.0879\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 33s 43us/step - loss: 1.9474 - val_loss: 0.8876\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 33s 43us/step - loss: 1.7507 - val_loss: 0.8010\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 33s 42us/step - loss: 1.5793 - val_loss: 0.7755\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 33s 43us/step - loss: 1.3788 - val_loss: 0.5573\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 1.1191 - val_loss: 0.4449\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 0.9775 - val_loss: 0.4008\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 0.8668 - val_loss: 0.3722\n",
      "385799/385799 [==============================] - 12s 30us/step\n",
      "771599/771599 [==============================] - 22s 29us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 13.5303 - val_loss: 2.7755\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 3.4774 - val_loss: 1.5371\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 2.3529 - val_loss: 1.1251\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 45s 59us/step - loss: 1.9809 - val_loss: 0.8823\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 40s 52us/step - loss: 1.7788 - val_loss: 0.8127\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 41s 54us/step - loss: 1.6069 - val_loss: 0.8491\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 1.4672 - val_loss: 0.6976\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 40s 52us/step - loss: 1.3416 - val_loss: 0.6585\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 1.2233 - val_loss: 0.6002\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 1.0546 - val_loss: 0.5174\n",
      "385799/385799 [==============================] - 16s 41us/step\n",
      "771599/771599 [==============================] - 29s 37us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 12.5675 - val_loss: 2.5843\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 40s 52us/step - loss: 3.6779 - val_loss: 1.5044\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 41s 53us/step - loss: 2.2972 - val_loss: 1.0479\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 41s 53us/step - loss: 1.9322 - val_loss: 0.8700\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 41s 53us/step - loss: 1.7385 - val_loss: 0.8632\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 41s 53us/step - loss: 1.5738 - val_loss: 0.7171\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 43s 56us/step - loss: 1.4345 - val_loss: 0.6727\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 42s 55us/step - loss: 1.3131 - val_loss: 0.6635\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 43s 55us/step - loss: 1.2017 - val_loss: 0.6428\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 40s 52us/step - loss: 1.1113 - val_loss: 0.6413\n",
      "385800/385800 [==============================] - 14s 36us/step\n",
      "771598/771598 [==============================] - 29s 37us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 13.5620 - val_loss: 2.6852\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 3.3519 - val_loss: 1.6047\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 43s 55us/step - loss: 2.3010 - val_loss: 1.0905\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 42s 54us/step - loss: 1.9778 - val_loss: 0.8796\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 1.7752 - val_loss: 0.9788\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 42s 54us/step - loss: 1.6095 - val_loss: 0.7318\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 43s 55us/step - loss: 1.4720 - val_loss: 0.7333\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 35s 45us/step - loss: 1.3541 - val_loss: 0.6971\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 33s 43us/step - loss: 1.1303 - val_loss: 0.4874\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 0.9884 - val_loss: 0.4901\n",
      "385799/385799 [==============================] - 11s 30us/step\n",
      "771599/771599 [==============================] - 22s 29us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 49s 64us/step - loss: 12.0521 - val_loss: 2.6553\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 34s 45us/step - loss: 3.3809 - val_loss: 1.5335\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 34s 44us/step - loss: 2.3073 - val_loss: 1.0996\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 35s 45us/step - loss: 1.8306 - val_loss: 0.6549\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 37s 48us/step - loss: 1.5306 - val_loss: 0.5291\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 1.3537 - val_loss: 0.4869\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 40s 52us/step - loss: 1.2038 - val_loss: 0.4464\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 40s 52us/step - loss: 1.0754 - val_loss: 0.4073\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 41s 54us/step - loss: 0.9632 - val_loss: 0.3780\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 46s 59us/step - loss: 0.8711 - val_loss: 0.3542\n",
      "385799/385799 [==============================] - 13s 34us/step\n",
      "771599/771599 [==============================] - 24s 31us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 177.8386 - val_loss: 159.9875\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 143.9660 - val_loss: 128.7503\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 115.0818 - val_loss: 102.1368\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 33s 42us/step - loss: 90.6066 - val_loss: 79.7619\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 32s 41us/step - loss: 70.2514 - val_loss: 61.4100\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 35s 45us/step - loss: 53.8535 - val_loss: 46.9474\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 40s 51us/step - loss: 41.2523 - val_loss: 36.1832\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 38s 49us/step - loss: 32.2459 - val_loss: 28.8943\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 39s 50us/step - loss: 26.5313 - val_loss: 24.6759\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 36s 46us/step - loss: 23.5995 - val_loss: 22.8939\n",
      "385800/385800 [==============================] - 14s 37us/step\n",
      "771598/771598 [==============================] - 30s 39us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 55s 71us/step - loss: 177.7816 - val_loss: 159.9884\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 143.9089 - val_loss: 128.7606\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 115.0247 - val_loss: 102.1399\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 90.5475 - val_loss: 79.7673\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 70.1973 - val_loss: 61.4214\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 53.7922 - val_loss: 46.9438\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 41.1892 - val_loss: 36.1894\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 37s 47us/step - loss: 32.1832 - val_loss: 28.8928\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 42s 54us/step - loss: 26.4719 - val_loss: 24.6843\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 40s 52us/step - loss: 23.5346 - val_loss: 22.8909\n",
      "385799/385799 [==============================] - 14s 37us/step\n",
      "771599/771599 [==============================] - 28s 36us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 178.1132 - val_loss: 159.9913\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 144.2038 - val_loss: 128.7474\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 115.2903 - val_loss: 102.1376\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 40s 51us/step - loss: 90.7889 - val_loss: 79.7614\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 40s 52us/step - loss: 70.4103 - val_loss: 61.4131\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 40s 51us/step - loss: 53.9831 - val_loss: 46.9419\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 44s 57us/step - loss: 41.3513 - val_loss: 36.1764\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 36s 47us/step - loss: 32.3237 - val_loss: 28.8884\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 35s 46us/step - loss: 26.5823 - val_loss: 24.6686\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 35s 45us/step - loss: 23.6296 - val_loss: 22.8840\n",
      "385799/385799 [==============================] - 12s 30us/step\n",
      "771599/771599 [==============================] - 24s 31us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 63s 82us/step - loss: 16.1120 - val_loss: 3.7509\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 4.9047 - val_loss: 2.4360\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 45s 58us/step - loss: 3.3545 - val_loss: 1.6928\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 2.7484 - val_loss: 1.2665\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 2.4177 - val_loss: 1.0449\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 2.1572 - val_loss: 0.8883\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 1.9393 - val_loss: 0.8312\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 45s 58us/step - loss: 1.7732 - val_loss: 0.7771\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 46s 59us/step - loss: 1.6191 - val_loss: 0.6510\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 1.4955 - val_loss: 0.6597\n",
      "385800/385800 [==============================] - 14s 35us/step\n",
      "771598/771598 [==============================] - 28s 36us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 60s 77us/step - loss: 17.4510 - val_loss: 3.6714\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 44s 57us/step - loss: 4.6147 - val_loss: 2.2622\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 43s 56us/step - loss: 3.2717 - val_loss: 1.5336\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 43s 55us/step - loss: 2.7487 - val_loss: 1.2167\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 43s 55us/step - loss: 2.4235 - val_loss: 1.0346\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 43s 55us/step - loss: 2.1567 - val_loss: 0.8947\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 1.9459 - val_loss: 0.7759\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 43s 55us/step - loss: 1.7735 - val_loss: 0.7023\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 1.6197 - val_loss: 0.7328\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 1.5047 - val_loss: 0.6715\n",
      "385799/385799 [==============================] - 15s 38us/step\n",
      "771599/771599 [==============================] - 28s 37us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 60s 78us/step - loss: 17.8700 - val_loss: 3.8561\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 4.8755 - val_loss: 2.4065\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 44s 57us/step - loss: 3.3477 - val_loss: 1.6616\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 46s 60us/step - loss: 2.8380 - val_loss: 1.3258\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 44s 57us/step - loss: 2.4670 - val_loss: 1.0527\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 43s 56us/step - loss: 2.1842 - val_loss: 0.9049\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 43s 56us/step - loss: 1.9600 - val_loss: 0.8119\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 43s 55us/step - loss: 1.7743 - val_loss: 0.7747\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 41s 53us/step - loss: 1.6259 - val_loss: 0.6879\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 1.4931 - val_loss: 0.6726\n",
      "385799/385799 [==============================] - 13s 34us/step\n",
      "771599/771599 [==============================] - 26s 34us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 61s 79us/step - loss: 15.9605 - val_loss: 3.6387\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 42s 55us/step - loss: 4.9472 - val_loss: 2.4339\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 3.3832 - val_loss: 1.6974\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 43s 56us/step - loss: 2.8145 - val_loss: 1.3428\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 38s 50us/step - loss: 2.4525 - val_loss: 1.0508\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 2.1779 - val_loss: 0.9469\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 43s 56us/step - loss: 1.9631 - val_loss: 0.8162\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 41s 53us/step - loss: 1.7841 - val_loss: 0.7396\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 40s 51us/step - loss: 1.6249 - val_loss: 0.6843\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 40s 52us/step - loss: 1.4934 - val_loss: 0.6476\n",
      "385800/385800 [==============================] - 12s 32us/step\n",
      "771598/771598 [==============================] - 28s 36us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 55s 72us/step - loss: 16.0765 - val_loss: 3.7999\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 40s 52us/step - loss: 4.8190 - val_loss: 2.3386\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 40s 52us/step - loss: 3.2410 - val_loss: 1.5440\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 40s 52us/step - loss: 2.8082 - val_loss: 1.3725\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 40s 52us/step - loss: 2.5801 - val_loss: 1.3126\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 2.3062 - val_loss: 0.9842\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 40s 51us/step - loss: 2.0052 - val_loss: 0.8591\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 40s 51us/step - loss: 1.8073 - val_loss: 0.7523\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 40s 52us/step - loss: 1.6611 - val_loss: 0.6955\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 42s 54us/step - loss: 1.5221 - val_loss: 0.6224\n",
      "385799/385799 [==============================] - 14s 36us/step\n",
      "771599/771599 [==============================] - 27s 35us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 16.3741 - val_loss: 4.0758\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 4.9389 - val_loss: 2.3537\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 3.3270 - val_loss: 1.5955\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 2.7732 - val_loss: 1.2898\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 43s 55us/step - loss: 2.4227 - val_loss: 1.0574\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 43s 56us/step - loss: 2.1556 - val_loss: 0.9177\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 43s 55us/step - loss: 1.9294 - val_loss: 0.9045\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 43s 56us/step - loss: 1.7533 - val_loss: 0.7236\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 1.6013 - val_loss: 0.6507\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 1.4726 - val_loss: 0.6630\n",
      "385799/385799 [==============================] - 14s 38us/step\n",
      "771599/771599 [==============================] - 26s 33us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 59s 77us/step - loss: 177.8422 - val_loss: 159.9845\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 143.9661 - val_loss: 128.7514\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 43s 56us/step - loss: 115.0880 - val_loss: 102.1429\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 43s 56us/step - loss: 90.6112 - val_loss: 79.7670\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 43s 56us/step - loss: 70.2562 - val_loss: 61.4190\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 43s 55us/step - loss: 53.8580 - val_loss: 46.9500\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 44s 57us/step - loss: 41.2553 - val_loss: 36.1884\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 43s 56us/step - loss: 32.2522 - val_loss: 28.8976\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 43s 56us/step - loss: 26.5341 - val_loss: 24.6770\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 43s 56us/step - loss: 23.5999 - val_loss: 22.8926\n",
      "385800/385800 [==============================] - 14s 36us/step\n",
      "771598/771598 [==============================] - 28s 36us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 177.7773 - val_loss: 159.9863\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 143.9034 - val_loss: 128.7519\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 43s 56us/step - loss: 115.0185 - val_loss: 102.1375\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 43s 55us/step - loss: 90.5429 - val_loss: 79.7629\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 70.1924 - val_loss: 61.4180\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 43s 55us/step - loss: 53.7906 - val_loss: 46.9445\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 41.1883 - val_loss: 36.1850\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 32.1823 - val_loss: 28.8928\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 26.4663 - val_loss: 24.6766\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 23.5307 - val_loss: 22.8874\n",
      "385799/385799 [==============================] - 13s 35us/step\n",
      "771599/771599 [==============================] - 27s 35us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 178.1259 - val_loss: 160.0067\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 44s 57us/step - loss: 144.2202 - val_loss: 128.7650\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 44s 57us/step - loss: 115.3047 - val_loss: 102.1472\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 46s 59us/step - loss: 90.7938 - val_loss: 79.7645\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 45s 59us/step - loss: 70.4151 - val_loss: 61.4176\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 53.9818 - val_loss: 46.9413\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 41.3513 - val_loss: 36.1768\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 46s 60us/step - loss: 32.3249 - val_loss: 28.8885\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 26.5853 - val_loss: 24.6720\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 44s 58us/step - loss: 23.6310 - val_loss: 22.8822\n",
      "385799/385799 [==============================] - 14s 37us/step\n",
      "771599/771599 [==============================] - 28s 37us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 65s 84us/step - loss: 21.4560 - val_loss: 4.9396\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 51s 67us/step - loss: 5.9414 - val_loss: 3.3897\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 50s 65us/step - loss: 4.4551 - val_loss: 2.4230\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 51s 66us/step - loss: 3.7896 - val_loss: 1.9922\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 51s 66us/step - loss: 3.4011 - val_loss: 1.6903\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 50s 65us/step - loss: 3.0625 - val_loss: 1.4918\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 50s 65us/step - loss: 2.7953 - val_loss: 1.3034\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 50s 65us/step - loss: 2.5562 - val_loss: 1.2491\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 50s 65us/step - loss: 2.3584 - val_loss: 1.1231\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 50s 64us/step - loss: 2.1861 - val_loss: 1.0737\n",
      "385800/385800 [==============================] - 15s 38us/step\n",
      "771598/771598 [==============================] - 29s 38us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 20.0565 - val_loss: 4.8278\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 5.7673 - val_loss: 3.2438\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 51s 66us/step - loss: 4.3765 - val_loss: 2.4749\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 50s 65us/step - loss: 3.8074 - val_loss: 2.1111\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 51s 66us/step - loss: 3.4148 - val_loss: 1.7838\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 3.0549 - val_loss: 1.6255\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 55s 71us/step - loss: 2.7421 - val_loss: 1.4469\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 51s 66us/step - loss: 2.5011 - val_loss: 1.2880\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 51s 66us/step - loss: 2.3188 - val_loss: 1.2596\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 48s 62us/step - loss: 2.1546 - val_loss: 1.0917\n",
      "385799/385799 [==============================] - 14s 37us/step\n",
      "771599/771599 [==============================] - 29s 38us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 20.4216 - val_loss: 4.6063\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 48s 62us/step - loss: 5.6465 - val_loss: 3.0582\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 48s 62us/step - loss: 4.3266 - val_loss: 2.3936\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 50s 64us/step - loss: 3.7596 - val_loss: 1.9445\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 50s 64us/step - loss: 3.3594 - val_loss: 1.6781\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 50s 65us/step - loss: 3.0490 - val_loss: 1.5165\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 49s 64us/step - loss: 2.7625 - val_loss: 1.3090\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 50s 65us/step - loss: 2.5185 - val_loss: 1.2012\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 49s 64us/step - loss: 2.3153 - val_loss: 1.2182\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 49s 64us/step - loss: 2.1519 - val_loss: 1.0653\n",
      "385799/385799 [==============================] - 14s 38us/step\n",
      "771599/771599 [==============================] - 29s 38us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 65s 85us/step - loss: 20.4574 - val_loss: 4.6440\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 51s 66us/step - loss: 5.6228 - val_loss: 2.9745\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 51s 66us/step - loss: 4.2629 - val_loss: 2.3532\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 52s 67us/step - loss: 3.7142 - val_loss: 1.8552\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 51s 66us/step - loss: 3.3302 - val_loss: 1.5752\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 52s 67us/step - loss: 3.0225 - val_loss: 1.4107\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 51s 66us/step - loss: 2.7350 - val_loss: 1.2153\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 51s 66us/step - loss: 2.4829 - val_loss: 1.1316\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 51s 66us/step - loss: 2.2981 - val_loss: 1.0434\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 51s 66us/step - loss: 2.1332 - val_loss: 0.9691\n",
      "385800/385800 [==============================] - 15s 39us/step\n",
      "771598/771598 [==============================] - 30s 39us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 21.5050 - val_loss: 3.7397\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 49s 63us/step - loss: 5.1560 - val_loss: 2.8282\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 49s 64us/step - loss: 4.2385 - val_loss: 2.3379\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 49s 64us/step - loss: 3.7298 - val_loss: 1.9596\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 49s 63us/step - loss: 3.3742 - val_loss: 1.7808\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 49s 63us/step - loss: 3.0888 - val_loss: 1.5663\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 49s 64us/step - loss: 2.8288 - val_loss: 1.4264\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 49s 63us/step - loss: 2.5802 - val_loss: 1.3396\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 49s 63us/step - loss: 2.3720 - val_loss: 1.2514\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 49s 63us/step - loss: 2.2002 - val_loss: 1.1151\n",
      "385799/385799 [==============================] - 14s 37us/step\n",
      "771599/771599 [==============================] - 28s 37us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 67s 86us/step - loss: 18.9645 - val_loss: 4.9878\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 5.7925 - val_loss: 3.2218\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 4.4107 - val_loss: 2.4226\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 3.7124 - val_loss: 1.8583\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 3.2631 - val_loss: 1.7152\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 2.9367 - val_loss: 1.3458\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 2.6657 - val_loss: 1.2395\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 2.4336 - val_loss: 1.1089\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 54s 69us/step - loss: 2.2385 - val_loss: 1.1879\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 2.0834 - val_loss: 1.0208\n",
      "385799/385799 [==============================] - 15s 39us/step\n",
      "771599/771599 [==============================] - 30s 39us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 67s 86us/step - loss: 177.8480 - val_loss: 159.9990\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 52s 68us/step - loss: 143.9757 - val_loss: 128.7559\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 49s 63us/step - loss: 115.0910 - val_loss: 102.1464\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 90.6196 - val_loss: 79.7756\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 70.2603 - val_loss: 61.4153\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 53.8577 - val_loss: 46.9489\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 41.2555 - val_loss: 36.1887\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 32.2520 - val_loss: 28.8963\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 26.5365 - val_loss: 24.6830\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 23.5990 - val_loss: 22.8895\n",
      "385800/385800 [==============================] - 13s 34us/step\n",
      "771598/771598 [==============================] - 26s 34us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 61s 80us/step - loss: 177.7991 - val_loss: 160.0080\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 47s 61us/step - loss: 143.9179 - val_loss: 128.7647\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 47s 61us/step - loss: 115.0348 - val_loss: 102.1551\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 47s 61us/step - loss: 90.5592 - val_loss: 79.7788\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 47s 60us/step - loss: 70.2025 - val_loss: 61.4244\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 47s 60us/step - loss: 53.7967 - val_loss: 46.9501\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 47s 61us/step - loss: 41.1918 - val_loss: 36.1878\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 47s 60us/step - loss: 32.1810 - val_loss: 28.8903\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 47s 61us/step - loss: 26.4698 - val_loss: 24.6823\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 47s 61us/step - loss: 23.5348 - val_loss: 22.8903\n",
      "385799/385799 [==============================] - 13s 35us/step\n",
      "771599/771599 [==============================] - 26s 34us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 61s 80us/step - loss: 178.1268 - val_loss: 160.0103\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 46s 60us/step - loss: 144.2188 - val_loss: 128.7604\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 46s 60us/step - loss: 115.3031 - val_loss: 102.1481\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 46s 60us/step - loss: 90.7968 - val_loss: 79.7698\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 46s 60us/step - loss: 70.4124 - val_loss: 61.4144\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 46s 60us/step - loss: 53.9811 - val_loss: 46.9414\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 46s 60us/step - loss: 41.3577 - val_loss: 36.1863\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 47s 60us/step - loss: 32.3273 - val_loss: 28.8887\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 46s 60us/step - loss: 26.5878 - val_loss: 24.6737\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 47s 60us/step - loss: 23.6303 - val_loss: 22.8843\n",
      "385799/385799 [==============================] - 13s 34us/step\n",
      "771599/771599 [==============================] - 26s 34us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 54s 70us/step - loss: 16.0879 - val_loss: 3.8228\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 5.9076 - val_loss: 2.6829\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 4.4394 - val_loss: 1.9633\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 3.6470 - val_loss: 1.6185\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 3.2207 - val_loss: 1.4506\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 2.8720 - val_loss: 1.3269\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 2.5093 - val_loss: 1.1269\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 2.1538 - val_loss: 0.9969\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 39s 51us/step - loss: 1.9215 - val_loss: 0.9794\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 39s 50us/step - loss: 1.7733 - val_loss: 0.9160\n",
      "385800/385800 [==============================] - 12s 31us/step\n",
      "771598/771598 [==============================] - 24s 31us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 18.5090 - val_loss: 4.2831\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 7.7373 - val_loss: 2.9120\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 6.1247 - val_loss: 2.1608\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 4.8781 - val_loss: 1.8598\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 4.1566 - val_loss: 1.5760\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 3.5913 - val_loss: 1.4818\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 3.0977 - val_loss: 1.2719\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 2.5779 - val_loss: 1.1326\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 2.3209 - val_loss: 1.1137\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 39s 51us/step - loss: 2.2048 - val_loss: 1.0438\n",
      "385799/385799 [==============================] - 12s 32us/step\n",
      "771599/771599 [==============================] - 25s 32us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 15.8032 - val_loss: 4.0192\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 6.1385 - val_loss: 2.7397\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 39s 50us/step - loss: 4.5675 - val_loss: 2.1082\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 42s 55us/step - loss: 3.7176 - val_loss: 1.7639\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 47s 61us/step - loss: 3.2039 - val_loss: 1.4946\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 45s 59us/step - loss: 2.8556 - val_loss: 1.3769\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 45s 59us/step - loss: 2.4372 - val_loss: 1.1052\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 45s 59us/step - loss: 2.0882 - val_loss: 0.9846\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 45s 59us/step - loss: 1.8663 - val_loss: 0.9177\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 45s 59us/step - loss: 1.7366 - val_loss: 0.9191\n",
      "385799/385799 [==============================] - 15s 38us/step\n",
      "771599/771599 [==============================] - 30s 38us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 62s 80us/step - loss: 16.5587 - val_loss: 4.2265\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 6.4075 - val_loss: 2.8343\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 47s 60us/step - loss: 4.7498 - val_loss: 2.0969\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 3.8728 - val_loss: 1.7296\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 3.3308 - val_loss: 1.4156\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 2.8280 - val_loss: 1.1979\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 2.4550 - val_loss: 1.0885\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 2.1548 - val_loss: 0.9425\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 47s 60us/step - loss: 1.9157 - val_loss: 0.9652\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 46s 60us/step - loss: 1.7930 - val_loss: 0.9338\n",
      "385800/385800 [==============================] - 15s 39us/step\n",
      "771598/771598 [==============================] - 30s 39us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 60s 78us/step - loss: 17.5101 - val_loss: 3.5596\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 47s 60us/step - loss: 6.2639 - val_loss: 2.3709\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 47s 60us/step - loss: 4.4398 - val_loss: 2.0234\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 46s 59us/step - loss: 3.7269 - val_loss: 1.5908\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 46s 59us/step - loss: 3.3488 - val_loss: 1.4771\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 46s 59us/step - loss: 2.9958 - val_loss: 1.3539\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 45s 59us/step - loss: 2.6650 - val_loss: 1.2760\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 45s 58us/step - loss: 2.3813 - val_loss: 1.1912\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 47s 62us/step - loss: 2.0674 - val_loss: 0.9936\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 51s 66us/step - loss: 1.8817 - val_loss: 0.9933\n",
      "385799/385799 [==============================] - 25s 65us/step\n",
      "771599/771599 [==============================] - 44s 57us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 81s 105us/step - loss: 17.3662 - val_loss: 3.8841\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 59s 77us/step - loss: 6.2958 - val_loss: 2.7125\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 61s 79us/step - loss: 4.6530 - val_loss: 2.0864\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 78s 101us/step - loss: 3.8238 - val_loss: 1.6537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 123s 160us/step - loss: 3.3636 - val_loss: 1.6385\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 99s 129us/step - loss: 3.0094 - val_loss: 1.4258\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 116s 151us/step - loss: 2.5735 - val_loss: 1.1372\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 101s 130us/step - loss: 2.2146 - val_loss: 1.0861\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 122s 159us/step - loss: 1.9796 - val_loss: 0.9572\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 130s 168us/step - loss: 1.8399 - val_loss: 0.9198\n",
      "385799/385799 [==============================] - 43s 112us/step\n",
      "771599/771599 [==============================] - 74s 95us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 156s 202us/step - loss: 177.8452 - val_loss: 159.9948\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 121s 157us/step - loss: 143.9673 - val_loss: 128.7491\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 120s 156us/step - loss: 115.0853 - val_loss: 102.1410\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 130s 168us/step - loss: 90.6062 - val_loss: 79.7629\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 106s 137us/step - loss: 70.2550 - val_loss: 61.4167\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 75s 98us/step - loss: 53.8582 - val_loss: 46.9508\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 94s 122us/step - loss: 41.2557 - val_loss: 36.1891\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 92s 119us/step - loss: 32.2508 - val_loss: 28.8945\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 96s 124us/step - loss: 26.5342 - val_loss: 24.6795\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 92s 120us/step - loss: 23.5972 - val_loss: 22.8906\n",
      "385800/385800 [==============================] - 35s 90us/step\n",
      "771598/771598 [==============================] - 64s 83us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 133s 172us/step - loss: 177.7961 - val_loss: 160.0044\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 93s 121us/step - loss: 143.9162 - val_loss: 128.7639\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 108s 139us/step - loss: 115.0376 - val_loss: 102.1571\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 74s 95us/step - loss: 90.5556 - val_loss: 79.7706\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 96s 124us/step - loss: 70.1964 - val_loss: 61.4207\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 102s 132us/step - loss: 53.7933 - val_loss: 46.9485\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 103s 133us/step - loss: 41.1925 - val_loss: 36.1911\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 95s 124us/step - loss: 32.1893 - val_loss: 28.8990\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 100s 130us/step - loss: 26.4720 - val_loss: 24.6840\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 97s 126us/step - loss: 23.5370 - val_loss: 22.8933\n",
      "385799/385799 [==============================] - 32s 84us/step\n",
      "771599/771599 [==============================] - 63s 81us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 130s 169us/step - loss: 178.1246 - val_loss: 160.0046\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 106s 137us/step - loss: 144.2143 - val_loss: 128.7585\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 114s 148us/step - loss: 115.3006 - val_loss: 102.1450\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 118s 154us/step - loss: 90.7936 - val_loss: 79.7646\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 111s 144us/step - loss: 70.4102 - val_loss: 61.4100\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 106s 137us/step - loss: 53.9796 - val_loss: 46.9404\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 95s 123us/step - loss: 41.3518 - val_loss: 36.1801\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 103s 133us/step - loss: 32.3253 - val_loss: 28.8887\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 103s 134us/step - loss: 26.5850 - val_loss: 24.6711\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 96s 125us/step - loss: 23.6321 - val_loss: 22.8857\n",
      "385799/385799 [==============================] - 39s 102us/step\n",
      "771599/771599 [==============================] - 96s 125us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 148s 192us/step - loss: 19.6271 - val_loss: 5.5335\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 111s 144us/step - loss: 6.8953 - val_loss: 3.8387\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 109s 141us/step - loss: 4.9115 - val_loss: 2.5814\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 102s 132us/step - loss: 4.1151 - val_loss: 2.3339\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 108s 140us/step - loss: 3.7471 - val_loss: 2.0420\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 104s 135us/step - loss: 3.3322 - val_loss: 1.7412\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 105s 136us/step - loss: 2.9993 - val_loss: 1.5440\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 92s 119us/step - loss: 2.7332 - val_loss: 1.4528\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 96s 124us/step - loss: 2.5037 - val_loss: 1.3461\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 119s 155us/step - loss: 2.3058 - val_loss: 1.2758\n",
      "385800/385800 [==============================] - 50s 129us/step\n",
      "771598/771598 [==============================] - 57s 74us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 126s 163us/step - loss: 22.9605 - val_loss: 4.3272\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 79s 103us/step - loss: 7.3567 - val_loss: 3.3084\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 5.5761 - val_loss: 2.7163\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 93s 120us/step - loss: 4.8962 - val_loss: 2.2727\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 88s 114us/step - loss: 4.3539 - val_loss: 2.1225\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 80s 104us/step - loss: 3.8851 - val_loss: 1.8904\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 120s 156us/step - loss: 3.4712 - val_loss: 1.7036\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 106s 137us/step - loss: 3.0998 - val_loss: 1.5793\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 109s 141us/step - loss: 2.7799 - val_loss: 1.4661\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 111s 144us/step - loss: 2.5565 - val_loss: 1.4342\n",
      "385799/385799 [==============================] - 33s 86us/step\n",
      "771599/771599 [==============================] - 66s 86us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 121s 156us/step - loss: 23.5779 - val_loss: 4.6980\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 83s 107us/step - loss: 9.3708 - val_loss: 3.6297\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 103s 134us/step - loss: 6.8291 - val_loss: 2.6312\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 105s 136us/step - loss: 5.7616 - val_loss: 2.3610\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 108s 140us/step - loss: 5.0685 - val_loss: 1.8290\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 111s 144us/step - loss: 4.3134 - val_loss: 1.5685\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 97s 126us/step - loss: 3.7590 - val_loss: 1.5408\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 98s 127us/step - loss: 3.2902 - val_loss: 1.4722\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 101s 131us/step - loss: 2.9507 - val_loss: 1.4065\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 102s 133us/step - loss: 2.7797 - val_loss: 1.2400\n",
      "385799/385799 [==============================] - 33s 86us/step\n",
      "771599/771599 [==============================] - 63s 82us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 139s 180us/step - loss: 21.0363 - val_loss: 5.3605 0s - loss:\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 110s 142us/step - loss: 7.5338 - val_loss: 4.1475\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 109s 141us/step - loss: 5.6249 - val_loss: 2.8806\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 90s 116us/step - loss: 4.6490 - val_loss: 2.4304\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 82s 106us/step - loss: 4.1369 - val_loss: 2.1053\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 79s 102us/step - loss: 3.6760 - val_loss: 1.8170\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 84s 109us/step - loss: 3.2338 - val_loss: 1.9205\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 83s 107us/step - loss: 2.8875 - val_loss: 1.6628\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 85s 110us/step - loss: 2.6246 - val_loss: 1.5852\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 84s 109us/step - loss: 2.4073 - val_loss: 1.4929\n",
      "385800/385800 [==============================] - 26s 68us/step\n",
      "771598/771598 [==============================] - 47s 61us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 114s 147us/step - loss: 20.0582 - val_loss: 4.6196\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 83s 107us/step - loss: 7.0542 - val_loss: 3.4044\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 87s 113us/step - loss: 5.3524 - val_loss: 2.5529\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 84s 109us/step - loss: 4.5923 - val_loss: 2.1961\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 4.1543 - val_loss: 1.9438\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 3.7117 - val_loss: 1.7927\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 3.3027 - val_loss: 1.6066\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 104s 134us/step - loss: 2.9408 - val_loss: 1.4537\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 83s 107us/step - loss: 2.6652 - val_loss: 1.3983\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 2.4735 - val_loss: 1.3348\n",
      "385799/385799 [==============================] - 42s 109us/step\n",
      "771599/771599 [==============================] - 95s 123us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 180s 233us/step - loss: 21.9372 - val_loss: 4.7753\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 111s 144us/step - loss: 7.7599 - val_loss: 3.6244\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 134s 174us/step - loss: 5.7584 - val_loss: 2.8137\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 124s 161us/step - loss: 4.8524 - val_loss: 2.2729\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 112s 145us/step - loss: 4.3059 - val_loss: 2.0746\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 112s 145us/step - loss: 3.8234 - val_loss: 1.7878\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 117s 151us/step - loss: 3.3753 - val_loss: 1.5901\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 100s 130us/step - loss: 3.0289 - val_loss: 1.5741\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 117s 152us/step - loss: 2.7310 - val_loss: 1.4098\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 111s 144us/step - loss: 2.5220 - val_loss: 1.4080\n",
      "385799/385799 [==============================] - 34s 89us/step\n",
      "771599/771599 [==============================] - 71s 92us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 147s 190us/step - loss: 177.8385 - val_loss: 159.9862\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 112s 145us/step - loss: 143.9626 - val_loss: 128.7474\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 126s 163us/step - loss: 115.0805 - val_loss: 102.1348\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 117s 151us/step - loss: 90.6071 - val_loss: 79.7610\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 118s 152us/step - loss: 70.2513 - val_loss: 61.4094\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 104s 135us/step - loss: 53.8542 - val_loss: 46.9476\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 119s 154us/step - loss: 41.2556 - val_loss: 36.1911\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 110s 142us/step - loss: 32.2498 - val_loss: 28.8929\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 111s 143us/step - loss: 26.5330 - val_loss: 24.6795\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 112s 145us/step - loss: 23.6015 - val_loss: 22.8928\n",
      "385800/385800 [==============================] - 31s 79us/step\n",
      "771598/771598 [==============================] - 68s 88us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 144s 186us/step - loss: 177.7926 - val_loss: 159.9996\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 108s 141us/step - loss: 143.9130 - val_loss: 128.7585\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 105s 136us/step - loss: 115.0302 - val_loss: 102.1523\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 112s 146us/step - loss: 90.5522 - val_loss: 79.7709\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 108s 140us/step - loss: 70.1987 - val_loss: 61.4209\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 107s 138us/step - loss: 53.7962 - val_loss: 46.9515\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 107s 138us/step - loss: 41.1936 - val_loss: 36.1917\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 96s 124us/step - loss: 32.1910 - val_loss: 28.8980\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 111s 144us/step - loss: 26.4716 - val_loss: 24.6822\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 107s 139us/step - loss: 23.5374 - val_loss: 22.8933\n",
      "385799/385799 [==============================] - 30s 79us/step\n",
      "771599/771599 [==============================] - 56s 72us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 114s 147us/step - loss: 178.1376 - val_loss: 160.0161\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 84s 109us/step - loss: 144.2293 - val_loss: 128.7735\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 115.3163 - val_loss: 102.1574\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 86s 111us/step - loss: 90.8054 - val_loss: 79.7743\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 84s 109us/step - loss: 70.4199 - val_loss: 61.4230\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 84s 109us/step - loss: 53.9900 - val_loss: 46.9473\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 99s 128us/step - loss: 41.3596 - val_loss: 36.1856\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 111s 144us/step - loss: 32.3255 - val_loss: 28.8845\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 101s 131us/step - loss: 26.5856 - val_loss: 24.6720\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 95s 124us/step - loss: 23.6327 - val_loss: 22.8873\n",
      "385799/385799 [==============================] - 27s 69us/step\n",
      "771599/771599 [==============================] - 66s 85us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 138s 179us/step - loss: 30.3722 - val_loss: 4.6107\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 72s 94us/step - loss: 7.5073 - val_loss: 3.3809\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 61s 80us/step - loss: 6.0000 - val_loss: 2.6843\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 61s 80us/step - loss: 5.3427 - val_loss: 2.3440\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 61s 80us/step - loss: 4.9066 - val_loss: 2.2347\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 62s 81us/step - loss: 4.4263 - val_loss: 1.8853\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 62s 81us/step - loss: 3.9449 - val_loss: 1.7204\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 60s 78us/step - loss: 3.5275 - val_loss: 1.6467\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 62s 80us/step - loss: 3.1893 - val_loss: 1.5200\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 62s 81us/step - loss: 2.8932 - val_loss: 1.5311\n",
      "385800/385800 [==============================] - 17s 44us/step\n",
      "771598/771598 [==============================] - 34s 44us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 80s 103us/step - loss: 30.3662 - val_loss: 4.4757\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 63s 81us/step - loss: 8.4948 - val_loss: 3.6990\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 6.7650 - val_loss: 3.3051\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 6.0929 - val_loss: 2.8254\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 5.5070 - val_loss: 2.3177\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 4.7954 - val_loss: 1.9711\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 4.2486 - val_loss: 1.8333\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 63s 81us/step - loss: 3.7303 - val_loss: 1.5697\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 3.3036 - val_loss: 1.4889\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 2.9840 - val_loss: 1.4256\n",
      "385799/385799 [==============================] - 17s 44us/step\n",
      "771599/771599 [==============================] - 34s 44us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 80s 104us/step - loss: 29.7432 - val_loss: 5.2481\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 10.0817 - val_loss: 4.1636\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 8.0144 - val_loss: 3.4027\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 62s 81us/step - loss: 6.7547 - val_loss: 2.7340\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 5.9335 - val_loss: 2.3483\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 5.1438 - val_loss: 2.1055\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 4.4685 - val_loss: 1.8487\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 62s 81us/step - loss: 3.8822 - val_loss: 1.6969\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 3.4205 - val_loss: 1.7412\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 62s 80us/step - loss: 3.0896 - val_loss: 1.4530\n",
      "385799/385799 [==============================] - 17s 44us/step\n",
      "771599/771599 [==============================] - 34s 44us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 80s 103us/step - loss: 25.8883 - val_loss: 5.3890\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 63s 81us/step - loss: 7.2660 - val_loss: 3.7561\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 63s 81us/step - loss: 5.7639 - val_loss: 3.1516\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 63s 81us/step - loss: 5.1296 - val_loss: 3.0096\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 63s 82us/step - loss: 4.7158 - val_loss: 2.4884\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 63s 81us/step - loss: 4.3007 - val_loss: 2.2353\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 63s 81us/step - loss: 3.8490 - val_loss: 1.9056\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 63s 81us/step - loss: 3.4510 - val_loss: 1.6622\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 63s 81us/step - loss: 3.1182 - val_loss: 1.5949\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 63s 81us/step - loss: 2.8343 - val_loss: 1.4902\n",
      "385800/385800 [==============================] - 17s 45us/step\n",
      "771598/771598 [==============================] - 35s 45us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 82s 106us/step - loss: 22.9391 - val_loss: 6.4773\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 8.2361 - val_loss: 4.4223\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 63s 81us/step - loss: 6.1718 - val_loss: 3.3911\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 64s 83us/step - loss: 5.2936 - val_loss: 2.9113\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 4.8010 - val_loss: 2.6207\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 4.3397 - val_loss: 2.3150\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 3.8874 - val_loss: 2.1990\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 3.5073 - val_loss: 1.9272\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 3.1851 - val_loss: 1.7642\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 2.9194 - val_loss: 1.7275\n",
      "385799/385799 [==============================] - 17s 45us/step\n",
      "771599/771599 [==============================] - 35s 45us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 82s 106us/step - loss: 26.8724 - val_loss: 5.0723\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 64s 83us/step - loss: 8.1640 - val_loss: 3.9637\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 64s 83us/step - loss: 6.5951 - val_loss: 3.2689\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 64s 83us/step - loss: 5.8796 - val_loss: 2.8590\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 64s 83us/step - loss: 5.3235 - val_loss: 2.4944\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 64s 83us/step - loss: 4.7904 - val_loss: 2.2936\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 64s 83us/step - loss: 4.3140 - val_loss: 2.1123\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 64s 83us/step - loss: 3.9113 - val_loss: 2.0450\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 64s 83us/step - loss: 3.5504 - val_loss: 1.9501\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 64s 83us/step - loss: 3.2586 - val_loss: 1.8831\n",
      "385799/385799 [==============================] - 18s 46us/step\n",
      "771599/771599 [==============================] - 35s 46us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771598/771598 [==============================] - 80s 104us/step - loss: 177.8762 - val_loss: 160.0250\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 61s 80us/step - loss: 143.9998 - val_loss: 128.7810\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 61s 79us/step - loss: 115.1100 - val_loss: 102.1601\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 61s 80us/step - loss: 90.6300 - val_loss: 79.7847\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 61s 80us/step - loss: 70.2690 - val_loss: 61.4273\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 62s 80us/step - loss: 53.8676 - val_loss: 46.9591\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 61s 79us/step - loss: 41.2645 - val_loss: 36.1930\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 61s 80us/step - loss: 32.2561 - val_loss: 28.9009\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 61s 80us/step - loss: 26.5390 - val_loss: 24.6839\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 64s 83us/step - loss: 23.6031 - val_loss: 22.8958\n",
      "385800/385800 [==============================] - 18s 47us/step\n",
      "771598/771598 [==============================] - 36s 47us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 82s 106us/step - loss: 177.7884 - val_loss: 159.9992\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 143.9125 - val_loss: 128.7573\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 115.0266 - val_loss: 102.1483\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 90.5538 - val_loss: 79.7735\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 70.1989 - val_loss: 61.4213\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 53.7956 - val_loss: 46.9495\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 64s 83us/step - loss: 41.1936 - val_loss: 36.1923\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 32.1875 - val_loss: 28.8953\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 26.4691 - val_loss: 24.6791\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 23.5343 - val_loss: 22.8897\n",
      "385799/385799 [==============================] - 18s 47us/step\n",
      "771599/771599 [==============================] - 36s 46us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 84s 109us/step - loss: 178.1132 - val_loss: 159.9938\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 144.2112 - val_loss: 128.7585\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 115.3009 - val_loss: 102.1461\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 90.7973 - val_loss: 79.7684\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 70.4113 - val_loss: 61.4116\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 65s 85us/step - loss: 53.9828 - val_loss: 46.9414\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 41.3551 - val_loss: 36.1807\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 32.3233 - val_loss: 28.8857\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 26.5822 - val_loss: 24.6676\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 65s 85us/step - loss: 23.6307 - val_loss: 22.8856\n",
      "385799/385799 [==============================] - 19s 48us/step\n",
      "771599/771599 [==============================] - 37s 48us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 72s 94us/step - loss: 19.4215 - val_loss: 5.1314\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 6.8411 - val_loss: 3.7775\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 5.5376 - val_loss: 3.0941\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 55s 72us/step - loss: 4.9203 - val_loss: 2.7336\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 4.4927 - val_loss: 2.3486\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 4.1029 - val_loss: 2.1622\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 3.7554 - val_loss: 1.8662\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 3.4827 - val_loss: 1.7385\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 3.2527 - val_loss: 1.6425\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 3.0511 - val_loss: 2.1484\n",
      "385800/385800 [==============================] - 16s 42us/step\n",
      "771598/771598 [==============================] - 33s 43us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 76s 99us/step - loss: 23.1588 - val_loss: 4.3950\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 7.7433 - val_loss: 3.3507\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 6.1484 - val_loss: 2.9520\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 5.6038 - val_loss: 2.7041\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 5.1508 - val_loss: 2.3952\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 4.7132 - val_loss: 2.4001\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 4.2215 - val_loss: 1.9513\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 3.8415 - val_loss: 1.8083\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 3.5244 - val_loss: 1.6510\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 57s 73us/step - loss: 3.2715 - val_loss: 1.5871\n",
      "385799/385799 [==============================] - 18s 46us/step\n",
      "771599/771599 [==============================] - 34s 44us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 74s 96us/step - loss: 25.9083 - val_loss: 4.1428\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 9.1773 - val_loss: 3.3644\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 7.0158 - val_loss: 2.8801\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 6.3985 - val_loss: 2.6488\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 5.7337 - val_loss: 2.2244\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 5.1482 - val_loss: 2.0808\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 4.6108 - val_loss: 1.9753\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 4.1380 - val_loss: 1.7976\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 3.7602 - val_loss: 1.7214\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 3.4692 - val_loss: 1.7866\n",
      "385799/385799 [==============================] - 17s 43us/step\n",
      "771599/771599 [==============================] - 34s 43us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 75s 97us/step - loss: 21.7269 - val_loss: 5.0712\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 57s 74us/step - loss: 7.5605 - val_loss: 3.8346\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 6.1340 - val_loss: 3.1168\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 56s 72us/step - loss: 5.4961 - val_loss: 2.8923\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 64s 83us/step - loss: 5.0083 - val_loss: 2.4463\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 66s 86us/step - loss: 4.5523 - val_loss: 2.2669\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 66s 86us/step - loss: 4.1344 - val_loss: 2.0434\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 56s 72us/step - loss: 3.7446 - val_loss: 1.8015\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 57s 73us/step - loss: 3.4423 - val_loss: 1.6888\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 57s 73us/step - loss: 3.1901 - val_loss: 1.7149\n",
      "385800/385800 [==============================] - 17s 45us/step\n",
      "771598/771598 [==============================] - 37s 48us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 79s 102us/step - loss: 19.7838 - val_loss: 5.1691\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 7.3438 - val_loss: 3.8692\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 5.7395 - val_loss: 3.0854\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 5.1124 - val_loss: 2.6423\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 4.6733 - val_loss: 2.4228\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 4.2280 - val_loss: 2.0654\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 3.8427 - val_loss: 1.9217\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 3.5446 - val_loss: 1.7631\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 3.2886 - val_loss: 1.7416\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 56s 73us/step - loss: 3.0730 - val_loss: 1.5478\n",
      "385799/385799 [==============================] - 17s 44us/step\n",
      "771599/771599 [==============================] - 34s 45us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 75s 97us/step - loss: 19.6382 - val_loss: 5.1221\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 6.9868 - val_loss: 3.8983\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 5.6456 - val_loss: 3.1122\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 4.9853 - val_loss: 2.6326\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 4.5651 - val_loss: 2.4859\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 4.1412 - val_loss: 2.0490\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 3.7894 - val_loss: 1.9049\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 3.5096 - val_loss: 1.7919\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 3.2649 - val_loss: 1.6013\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 3.0731 - val_loss: 1.6224\n",
      "385799/385799 [==============================] - 17s 44us/step\n",
      "771599/771599 [==============================] - 34s 44us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 75s 97us/step - loss: 177.8635 - val_loss: 160.0087\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 143.9843 - val_loss: 128.7688\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 115.0978 - val_loss: 102.1529\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 90.6230 - val_loss: 79.7786\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 70.2682 - val_loss: 61.4256\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 53.8644 - val_loss: 46.9555\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 41.2598 - val_loss: 36.1933\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 32.2541 - val_loss: 28.8979\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 26.5365 - val_loss: 24.6833\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 23.6034 - val_loss: 22.8950\n",
      "385800/385800 [==============================] - 17s 43us/step\n",
      "771598/771598 [==============================] - 34s 43us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 76s 98us/step - loss: 177.8014 - val_loss: 160.0145\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 57s 73us/step - loss: 143.9230 - val_loss: 128.7642\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 57s 73us/step - loss: 115.0346 - val_loss: 102.1521\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 57s 73us/step - loss: 90.5582 - val_loss: 79.7777\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 56s 72us/step - loss: 70.2039 - val_loss: 61.4286\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 51s 66us/step - loss: 53.8048 - val_loss: 46.9616\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 51s 66us/step - loss: 41.1978 - val_loss: 36.1886\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 51s 66us/step - loss: 32.1831 - val_loss: 28.8922\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 51s 66us/step - loss: 26.4676 - val_loss: 24.6792\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 51s 66us/step - loss: 23.5323 - val_loss: 22.8891\n",
      "385799/385799 [==============================] - 15s 39us/step\n",
      "771599/771599 [==============================] - 30s 39us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 72s 93us/step - loss: 178.1273 - val_loss: 160.0060\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 144.2174 - val_loss: 128.7621\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 115.3012 - val_loss: 102.1453\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 90.7938 - val_loss: 79.7618\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 70.4125 - val_loss: 61.4136\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 53.9817 - val_loss: 46.9396\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 41.3517 - val_loss: 36.1788\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 32.3253 - val_loss: 28.8874\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 26.5845 - val_loss: 24.6723\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 23.6325 - val_loss: 22.8856\n",
      "385799/385799 [==============================] - 16s 41us/step\n",
      "771599/771599 [==============================] - 30s 39us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 77s 100us/step - loss: 25.9028 - val_loss: 5.9613\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 58s 75us/step - loss: 8.5526 - val_loss: 4.5625\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 58s 75us/step - loss: 7.0474 - val_loss: 4.3095\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 58s 75us/step - loss: 6.3611 - val_loss: 3.6711\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 58s 75us/step - loss: 5.7986 - val_loss: 3.1153\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 58s 75us/step - loss: 5.2832 - val_loss: 2.7043\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771598/771598 [==============================] - 58s 75us/step - loss: 4.8386 - val_loss: 2.6871\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 58s 75us/step - loss: 4.4389 - val_loss: 2.3172\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 58s 75us/step - loss: 4.0945 - val_loss: 2.1780\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 58s 75us/step - loss: 3.8157 - val_loss: 2.0392\n",
      "385800/385800 [==============================] - 15s 40us/step\n",
      "771598/771598 [==============================] - 31s 40us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 77s 99us/step - loss: 24.4914 - val_loss: 4.9002\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 7.0051 - val_loss: 3.9196\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 5.9929 - val_loss: 3.3146\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 5.4207 - val_loss: 2.9048\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 57s 73us/step - loss: 4.9075 - val_loss: 2.4849\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 57s 73us/step - loss: 4.5232 - val_loss: 2.5437\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 57s 73us/step - loss: 4.2043 - val_loss: 2.0517\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 57s 73us/step - loss: 3.9344 - val_loss: 1.8644\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 57s 73us/step - loss: 3.7230 - val_loss: 1.7173\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 57s 73us/step - loss: 3.5215 - val_loss: 1.7388\n",
      "385799/385799 [==============================] - 15s 39us/step\n",
      "771599/771599 [==============================] - 30s 39us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 78s 101us/step - loss: 29.2197 - val_loss: 5.2527\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 7.9738 - val_loss: 4.1856\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 6.9755 - val_loss: 3.7981\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 6.4368 - val_loss: 3.3881\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 6.0078 - val_loss: 2.9323\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 5.6283 - val_loss: 2.6196\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 5.2132 - val_loss: 2.4483\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 4.8904 - val_loss: 2.4000\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 4.6062 - val_loss: 2.3105\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 4.3706 - val_loss: 2.2586\n",
      "385799/385799 [==============================] - 16s 41us/step\n",
      "771599/771599 [==============================] - 31s 41us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 75s 98us/step - loss: 24.0753 - val_loss: 5.8227\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 8.2044 - val_loss: 4.5627\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 6.7277 - val_loss: 3.8124\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 57s 73us/step - loss: 6.0613 - val_loss: 3.2556\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 5.6168 - val_loss: 3.1002\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 5.2321 - val_loss: 2.8133\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 4.8768 - val_loss: 2.6836\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 4.5484 - val_loss: 2.4242\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 56s 73us/step - loss: 4.2266 - val_loss: 2.2323\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 57s 73us/step - loss: 3.9640 - val_loss: 2.2184\n",
      "385800/385800 [==============================] - 15s 38us/step\n",
      "771598/771598 [==============================] - 30s 38us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 77s 100us/step - loss: 24.8837 - val_loss: 5.9522\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 8.7947 - val_loss: 4.6997\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 7.0162 - val_loss: 3.6405\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 6.3432 - val_loss: 3.2458\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 5.8167 - val_loss: 2.9284\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 5.3751 - val_loss: 2.6676\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 4.9310 - val_loss: 2.4843\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 4.5526 - val_loss: 2.2844\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 4.2354 - val_loss: 2.1718\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 3.9673 - val_loss: 2.0005\n",
      "385799/385799 [==============================] - 15s 40us/step\n",
      "771599/771599 [==============================] - 31s 40us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 78s 101us/step - loss: 25.5932 - val_loss: 5.5893\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 8.7657 - val_loss: 4.5903\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 7.1118 - val_loss: 3.5164\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 58s 76us/step - loss: 6.5054 - val_loss: 3.1425\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 5.9863 - val_loss: 2.8446\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 5.4787 - val_loss: 2.5818\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 5.0264 - val_loss: 2.4408\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 4.6517 - val_loss: 2.3027\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 4.3340 - val_loss: 2.1554\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 4.0340 - val_loss: 1.9969\n",
      "385799/385799 [==============================] - 15s 40us/step\n",
      "771599/771599 [==============================] - 31s 40us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 75s 97us/step - loss: 177.8588 - val_loss: 160.0039\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 56s 72us/step - loss: 143.9754 - val_loss: 128.7587\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 55s 72us/step - loss: 115.0928 - val_loss: 102.1490\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 55s 72us/step - loss: 90.6181 - val_loss: 79.7746\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 55s 72us/step - loss: 70.2632 - val_loss: 61.4225\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 56s 72us/step - loss: 53.8646 - val_loss: 46.9546\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 55s 72us/step - loss: 41.2575 - val_loss: 36.1911\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 55s 72us/step - loss: 32.2540 - val_loss: 28.9014\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 55s 72us/step - loss: 26.5374 - val_loss: 24.6825\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 55s 71us/step - loss: 23.5958 - val_loss: 22.8874\n",
      "385800/385800 [==============================] - 14s 37us/step\n",
      "771598/771598 [==============================] - 29s 37us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 78s 101us/step - loss: 177.7983 - val_loss: 160.0067\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 58s 76us/step - loss: 143.9151 - val_loss: 128.7623\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 58s 76us/step - loss: 115.0281 - val_loss: 102.1452\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 58s 76us/step - loss: 90.5467 - val_loss: 79.7656\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 58s 76us/step - loss: 70.1966 - val_loss: 61.4235\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 59s 76us/step - loss: 53.7942 - val_loss: 46.9491\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 58s 76us/step - loss: 41.1965 - val_loss: 36.1946\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 58s 76us/step - loss: 32.1904 - val_loss: 28.8959\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 58s 76us/step - loss: 26.4693 - val_loss: 24.6805\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 58s 76us/step - loss: 23.5318 - val_loss: 22.8889\n",
      "385799/385799 [==============================] - 16s 40us/step\n",
      "771599/771599 [==============================] - 31s 40us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 89s 115us/step - loss: 178.1088 - val_loss: 159.9885\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 55s 72us/step - loss: 144.2040 - val_loss: 128.7514\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 115.2915 - val_loss: 102.1357\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 90.7850 - val_loss: 79.7565\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 70.4061 - val_loss: 61.4110\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 53.9828 - val_loss: 46.9443\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 41.3522 - val_loss: 36.1765\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 32.3168 - val_loss: 28.8794\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 26.5798 - val_loss: 24.6696\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 23.6335 - val_loss: 22.8875\n",
      "385799/385799 [==============================] - 16s 41us/step\n",
      "771599/771599 [==============================] - 26s 34us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 94s 122us/step - loss: 33.3302 - val_loss: 5.7784\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 11.4249 - val_loss: 4.6975\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 9.0947 - val_loss: 4.1927\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 8.4041 - val_loss: 4.0112\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 7.7016 - val_loss: 3.6550\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 7.0536 - val_loss: 3.5315\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 6.4377 - val_loss: 3.2257\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 67s 86us/step - loss: 5.8902 - val_loss: 2.9876\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 5.4280 - val_loss: 2.7629\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 5.0556 - val_loss: 2.5966\n",
      "385800/385800 [==============================] - 17s 43us/step\n",
      "771598/771598 [==============================] - 33s 43us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 82s 106us/step - loss: 38.2265 - val_loss: 4.9605\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 61s 79us/step - loss: 10.1049 - val_loss: 4.7075\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 60s 78us/step - loss: 9.0856 - val_loss: 4.6639\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 60s 78us/step - loss: 8.7228 - val_loss: 4.2493\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 60s 78us/step - loss: 8.3127 - val_loss: 4.0005\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 60s 78us/step - loss: 7.8385 - val_loss: 3.4483\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 61s 78us/step - loss: 7.3673 - val_loss: 3.5087\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 6.9695 - val_loss: 3.4107\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 64s 82us/step - loss: 6.6395 - val_loss: 2.9771\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 63s 82us/step - loss: 6.3815 - val_loss: 2.9198\n",
      "385799/385799 [==============================] - ETA:  - 15s 38us/step\n",
      "771599/771599 [==============================] - 31s 40us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 103s 133us/step - loss: 29.0237 - val_loss: 5.9694\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 76s 98us/step - loss: 9.0318 - val_loss: 4.8522\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 75s 98us/step - loss: 7.4677 - val_loss: 3.8361\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 76s 98us/step - loss: 6.7223 - val_loss: 3.2560\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 76s 99us/step - loss: 6.0692 - val_loss: 3.0852\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 76s 98us/step - loss: 5.5555 - val_loss: 2.5977\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 76s 99us/step - loss: 5.1432 - val_loss: 2.5040\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 75s 98us/step - loss: 4.7566 - val_loss: 2.2037\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 76s 98us/step - loss: 4.4465 - val_loss: 2.0977\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 76s 98us/step - loss: 4.2064 - val_loss: 2.2649\n",
      "385799/385799 [==============================] - 20s 51us/step\n",
      "771599/771599 [==============================] - 39s 51us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 103s 133us/step - loss: 32.0664 - val_loss: 5.7350\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 76s 99us/step - loss: 9.6941 - val_loss: 4.6061\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 76s 99us/step - loss: 8.1240 - val_loss: 4.0497\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 77s 100us/step - loss: 7.5805 - val_loss: 3.7793\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 78s 101us/step - loss: 7.0134 - val_loss: 3.3809\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 76s 99us/step - loss: 6.4530 - val_loss: 3.2661\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 73s 95us/step - loss: 5.9784 - val_loss: 3.0943\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 72s 94us/step - loss: 5.5893 - val_loss: 3.0560\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 73s 94us/step - loss: 5.2578 - val_loss: 2.5577\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 73s 94us/step - loss: 4.9639 - val_loss: 2.3882\n",
      "385800/385800 [==============================] - 19s 50us/step\n",
      "771598/771598 [==============================] - 38s 49us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 92s 119us/step - loss: 37.4602 - val_loss: 5.6723\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 10.6355 - val_loss: 4.3297\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 8.7470 - val_loss: 4.4971\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 8.2674 - val_loss: 3.9487\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 7.7226 - val_loss: 3.4813\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 7.1667 - val_loss: 3.2849\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 6.6364 - val_loss: 3.0261\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 6.1813 - val_loss: 3.0232\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 5.7875 - val_loss: 2.9543\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 5.4185 - val_loss: 2.5654\n",
      "385799/385799 [==============================] - 16s 42us/step\n",
      "771599/771599 [==============================] - 32s 42us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 87s 113us/step - loss: 28.8011 - val_loss: 6.6856\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 9.2822 - val_loss: 5.0395\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 7.7487 - val_loss: 4.2607\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 7.0980 - val_loss: 3.9070\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 6.6079 - val_loss: 3.4547\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 6.1917 - val_loss: 3.3678\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 5.7766 - val_loss: 3.0227\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 5.4194 - val_loss: 3.0657\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 5.1364 - val_loss: 2.8525\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 4.8952 - val_loss: 2.9726\n",
      "385799/385799 [==============================] - 16s 42us/step\n",
      "771599/771599 [==============================] - 33s 42us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 90s 116us/step - loss: 177.8639 - val_loss: 160.0157\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 143.9885 - val_loss: 128.7702\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 69s 89us/step - loss: 115.1026 - val_loss: 102.1574\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 90.6246 - val_loss: 79.7757\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 70.2638 - val_loss: 61.4246\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 73s 94us/step - loss: 53.8615 - val_loss: 46.9491\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 41.2607 - val_loss: 36.1968\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 32.2577 - val_loss: 28.9007\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 26.5419 - val_loss: 24.6855\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 23.6009 - val_loss: 22.8934\n",
      "385800/385800 [==============================] - 17s 45us/step\n",
      "771598/771598 [==============================] - 34s 45us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 90s 116us/step - loss: 177.7954 - val_loss: 160.0062\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 143.9169 - val_loss: 128.7604\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 115.0310 - val_loss: 102.1516\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 90.5570 - val_loss: 79.7753\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 70.2020 - val_loss: 61.4244\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 53.7981 - val_loss: 46.9533\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 41.1992 - val_loss: 36.1952\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 32.1907 - val_loss: 28.9017\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 26.4720 - val_loss: 24.6812\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 23.5368 - val_loss: 22.8943\n",
      "385799/385799 [==============================] - 17s 44us/step\n",
      "771599/771599 [==============================] - 34s 45us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 91s 117us/step - loss: 178.1029 - val_loss: 159.9813\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 144.1979 - val_loss: 128.7458\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 115.2913 - val_loss: 102.1393\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 90.7828 - val_loss: 79.7522\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 70.4013 - val_loss: 61.4055\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 53.9764 - val_loss: 46.9356\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 41.3520 - val_loss: 36.1770\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 32.3208 - val_loss: 28.8846\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 26.5833 - val_loss: 24.6706\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 23.6316 - val_loss: 22.8856\n",
      "385799/385799 [==============================] - 17s 43us/step\n",
      "771599/771599 [==============================] - 33s 43us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 74s 96us/step - loss: 11.3852 - val_loss: 2.6575\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 52s 68us/step - loss: 3.0653 - val_loss: 1.4297\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 52s 68us/step - loss: 2.0374 - val_loss: 1.0373\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 52s 68us/step - loss: 1.6371 - val_loss: 0.8591\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 52s 68us/step - loss: 1.4128 - val_loss: 0.6412\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 52s 68us/step - loss: 1.2371 - val_loss: 0.5262\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 53s 68us/step - loss: 1.1073 - val_loss: 0.4922\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 52s 68us/step - loss: 0.9892 - val_loss: 0.4231\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 53s 68us/step - loss: 0.9050 - val_loss: 0.3897\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 52s 68us/step - loss: 0.8345 - val_loss: 0.3593\n",
      "385800/385800 [==============================] - 16s 41us/step\n",
      "771598/771598 [==============================] - 32s 41us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 75s 97us/step - loss: 12.1239 - val_loss: 2.5985\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 3.1270 - val_loss: 1.3422\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 1.9401 - val_loss: 1.0281\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 1.6244 - val_loss: 0.8848\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 1.4428 - val_loss: 0.6652\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 1.2045 - val_loss: 0.5660\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 1.0780 - val_loss: 0.5186\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 0.9600 - val_loss: 0.4350\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 0.8748 - val_loss: 0.4146\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 0.7995 - val_loss: 0.4137\n",
      "385799/385799 [==============================] - 16s 42us/step\n",
      "771599/771599 [==============================] - 32s 42us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 77s 100us/step - loss: 12.4018 - val_loss: 2.7615\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 3.3266 - val_loss: 1.4704\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 2.0750 - val_loss: 1.1388\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 1.6949 - val_loss: 0.9118\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 1.5197 - val_loss: 0.8425\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 1.3700 - val_loss: 0.6774\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 55s 71us/step - loss: 1.1375 - val_loss: 0.5777\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 1.0129 - val_loss: 0.6087\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 0.9090 - val_loss: 0.4503\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 0.8256 - val_loss: 0.3714\n",
      "385799/385799 [==============================] - 17s 43us/step\n",
      "771599/771599 [==============================] - 33s 43us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 76s 99us/step - loss: 10.9787 - val_loss: 2.6840\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 53s 69us/step - loss: 2.9044 - val_loss: 1.4877\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 53s 69us/step - loss: 1.9422 - val_loss: 1.2373\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 53s 69us/step - loss: 1.6493 - val_loss: 0.8817\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 53s 69us/step - loss: 1.4354 - val_loss: 0.6348\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 53s 69us/step - loss: 1.1849 - val_loss: 0.5264\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 53s 69us/step - loss: 1.0427 - val_loss: 0.4706\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 53s 69us/step - loss: 0.9328 - val_loss: 0.4234\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 53s 69us/step - loss: 0.8524 - val_loss: 0.4433\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 53s 69us/step - loss: 0.7783 - val_loss: 0.3862\n",
      "385800/385800 [==============================] - 16s 42us/step\n",
      "771598/771598 [==============================] - 32s 42us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 75s 97us/step - loss: 10.5464 - val_loss: 2.6071\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 2.9642 - val_loss: 1.3986\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 2.0653 - val_loss: 1.0837\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 1.6501 - val_loss: 0.9017\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 1.3638 - val_loss: 0.5940\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 1.1462 - val_loss: 0.5351\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 1.0227 - val_loss: 0.5456\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 0.9149 - val_loss: 0.3976\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 0.8241 - val_loss: 0.3988\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 0.7444 - val_loss: 0.3609\n",
      "385799/385799 [==============================] - 16s 41us/step\n",
      "771599/771599 [==============================] - 31s 41us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 76s 98us/step - loss: 11.7013 - val_loss: 2.7189\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 3.1789 - val_loss: 1.5488\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 2.0863 - val_loss: 1.0936\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 1.7067 - val_loss: 0.9345\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 1.5178 - val_loss: 0.8486\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 1.3516 - val_loss: 0.7645\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 1.1272 - val_loss: 0.5186\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 0.9879 - val_loss: 0.4470\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 53s 69us/step - loss: 0.8915 - val_loss: 0.4304\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 54s 70us/step - loss: 0.8169 - val_loss: 0.3730\n",
      "385799/385799 [==============================] - 16s 42us/step\n",
      "771599/771599 [==============================] - 32s 42us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 75s 98us/step - loss: 177.8603 - val_loss: 160.0065\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 53s 68us/step - loss: 143.9797 - val_loss: 128.7620\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 53s 68us/step - loss: 115.0890 - val_loss: 102.1427\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 53s 68us/step - loss: 90.6153 - val_loss: 79.7714\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 53s 68us/step - loss: 70.2614 - val_loss: 61.4240\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 53s 69us/step - loss: 53.8602 - val_loss: 46.9510\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 53s 68us/step - loss: 41.2599 - val_loss: 36.1943\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 53s 68us/step - loss: 32.2569 - val_loss: 28.9006\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 53s 69us/step - loss: 26.5370 - val_loss: 24.6797\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 53s 68us/step - loss: 23.5963 - val_loss: 22.8884\n",
      "385800/385800 [==============================] - 16s 41us/step\n",
      "771598/771598 [==============================] - 32s 41us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 75s 97us/step - loss: 177.7989 - val_loss: 160.0074\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 143.9178 - val_loss: 128.7666\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 115.0360 - val_loss: 102.1554\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 90.5532 - val_loss: 79.7697\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 70.1983 - val_loss: 61.4215\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 53.7981 - val_loss: 46.9537\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 52s 67us/step - loss: 41.1943 - val_loss: 36.1888\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 32.1867 - val_loss: 28.8982\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 26.4681 - val_loss: 24.6756\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 52s 67us/step - loss: 23.5331 - val_loss: 22.8909\n",
      "385799/385799 [==============================] - 16s 41us/step\n",
      "771599/771599 [==============================] - 32s 41us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 75s 97us/step - loss: 178.1038 - val_loss: 159.9861\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 144.1953 - val_loss: 128.7349\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 53s 68us/step - loss: 115.2840 - val_loss: 102.1327\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 90.7812 - val_loss: 79.7514\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 70.4062 - val_loss: 61.4117\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 53.9836 - val_loss: 46.9441\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 41.3526 - val_loss: 36.1809\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 32.3221 - val_loss: 28.8854\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 26.5846 - val_loss: 24.6698\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 52s 68us/step - loss: 23.6306 - val_loss: 22.8845\n",
      "385799/385799 [==============================] - 16s 42us/step\n",
      "771599/771599 [==============================] - 35s 45us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 100s 130us/step - loss: 14.7260 - val_loss: 4.1906\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 71s 92us/step - loss: 4.6797 - val_loss: 2.5408\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 71s 92us/step - loss: 2.9394 - val_loss: 1.6507\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 71s 92us/step - loss: 2.5061 - val_loss: 1.3694\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 72s 93us/step - loss: 2.1628 - val_loss: 1.0292\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 71s 92us/step - loss: 1.9016 - val_loss: 0.8811\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 71s 92us/step - loss: 1.7299 - val_loss: 0.7953\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 71s 92us/step - loss: 1.5875 - val_loss: 0.7348\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 70s 91us/step - loss: 1.4753 - val_loss: 0.6954\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 70s 91us/step - loss: 1.3796 - val_loss: 0.6299\n",
      "385800/385800 [==============================] - 20s 52us/step\n",
      "771598/771598 [==============================] - 40s 51us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 97s 126us/step - loss: 16.5710 - val_loss: 4.0226\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 74s 95us/step - loss: 4.6629 - val_loss: 2.0750\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 74s 96us/step - loss: 2.8987 - val_loss: 1.5915\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 73s 95us/step - loss: 2.3834 - val_loss: 1.1269\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 73s 95us/step - loss: 2.1037 - val_loss: 0.9372\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 74s 96us/step - loss: 1.9047 - val_loss: 0.8778\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 73s 95us/step - loss: 1.7347 - val_loss: 0.7240\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 73s 95us/step - loss: 1.5934 - val_loss: 0.6777\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 74s 95us/step - loss: 1.4850 - val_loss: 0.6052\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 74s 96us/step - loss: 1.3880 - val_loss: 0.5642\n",
      "385799/385799 [==============================] - 21s 54us/step\n",
      "771599/771599 [==============================] - 41s 54us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 90s 116us/step - loss: 15.0532 - val_loss: 4.2179\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 4.5699 - val_loss: 2.1016\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 2.8205 - val_loss: 1.5832\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 67s 86us/step - loss: 2.2973 - val_loss: 1.0627\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 1.9813 - val_loss: 0.8868\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 1.7818 - val_loss: 0.7824\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 1.6265 - val_loss: 0.7397\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 1.5069 - val_loss: 0.6816\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 1.4078 - val_loss: 0.6243\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 1.3238 - val_loss: 0.5636\n",
      "385799/385799 [==============================] - 17s 44us/step\n",
      "771599/771599 [==============================] - 34s 44us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 91s 118us/step - loss: 14.2806 - val_loss: 4.4091\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 67s 86us/step - loss: 4.8683 - val_loss: 2.2102\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 2.9306 - val_loss: 1.5994\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 2.4926 - val_loss: 1.4139\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 2.2137 - val_loss: 1.0562\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 1.9340 - val_loss: 0.9263\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 62s 81us/step - loss: 1.7457 - val_loss: 0.8468\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 60s 78us/step - loss: 1.6074 - val_loss: 0.7635\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 60s 78us/step - loss: 1.4811 - val_loss: 0.7136\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 60s 78us/step - loss: 1.3850 - val_loss: 0.6499\n",
      "385800/385800 [==============================] - 15s 40us/step\n",
      "771598/771598 [==============================] - 34s 44us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 91s 119us/step - loss: 15.4507 - val_loss: 4.4349\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 4.9898 - val_loss: 2.4087\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 3.0588 - val_loss: 1.6377\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 2.4671 - val_loss: 1.2295\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 2.1500 - val_loss: 0.9992\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 1.9307 - val_loss: 1.0250\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 1.7638 - val_loss: 0.8636\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 1.6234 - val_loss: 0.7823\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 1.5003 - val_loss: 0.6467\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 1.4007 - val_loss: 0.5973\n",
      "385799/385799 [==============================] - 18s 47us/step\n",
      "771599/771599 [==============================] - 36s 47us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 91s 117us/step - loss: 15.8230 - val_loss: 4.1731\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 4.7419 - val_loss: 2.1872\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 2.9770 - val_loss: 1.5247\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 2.5299 - val_loss: 1.2938\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 2.2475 - val_loss: 1.0548\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 2.0217 - val_loss: 0.9070\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 1.8408 - val_loss: 0.8000\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 1.7044 - val_loss: 0.7139\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 1.5942 - val_loss: 0.7050\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 1.4982 - val_loss: 0.6398\n",
      "385799/385799 [==============================] - 18s 46us/step\n",
      "771599/771599 [==============================] - 36s 47us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 91s 118us/step - loss: 177.8633 - val_loss: 160.0121\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 143.9844 - val_loss: 128.7676\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 115.1002 - val_loss: 102.1550\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 90.6193 - val_loss: 79.7694\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 70.2602 - val_loss: 61.4196\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 53.8581 - val_loss: 46.9528\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 41.2588 - val_loss: 36.1934\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 32.2541 - val_loss: 28.8999\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 26.5395 - val_loss: 24.6819\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 23.5980 - val_loss: 22.8905\n",
      "385800/385800 [==============================] - 18s 47us/step\n",
      "771598/771598 [==============================] - 36s 47us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 90s 117us/step - loss: 177.8020 - val_loss: 160.0143\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 67s 86us/step - loss: 143.9209 - val_loss: 128.7620\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 115.0298 - val_loss: 102.1501\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 90.5548 - val_loss: 79.7713\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 70.2017 - val_loss: 61.4273\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 53.8008 - val_loss: 46.9529\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 41.1945 - val_loss: 36.1914\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 32.1910 - val_loss: 28.9000\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 26.4735 - val_loss: 24.6845\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 23.5367 - val_loss: 22.8938\n",
      "385799/385799 [==============================] - 18s 46us/step\n",
      "771599/771599 [==============================] - 35s 46us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 90s 117us/step - loss: 178.1235 - val_loss: 160.0042\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 144.2127 - val_loss: 128.7586\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 115.3008 - val_loss: 102.1466\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 90.7944 - val_loss: 79.7639\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 70.4123 - val_loss: 61.4141\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 53.9884 - val_loss: 46.9472\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 41.3557 - val_loss: 36.1802\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 67s 86us/step - loss: 32.3258 - val_loss: 28.8898\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 26.5855 - val_loss: 24.6703\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 23.6316 - val_loss: 22.8860\n",
      "385799/385799 [==============================] - 18s 46us/step\n",
      "771599/771599 [==============================] - 35s 45us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 103s 133us/step - loss: 18.5399 - val_loss: 5.9250\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 79s 103us/step - loss: 5.3973 - val_loss: 2.7337\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 79s 102us/step - loss: 3.7502 - val_loss: 2.1093\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 79s 102us/step - loss: 3.2712 - val_loss: 1.8151\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 81s 106us/step - loss: 2.9551 - val_loss: 1.5068\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 79s 103us/step - loss: 2.6544 - val_loss: 1.3242\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 79s 102us/step - loss: 2.4235 - val_loss: 1.1551\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 79s 102us/step - loss: 2.2524 - val_loss: 1.1245\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 79s 102us/step - loss: 2.1159 - val_loss: 1.0015\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 79s 102us/step - loss: 1.9973 - val_loss: 1.0126\n",
      "385800/385800 [==============================] - 20s 51us/step\n",
      "771598/771598 [==============================] - 39s 50us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 108s 140us/step - loss: 20.6065 - val_loss: 5.3766\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 83s 107us/step - loss: 5.5592 - val_loss: 2.8372\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 3.8286 - val_loss: 2.2370\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 83s 107us/step - loss: 3.4086 - val_loss: 1.9455\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 3.0897 - val_loss: 1.6232\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 86s 111us/step - loss: 2.7810 - val_loss: 1.3731\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 2.5331 - val_loss: 1.2818\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 83s 107us/step - loss: 2.3298 - val_loss: 1.1462\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 83s 107us/step - loss: 2.1601 - val_loss: 1.0566\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 2.0227 - val_loss: 1.0204\n",
      "385799/385799 [==============================] - 20s 53us/step\n",
      "771599/771599 [==============================] - 41s 53us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771599/771599 [==============================] - 108s 140us/step - loss: 20.8775 - val_loss: 5.3337\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 5.4112 - val_loss: 2.8477\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 3.8617 - val_loss: 2.4257\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 3.4712 - val_loss: 2.1012\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 3.1787 - val_loss: 1.7759\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 84s 108us/step - loss: 2.8079 - val_loss: 1.3916\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 2.4995 - val_loss: 1.2453\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 84s 108us/step - loss: 2.2900 - val_loss: 1.1626\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 2.1186 - val_loss: 1.0776\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 1.9977 - val_loss: 0.9747\n",
      "385799/385799 [==============================] - 20s 53us/step\n",
      "771599/771599 [==============================] - 41s 53us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 111s 143us/step - loss: 19.0893 - val_loss: 5.3051\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 85s 111us/step - loss: 5.5439 - val_loss: 2.6740\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 85s 110us/step - loss: 3.7752 - val_loss: 1.9347\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 85s 111us/step - loss: 3.1864 - val_loss: 1.5587\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 86s 111us/step - loss: 2.8618 - val_loss: 1.3515\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 85s 111us/step - loss: 2.6271 - val_loss: 1.2517\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 85s 111us/step - loss: 2.4311 - val_loss: 1.1010\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 85s 111us/step - loss: 2.2767 - val_loss: 1.1446\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 85s 111us/step - loss: 2.1371 - val_loss: 1.2324\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 85s 111us/step - loss: 2.0275 - val_loss: 0.9255\n",
      "385800/385800 [==============================] - 21s 54us/step\n",
      "771598/771598 [==============================] - 42s 54us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 105s 136us/step - loss: 19.3637 - val_loss: 5.4986\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 79s 103us/step - loss: 5.8380 - val_loss: 2.9632\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 79s 103us/step - loss: 3.9534 - val_loss: 2.3803\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 79s 103us/step - loss: 3.5113 - val_loss: 2.0203\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 80s 103us/step - loss: 3.1885 - val_loss: 1.7430\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 79s 103us/step - loss: 2.8791 - val_loss: 1.5333\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 79s 103us/step - loss: 2.6224 - val_loss: 1.4355\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 79s 103us/step - loss: 2.4240 - val_loss: 1.2139\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 79s 102us/step - loss: 2.2524 - val_loss: 1.0981\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 80s 103us/step - loss: 2.1219 - val_loss: 1.0834\n",
      "385799/385799 [==============================] - 19s 50us/step\n",
      "771599/771599 [==============================] - 38s 49us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 110s 143us/step - loss: 19.9186 - val_loss: 5.2748\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 5.7187 - val_loss: 3.2913\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 3.9470 - val_loss: 2.2876\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 3.4228 - val_loss: 1.8047\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 85s 111us/step - loss: 3.0699 - val_loss: 1.5734\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 2.7739 - val_loss: 1.3726\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 85s 111us/step - loss: 2.5417 - val_loss: 1.2245\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 2.3468 - val_loss: 1.1465\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 2.1936 - val_loss: 1.0469\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 2.0726 - val_loss: 0.9638\n",
      "385799/385799 [==============================] - 21s 53us/step\n",
      "771599/771599 [==============================] - 41s 53us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 110s 142us/step - loss: 177.8565 - val_loss: 160.0061\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 85s 110us/step - loss: 143.9767 - val_loss: 128.7593\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 85s 110us/step - loss: 115.0934 - val_loss: 102.1489\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 85s 110us/step - loss: 90.6184 - val_loss: 79.7713\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 85s 110us/step - loss: 70.2608 - val_loss: 61.4221\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 85s 110us/step - loss: 53.8647 - val_loss: 46.9542\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 85s 111us/step - loss: 41.2620 - val_loss: 36.1932\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 84s 109us/step - loss: 32.2535 - val_loss: 28.8971\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 85s 110us/step - loss: 26.5357 - val_loss: 24.6798\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 85s 110us/step - loss: 23.5995 - val_loss: 22.8911\n",
      "385800/385800 [==============================] - 20s 53us/step\n",
      "771598/771598 [==============================] - 41s 53us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 109s 141us/step - loss: 177.8055 - val_loss: 160.0172\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 143.9257 - val_loss: 128.7696\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 115.0375 - val_loss: 102.1567\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 84s 108us/step - loss: 90.5575 - val_loss: 79.7750\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 84s 109us/step - loss: 70.2058 - val_loss: 61.4319\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 53.8057 - val_loss: 46.9597\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 84s 109us/step - loss: 41.1992 - val_loss: 36.1944\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 32.1918 - val_loss: 28.8984\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 26.4689 - val_loss: 24.6782\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 23.5335 - val_loss: 22.8918\n",
      "385799/385799 [==============================] - 20s 52us/step\n",
      "771599/771599 [==============================] - 40s 52us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 110s 143us/step - loss: 178.1298 - val_loss: 160.0120\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 144.2175 - val_loss: 128.7576\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 115.3025 - val_loss: 102.1496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 90.8010 - val_loss: 79.7711\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 70.4183 - val_loss: 61.4186\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 53.9849 - val_loss: 46.9442\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 41.3507 - val_loss: 36.1760\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 32.3193 - val_loss: 28.8841\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 26.5840 - val_loss: 24.6701\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 23.6317 - val_loss: 22.8846\n",
      "385799/385799 [==============================] - 20s 53us/step\n",
      "771599/771599 [==============================] - 41s 53us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 91s 119us/step - loss: 15.3011 - val_loss: 4.1864\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 66s 86us/step - loss: 5.5071 - val_loss: 2.6463\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 66s 86us/step - loss: 3.9370 - val_loss: 2.0204\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 66s 86us/step - loss: 3.2563 - val_loss: 1.6734\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 67s 86us/step - loss: 2.9130 - val_loss: 1.5143\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 66s 86us/step - loss: 2.6332 - val_loss: 1.3900\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 2.3707 - val_loss: 1.2782\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 66s 86us/step - loss: 2.0813 - val_loss: 1.0284\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 66s 86us/step - loss: 1.8197 - val_loss: 0.9406\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 66s 86us/step - loss: 1.6692 - val_loss: 0.8983\n",
      "385800/385800 [==============================] - 19s 49us/step\n",
      "771598/771598 [==============================] - 38s 49us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 91s 117us/step - loss: 15.0914 - val_loss: 4.7349\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 5.5408 - val_loss: 3.0193\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 3.8642 - val_loss: 1.9516\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 3.1057 - val_loss: 1.6594\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 65s 84us/step - loss: 2.7909 - val_loss: 1.4561\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 65s 85us/step - loss: 2.4450 - val_loss: 1.1886\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 66s 85us/step - loss: 2.1069 - val_loss: 1.0442\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 65s 85us/step - loss: 1.8797 - val_loss: 0.9398\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 65s 85us/step - loss: 1.7060 - val_loss: 0.9212\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 65s 85us/step - loss: 1.5742 - val_loss: 0.8561\n",
      "385799/385799 [==============================] - 18s 46us/step\n",
      "771599/771599 [==============================] - 36s 46us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 83s 107us/step - loss: 17.5879 - val_loss: 4.6191\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 6.4514 - val_loss: 2.8162\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 4.8029 - val_loss: 2.1625\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 3.8731 - val_loss: 1.7880\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 3.3918 - val_loss: 1.5200\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 3.0402 - val_loss: 1.4057\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 2.7233 - val_loss: 1.3933\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 2.4341 - val_loss: 1.2811\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 2.1941 - val_loss: 1.2741\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 57s 74us/step - loss: 2.0681 - val_loss: 1.2192\n",
      "385799/385799 [==============================] - 15s 38us/step\n",
      "771599/771599 [==============================] - 30s 38us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 90s 117us/step - loss: 14.5223 - val_loss: 4.3072\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 64s 84us/step - loss: 5.0795 - val_loss: 2.9471\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 65s 85us/step - loss: 3.4946 - val_loss: 1.8189\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 64s 84us/step - loss: 2.9221 - val_loss: 1.5933\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 64s 84us/step - loss: 2.6482 - val_loss: 1.4516\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 64s 84us/step - loss: 2.4186 - val_loss: 1.2921\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 64s 83us/step - loss: 2.1550 - val_loss: 1.1317\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 65s 84us/step - loss: 1.8937 - val_loss: 1.0001\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 65s 84us/step - loss: 1.7014 - val_loss: 0.8851\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 64s 83us/step - loss: 1.5682 - val_loss: 0.9108\n",
      "385800/385800 [==============================] - 17s 45us/step\n",
      "771598/771598 [==============================] - 35s 45us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 84s 109us/step - loss: 14.8196 - val_loss: 4.6141\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 5.3925 - val_loss: 2.7628\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 58s 76us/step - loss: 3.7413 - val_loss: 1.9207\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 3.0327 - val_loss: 1.7930\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 2.7595 - val_loss: 1.4414\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 2.5096 - val_loss: 1.3266\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 2.2264 - val_loss: 1.1000\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 1.9450 - val_loss: 0.9683\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 1.7423 - val_loss: 0.9259\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 58s 75us/step - loss: 1.6206 - val_loss: 0.8445\n",
      "385799/385799 [==============================] - 15s 39us/step\n",
      "771599/771599 [==============================] - 30s 38us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 92s 119us/step - loss: 14.8499 - val_loss: 4.4109\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 5.3925 - val_loss: 2.8032\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 3.8018 - val_loss: 1.8960\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 3.1338 - val_loss: 1.6228\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 2.8214 - val_loss: 1.4486\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 67s 87us/step - loss: 2.4821 - val_loss: 1.1561\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 2.1538 - val_loss: 1.0724\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 1.9257 - val_loss: 0.9551\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 67s 86us/step - loss: 1.7330 - val_loss: 0.9022\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 66s 86us/step - loss: 1.6092 - val_loss: 0.8732\n",
      "385799/385799 [==============================] - 18s 48us/step\n",
      "771599/771599 [==============================] - 37s 47us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 94s 122us/step - loss: 177.8594 - val_loss: 160.0071\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 143.9831 - val_loss: 128.7670\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 67s 87us/step - loss: 115.0952 - val_loss: 102.1492\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 90.6141 - val_loss: 79.7676\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 70.2606 - val_loss: 61.4234\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 53.8600 - val_loss: 46.9522\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 68s 87us/step - loss: 41.2588 - val_loss: 36.1925\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 32.2556 - val_loss: 28.8978\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 68s 87us/step - loss: 26.5371 - val_loss: 24.6834\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 68s 88us/step - loss: 23.6011 - val_loss: 22.8917\n",
      "385800/385800 [==============================] - 19s 49us/step\n",
      "771598/771598 [==============================] - 37s 48us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 94s 122us/step - loss: 177.7871 - val_loss: 159.9990\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 143.9092 - val_loss: 128.7517\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 115.0209 - val_loss: 102.1422\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 90.5461 - val_loss: 79.7630\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 70.1887 - val_loss: 61.4143\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 53.7913 - val_loss: 46.9499\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 41.1936 - val_loss: 36.1908\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 32.1881 - val_loss: 28.8986\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 68s 88us/step - loss: 26.4682 - val_loss: 24.6790\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 68s 89us/step - loss: 23.5330 - val_loss: 22.8902\n",
      "385799/385799 [==============================] - 19s 50us/step\n",
      "771599/771599 [==============================] - 38s 49us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 113s 146us/step - loss: 178.1242 - val_loss: 160.0065\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 88s 114us/step - loss: 144.2150 - val_loss: 128.7608\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 85s 110us/step - loss: 115.3001 - val_loss: 102.1445\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 84s 109us/step - loss: 90.7917 - val_loss: 79.7624\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 84s 109us/step - loss: 70.4117 - val_loss: 61.4132\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 84s 108us/step - loss: 53.9826 - val_loss: 46.9429\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 41.3558 - val_loss: 36.1823\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 32.3279 - val_loss: 28.8910\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 83s 108us/step - loss: 26.5862 - val_loss: 24.6718\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 84s 108us/step - loss: 23.6319 - val_loss: 22.8842\n",
      "385799/385799 [==============================] - 23s 60us/step\n",
      "771599/771599 [==============================] - 46s 60us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 124s 161us/step - loss: 17.7843 - val_loss: 6.2389\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 97s 126us/step - loss: 6.8005 - val_loss: 3.7256\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 97s 126us/step - loss: 4.5015 - val_loss: 2.4961\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 97s 126us/step - loss: 3.8282 - val_loss: 2.2539\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 97s 126us/step - loss: 3.4824 - val_loss: 1.8847\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 97s 126us/step - loss: 3.1341 - val_loss: 1.6771\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 97s 126us/step - loss: 2.8367 - val_loss: 1.5981\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 98s 127us/step - loss: 2.5881 - val_loss: 1.4593\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 97s 126us/step - loss: 2.3706 - val_loss: 1.3363\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 97s 126us/step - loss: 2.1842 - val_loss: 1.2386\n",
      "385800/385800 [==============================] - 23s 60us/step\n",
      "771598/771598 [==============================] - 46s 60us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 147s 191us/step - loss: 20.7556 - val_loss: 5.0854\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 86s 112us/step - loss: 7.2722 - val_loss: 3.7421\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 86s 111us/step - loss: 5.2785 - val_loss: 2.7636\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 86s 111us/step - loss: 4.4140 - val_loss: 2.2781\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 86s 111us/step - loss: 4.0470 - val_loss: 2.1091\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 86s 111us/step - loss: 3.6767 - val_loss: 1.8621\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 86s 112us/step - loss: 3.2337 - val_loss: 1.8258\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 86s 111us/step - loss: 2.8861 - val_loss: 1.5097\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 86s 111us/step - loss: 2.6203 - val_loss: 1.4197\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 86s 111us/step - loss: 2.4007 - val_loss: 1.3195\n",
      "385799/385799 [==============================] - 25s 65us/step\n",
      "771599/771599 [==============================] - 43s 55us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 113s 146us/step - loss: 20.5888 - val_loss: 5.1726\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 75s 97us/step - loss: 6.8619 - val_loss: 3.8200\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 75s 97us/step - loss: 4.9976 - val_loss: 2.7305\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 75s 97us/step - loss: 4.2907 - val_loss: 2.3232\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 75s 97us/step - loss: 3.8977 - val_loss: 2.1138\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 79s 103us/step - loss: 3.5042 - val_loss: 1.8275\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 96s 124us/step - loss: 3.1254 - val_loss: 1.6590\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 89s 115us/step - loss: 2.8191 - val_loss: 1.5127\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 87s 113us/step - loss: 2.5555 - val_loss: 1.4194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 88s 114us/step - loss: 2.3554 - val_loss: 1.3319\n",
      "385799/385799 [==============================] - 20s 53us/step\n",
      "771599/771599 [==============================] - 38s 49us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771598 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771598/771598 [==============================] - 129s 168us/step - loss: 18.8758 - val_loss: 6.0570\n",
      "Epoch 2/10\n",
      "771598/771598 [==============================] - 94s 122us/step - loss: 7.1631 - val_loss: 3.7910\n",
      "Epoch 3/10\n",
      "771598/771598 [==============================] - 93s 120us/step - loss: 4.9254 - val_loss: 2.7485\n",
      "Epoch 4/10\n",
      "771598/771598 [==============================] - 102s 132us/step - loss: 4.2125 - val_loss: 2.2971\n",
      "Epoch 5/10\n",
      "771598/771598 [==============================] - 103s 134us/step - loss: 3.8261 - val_loss: 2.0626\n",
      "Epoch 6/10\n",
      "771598/771598 [==============================] - 103s 133us/step - loss: 3.4437 - val_loss: 1.7815\n",
      "Epoch 7/10\n",
      "771598/771598 [==============================] - 102s 132us/step - loss: 3.0673 - val_loss: 1.5945\n",
      "Epoch 8/10\n",
      "771598/771598 [==============================] - 102s 132us/step - loss: 2.7714 - val_loss: 1.4790\n",
      "Epoch 9/10\n",
      "771598/771598 [==============================] - 101s 131us/step - loss: 2.5208 - val_loss: 1.3436\n",
      "Epoch 10/10\n",
      "771598/771598 [==============================] - 101s 131us/step - loss: 2.3263 - val_loss: 1.3588\n",
      "385800/385800 [==============================] - 25s 65us/step\n",
      "771598/771598 [==============================] - 50s 65us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n",
      "771599/771599 [==============================] - 128s 166us/step - loss: 18.5791 - val_loss: 6.0149\n",
      "Epoch 2/10\n",
      "771599/771599 [==============================] - 100s 129us/step - loss: 6.7589 - val_loss: 3.4243\n",
      "Epoch 3/10\n",
      "771599/771599 [==============================] - 100s 130us/step - loss: 4.5787 - val_loss: 2.5614\n",
      "Epoch 4/10\n",
      "771599/771599 [==============================] - 100s 130us/step - loss: 4.0325 - val_loss: 2.2295\n",
      "Epoch 5/10\n",
      "771599/771599 [==============================] - 99s 129us/step - loss: 3.6797 - val_loss: 2.0494\n",
      "Epoch 6/10\n",
      "771599/771599 [==============================] - 106s 137us/step - loss: 3.3207 - val_loss: 1.7570\n",
      "Epoch 7/10\n",
      "771599/771599 [==============================] - 111s 144us/step - loss: 2.9729 - val_loss: 1.5570\n",
      "Epoch 8/10\n",
      "771599/771599 [==============================] - 98s 126us/step - loss: 2.6965 - val_loss: 1.4991\n",
      "Epoch 9/10\n",
      "771599/771599 [==============================] - 90s 116us/step - loss: 2.4764 - val_loss: 1.3505\n",
      "Epoch 10/10\n",
      "771599/771599 [==============================] - 95s 123us/step - loss: 2.2713 - val_loss: 1.2975\n",
      "385799/385799 [==============================] - 26s 66us/step\n",
      "771599/771599 [==============================] - 53s 69us/step\n",
      "Loss function: mean_squared_error\n",
      "Train on 771599 samples, validate on 128600 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-305-b9860e5cdc4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#Run it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mregressor_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    636\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    637\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 638\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2471\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2472\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2473\u001b[1;33m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m   2475\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;31m# not already marked as initialized.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                 is_initialized = session.run(\n\u001b[1;32m--> 189\u001b[1;33m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[0;32m    190\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1015\u001b[0m                 run_metadata):\n\u001b[0;32m   1016\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1064\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[1;32m-> 1066\u001b[1;33m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[0;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#############WARNING THIS TAKES LITERALLY FOREVER#############\n",
    "\n",
    "#This tope code makes sure that keras runs all cores. \n",
    "import tensorflow as tf\n",
    "from keras.backend import tensorflow_backend as K\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(\n",
    "                    intra_op_parallelism_threads=6)) as sess:\n",
    "    K.set_session(sess)\n",
    "    \n",
    "#Make the grid search object\n",
    "grid_search = GridSearchCV(estimator = regressor,\n",
    "                       param_grid = parameters)\n",
    "\n",
    "#Run it - note n_jobs = -1 ensures you use all cores on sklearn side. \n",
    "\n",
    "regressor_result = grid_search.fit(X_train, y_train, validation_data=(X_test, y_test), n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regressor_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-306-c27774be1132>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregressor_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/ML Case Study/lending-club/gridsearchCV_results.p'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'regressor_result' is not defined"
     ]
    }
   ],
   "source": [
    "#Save the grid search results. Not tested since the grid search never finished. \n",
    "pickle.dump(regressor_result, open('D:/ML Case Study/lending-club/gridsearchCV_results.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96999114782044793"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, predictions)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gradient Boosted Tree without grade and sub-grade\n",
    "grad_model = GradientBoostingRegressor()\n",
    "grad_model_hist = grad_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is:  0.64288672015\n",
      "MSE is: 8.01733836742\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosted Tree Metrics without grade and sub-grade. \n",
    "grad_pred = grad_model_hist.predict(X_test)\n",
    "print('R2 is: ', r2_score(y_test, grad_pred))\n",
    "print('MSE is:', mean_squared_error(y_test, grad_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>feat_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>installment</td>\n",
       "      <td>0.159929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>0.081189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>term_36</td>\n",
       "      <td>0.054869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>total_rec_prncp</td>\n",
       "      <td>0.050211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bc_open_to_buy</td>\n",
       "      <td>0.047106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>inq_last_6mths</td>\n",
       "      <td>0.044352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>average_fico</td>\n",
       "      <td>0.043216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>dti_final</td>\n",
       "      <td>0.041320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>num_tl_op_past_12m</td>\n",
       "      <td>0.038178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>total_acc</td>\n",
       "      <td>0.035583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>term_60</td>\n",
       "      <td>0.032177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>annual_inc_final</td>\n",
       "      <td>0.026425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>purpose_credit_card</td>\n",
       "      <td>0.025962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>acc_open_past_24mths</td>\n",
       "      <td>0.022187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>0.021971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>total_pymnt</td>\n",
       "      <td>0.020154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>purpose_other</td>\n",
       "      <td>0.019335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>percent_bc_gt_75</td>\n",
       "      <td>0.014493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>purpose_small_business</td>\n",
       "      <td>0.014340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>initial_list_status_w</td>\n",
       "      <td>0.012812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>loan_status_Current</td>\n",
       "      <td>0.012652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>tot_hi_cred_lim</td>\n",
       "      <td>0.012038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>mo_sin_old_rev_tl_op</td>\n",
       "      <td>0.011824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>total_pymnt_inv</td>\n",
       "      <td>0.011730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>mths_since_rcnt_il_cat_&lt;50</td>\n",
       "      <td>0.011386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>last_fico_range_high</td>\n",
       "      <td>0.011216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mo_sin_rcnt_tl</td>\n",
       "      <td>0.010973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>last_fico_range_low</td>\n",
       "      <td>0.010736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>verification_status_Verified</td>\n",
       "      <td>0.010621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>total_rev_hi_lim</td>\n",
       "      <td>0.010488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>initial_list_status_f</td>\n",
       "      <td>0.009224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>mths_since_rcnt_il_cat_Not Reported</td>\n",
       "      <td>0.008298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>num_il_tl</td>\n",
       "      <td>0.008101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>last_pymnt_amnt</td>\n",
       "      <td>0.007398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>loan_status_Fully Paid</td>\n",
       "      <td>0.006548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>verification_status_Not Verified</td>\n",
       "      <td>0.005807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>purpose_debt_consolidation</td>\n",
       "      <td>0.004961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>purpose_house</td>\n",
       "      <td>0.004590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>recoveries</td>\n",
       "      <td>0.003458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>mths_since_recent_bc_cat_50+</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>purpose_moving</td>\n",
       "      <td>0.002601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>mort_acc</td>\n",
       "      <td>0.002552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>purpose_home_improvement</td>\n",
       "      <td>0.002342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bc_util</td>\n",
       "      <td>0.002220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dti</td>\n",
       "      <td>0.002109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>disbursement_method_DirectPay</td>\n",
       "      <td>0.001195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>loan_status_Charged Off</td>\n",
       "      <td>0.000943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>mo_sin_old_il_acct_cat_Not Reported</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>disbursement_method_Cash</td>\n",
       "      <td>0.000630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>num_actv_bc_tl</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>total_bc_limit</td>\n",
       "      <td>0.000550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>issue_d_Oct-2013</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>mths_since_recent_bc_cat_&lt;50</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>issue_d_Aug-2013</td>\n",
       "      <td>0.000350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>issue_d_Mar-2017</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>issue_d_Apr-2017</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>pct_tl_nvr_dlq</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>issue_d_Sep-2013</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>issue_d_Oct-2012</td>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>total_bal_ex_mort</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>last_credit_pull_year_2017</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>issue_d_Sep-2012</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>last_credit_pull_month_12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>out_prncp</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>last_credit_pull_month_11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>last_credit_pull_month_10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>last_credit_pull_month_1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>last_credit_pull_month_9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>last_credit_pull_month_2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>last_credit_pull_month_3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>last_credit_pull_month_4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>last_credit_pull_month_5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>last_credit_pull_month_6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>last_credit_pull_month_7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>last_credit_pull_month_8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>sec_app_flag_0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>last_credit_pull_year_2012</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>last_credit_pull_year_2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>last_credit_pull_year_2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>last_credit_pull_year_2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>last_credit_pull_year_2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>earliest_cr_line_year_1933</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>sec_app_flag_1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>mths_since_recent_revol_delinq_cat_75+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>mths_since_last_record_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>mths_since_last_record_cat_80+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>mths_since_last_delinq_cat_0-50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>mths_since_recent_inq_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>mths_since_recent_inq_cat_5-15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>mths_since_recent_inq_cat_15+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>mths_since_recent_inq_cat_0-5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>mo_sin_old_il_acct_cat_200+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>mo_sin_old_il_acct_cat_150-200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>mo_sin_old_il_acct_cat_0-150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>mths_since_recent_bc_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>debt_settlement_flag_Y</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>debt_settlement_flag_N</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>application_type_Joint App</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>application_type_Individual</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>addr_state_WY</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>addr_state_WV</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>mths_since_last_delinq_cat_50-75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>mths_since_last_delinq_cat_75+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>mths_since_last_delinq_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>mths_since_last_major_derog_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>mths_since_last_record_cat_40-80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>mths_since_last_record_cat_0-40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>mths_since_recent_bc_dlq_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>mths_since_recent_bc_dlq_cat_75+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>mths_since_recent_bc_dlq_cat_50-75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>mths_since_recent_bc_dlq_cat_0-50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>mths_since_last_major_derog_cat_75+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>mths_since_rcnt_il_cat_50+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>mths_since_last_major_derog_cat_50-75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>mths_since_last_major_derog_cat_0-50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>mths_since_recent_revol_delinq_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>earliest_cr_line_year_1944</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>mths_since_recent_revol_delinq_cat_50-75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>mths_since_recent_revol_delinq_cat_0-50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>earliest_cr_line_year_1934</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>earliest_cr_line_year_1957</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>earliest_cr_line_year_1945</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>earliest_cr_line_year_2007</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>earliest_cr_line_year_2005</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>earliest_cr_line_year_2004</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>earliest_cr_line_year_2003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>earliest_cr_line_year_2002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>earliest_cr_line_year_2001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>earliest_cr_line_year_2000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>earliest_cr_line_year_1999</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>earliest_cr_line_year_1998</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>earliest_cr_line_year_1997</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>earliest_cr_line_year_1996</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>earliest_cr_line_year_1995</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>earliest_cr_line_year_1994</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>earliest_cr_line_year_1993</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>earliest_cr_line_year_1992</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>earliest_cr_line_year_1991</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>earliest_cr_line_year_1990</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>earliest_cr_line_year_1989</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>earliest_cr_line_year_2006</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>earliest_cr_line_year_2008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>earliest_cr_line_year_1946</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>earliest_cr_line_year_2009</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>earliest_cr_line_month_12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>earliest_cr_line_month_11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>earliest_cr_line_month_10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>earliest_cr_line_month_09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>earliest_cr_line_month_08</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>earliest_cr_line_month_07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>earliest_cr_line_month_06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>earliest_cr_line_month_05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>earliest_cr_line_month_04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>earliest_cr_line_month_03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>earliest_cr_line_month_02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>earliest_cr_line_month_01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>earliest_cr_line_year_2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>earliest_cr_line_year_2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>earliest_cr_line_year_2012</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>earliest_cr_line_year_2011</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>earliest_cr_line_year_2010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>earliest_cr_line_year_1988</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>earliest_cr_line_year_1987</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>earliest_cr_line_year_1986</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>earliest_cr_line_year_1985</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>earliest_cr_line_year_1964</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>earliest_cr_line_year_1963</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>earliest_cr_line_year_1962</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>earliest_cr_line_year_1961</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>earliest_cr_line_year_1960</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>earliest_cr_line_year_1959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>earliest_cr_line_year_1958</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>addr_state_WA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>earliest_cr_line_year_1956</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>earliest_cr_line_year_1955</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>earliest_cr_line_year_1954</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>earliest_cr_line_year_1953</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>earliest_cr_line_year_1952</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>earliest_cr_line_year_1951</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>earliest_cr_line_year_1950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>earliest_cr_line_year_1949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>earliest_cr_line_year_1948</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>earliest_cr_line_year_1965</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>earliest_cr_line_year_1966</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>earliest_cr_line_year_1967</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>earliest_cr_line_year_1977</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>earliest_cr_line_year_1984</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>earliest_cr_line_year_1983</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>earliest_cr_line_year_1982</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>earliest_cr_line_year_1981</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>earliest_cr_line_year_1980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>earliest_cr_line_year_1979</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>earliest_cr_line_year_1978</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>earliest_cr_line_year_1976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>earliest_cr_line_year_1968</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>earliest_cr_line_year_1975</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>earliest_cr_line_year_1974</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>earliest_cr_line_year_1973</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>earliest_cr_line_year_1972</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>earliest_cr_line_year_1971</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>earliest_cr_line_year_1970</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>earliest_cr_line_year_1969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>addr_state_WI</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>addr_state_MT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>addr_state_VT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>issue_d_Aug-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>issue_d_Feb-2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>issue_d_Dec-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>issue_d_Dec-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>issue_d_Dec-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>issue_d_Dec-2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>issue_d_Dec-2012</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>issue_d_Aug-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>issue_d_Aug-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>addr_state_VA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>issue_d_Aug-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>issue_d_Aug-2012</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>issue_d_Apr-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>issue_d_Apr-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>issue_d_Apr-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>issue_d_Apr-2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>verification_status_Source Verified</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>issue_d_Feb-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>issue_d_Feb-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>issue_d_Feb-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>issue_d_Feb-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>issue_d_Jun-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>issue_d_Jun-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>issue_d_Jun-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>issue_d_Jun-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>issue_d_Jun-2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>issue_d_Jul-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>issue_d_Jul-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>issue_d_Jul-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>issue_d_Jul-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>issue_d_Jul-2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>issue_d_Jan-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>issue_d_Jan-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>issue_d_Jan-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>issue_d_Jan-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>issue_d_Jan-2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>home_ownership_RENT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>home_ownership_OWN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>home_ownership_OTHER</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>num_op_rev_tl</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>num_bc_sats</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>num_actv_rev_tl</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>num_accts_ever_120_pd</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>mo_sin_rcnt_rev_tl_op</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>delinq_amnt</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>chargeoff_within_12_mths</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>revol_bal</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>avg_cur_bal</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tot_cur_bal</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tot_coll_amt</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>acc_now_delinq</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>collections_12_mths_ex_med</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>collection_recovery_fee</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>total_rec_late_fee</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>revol_util</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>num_bc_tl</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>num_rev_accts</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>home_ownership_NONE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>num_rev_tl_bal_gt_0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>home_ownership_MORTGAGE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>home_ownership_ANY</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>delinq_2yrs</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>open_acc</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>emp_length_floats</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>earliest_cr_line_months</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>sec_app_fico_best</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>total_il_high_credit_limit</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>tax_liens</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>pub_rec_bankruptcies</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pub_rec</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>num_tl_90g_dpd_24m</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>num_tl_30dpd</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>num_tl_120dpd_2m</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>num_sats</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>issue_d_Mar-2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>issue_d_Mar-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>issue_d_Mar-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>addr_state_NC</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>addr_state_MS</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>addr_state_MO</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>addr_state_MN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>addr_state_MI</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>addr_state_MD</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>addr_state_MA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>addr_state_LA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>addr_state_KY</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>addr_state_KS</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>addr_state_IN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>addr_state_IL</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>addr_state_ID</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>addr_state_IA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>addr_state_HI</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>addr_state_GA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>out_prncp_inv</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>addr_state_ND</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>addr_state_DE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>addr_state_NE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>addr_state_UT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>addr_state_TX</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>addr_state_TN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>addr_state_SD</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>addr_state_SC</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>addr_state_RI</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>addr_state_PA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>addr_state_OR</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>addr_state_OK</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>addr_state_OH</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>addr_state_NY</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>addr_state_NV</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>addr_state_NM</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>addr_state_NJ</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>addr_state_NH</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>addr_state_FL</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>addr_state_DC</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>issue_d_Mar-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>issue_d_Sep-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>issue_d_Sep-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>issue_d_Sep-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>issue_d_Oct-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>issue_d_Oct-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>issue_d_Oct-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>issue_d_Nov-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>issue_d_Nov-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>issue_d_Nov-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>issue_d_Nov-2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>issue_d_Nov-2012</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>issue_d_May-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>issue_d_May-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>issue_d_May-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>issue_d_May-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>issue_d_May-2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>issue_d_Sep-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>loan_status_Default</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>addr_state_CT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>loan_status_In Grace Period</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>addr_state_CO</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>addr_state_CA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>addr_state_AZ</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>addr_state_AR</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>addr_state_AL</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>addr_state_AK</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>purpose_wedding</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>purpose_vacation</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>purpose_renewable_energy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>purpose_medical</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>purpose_major_purchase</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>purpose_educational</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>purpose_car</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>loan_status_Late (31-120 days)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>loan_status_Late (16-30 days)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>addr_state_ME</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             columns  feat_score\n",
       "1                                        installment    0.159929\n",
       "0                                          loan_amnt    0.081189\n",
       "354                                          term_36    0.054869\n",
       "15                                   total_rec_prncp    0.050211\n",
       "29                                    bc_open_to_buy    0.047106\n",
       "5                                     inq_last_6mths    0.044352\n",
       "63                                      average_fico    0.043216\n",
       "64                                         dti_final    0.041320\n",
       "50                                num_tl_op_past_12m    0.038178\n",
       "10                                         total_acc    0.035583\n",
       "355                                          term_60    0.032177\n",
       "60                                  annual_inc_final    0.026425\n",
       "144                              purpose_credit_card    0.025962\n",
       "27                              acc_open_past_24mths    0.022187\n",
       "2                                         annual_inc    0.021971\n",
       "13                                       total_pymnt    0.020154\n",
       "152                                    purpose_other    0.019335\n",
       "52                                  percent_bc_gt_75    0.014493\n",
       "154                           purpose_small_business    0.014340\n",
       "209                            initial_list_status_w    0.012812\n",
       "137                              loan_status_Current    0.012652\n",
       "55                                   tot_hi_cred_lim    0.012038\n",
       "33                              mo_sin_old_rev_tl_op    0.011824\n",
       "14                                   total_pymnt_inv    0.011730\n",
       "232                       mths_since_rcnt_il_cat_<50    0.011386\n",
       "20                              last_fico_range_high    0.011216\n",
       "35                                    mo_sin_rcnt_tl    0.010973\n",
       "21                               last_fico_range_low    0.010736\n",
       "73                      verification_status_Verified    0.010621\n",
       "26                                  total_rev_hi_lim    0.010488\n",
       "208                            initial_list_status_f    0.009224\n",
       "233              mths_since_rcnt_il_cat_Not Reported    0.008298\n",
       "42                                         num_il_tl    0.008101\n",
       "19                                   last_pymnt_amnt    0.007398\n",
       "139                           loan_status_Fully Paid    0.006548\n",
       "71                  verification_status_Not Verified    0.005807\n",
       "145                       purpose_debt_consolidation    0.004961\n",
       "148                                    purpose_house    0.004590\n",
       "17                                        recoveries    0.003458\n",
       "216                     mths_since_recent_bc_cat_50+    0.002849\n",
       "151                                   purpose_moving    0.002601\n",
       "36                                          mort_acc    0.002552\n",
       "147                         purpose_home_improvement    0.002342\n",
       "30                                           bc_util    0.002220\n",
       "3                                                dti    0.002109\n",
       "213                    disbursement_method_DirectPay    0.001195\n",
       "136                          loan_status_Charged Off    0.000943\n",
       "222              mo_sin_old_il_acct_cat_Not Reported    0.000692\n",
       "212                         disbursement_method_Cash    0.000630\n",
       "38                                    num_actv_bc_tl    0.000600\n",
       "57                                    total_bc_limit    0.000550\n",
       "126                                 issue_d_Oct-2013    0.000460\n",
       "217                     mths_since_recent_bc_cat_<50    0.000449\n",
       "80                                  issue_d_Aug-2013    0.000350\n",
       "114                                 issue_d_Mar-2017    0.000330\n",
       "78                                  issue_d_Apr-2017    0.000326\n",
       "51                                    pct_tl_nvr_dlq    0.000293\n",
       "131                                 issue_d_Sep-2013    0.000278\n",
       "125                                 issue_d_Oct-2012    0.000134\n",
       "56                                 total_bal_ex_mort    0.000094\n",
       "269                       last_credit_pull_year_2017    0.000077\n",
       "130                                 issue_d_Sep-2012    0.000070\n",
       "255                        last_credit_pull_month_12    0.000000\n",
       "11                                         out_prncp    0.000000\n",
       "254                        last_credit_pull_month_11    0.000000\n",
       "253                        last_credit_pull_month_10    0.000000\n",
       "252                         last_credit_pull_month_1    0.000000\n",
       "263                         last_credit_pull_month_9    0.000000\n",
       "256                         last_credit_pull_month_2    0.000000\n",
       "257                         last_credit_pull_month_3    0.000000\n",
       "258                         last_credit_pull_month_4    0.000000\n",
       "259                         last_credit_pull_month_5    0.000000\n",
       "260                         last_credit_pull_month_6    0.000000\n",
       "261                         last_credit_pull_month_7    0.000000\n",
       "262                         last_credit_pull_month_8    0.000000\n",
       "250                                   sec_app_flag_0    0.000000\n",
       "264                       last_credit_pull_year_2012    0.000000\n",
       "265                       last_credit_pull_year_2013    0.000000\n",
       "266                       last_credit_pull_year_2014    0.000000\n",
       "267                       last_credit_pull_year_2015    0.000000\n",
       "268                       last_credit_pull_year_2016    0.000000\n",
       "270                       earliest_cr_line_year_1933    0.000000\n",
       "251                                   sec_app_flag_1    0.000000\n",
       "236           mths_since_recent_revol_delinq_cat_75+    0.000000\n",
       "249          mths_since_last_record_cat_Not Reported    0.000000\n",
       "248                   mths_since_last_record_cat_80+    0.000000\n",
       "227                  mths_since_last_delinq_cat_0-50    0.000000\n",
       "226           mths_since_recent_inq_cat_Not Reported    0.000000\n",
       "225                   mths_since_recent_inq_cat_5-15    0.000000\n",
       "224                    mths_since_recent_inq_cat_15+    0.000000\n",
       "223                    mths_since_recent_inq_cat_0-5    0.000000\n",
       "221                      mo_sin_old_il_acct_cat_200+    0.000000\n",
       "220                   mo_sin_old_il_acct_cat_150-200    0.000000\n",
       "219                     mo_sin_old_il_acct_cat_0-150    0.000000\n",
       "218            mths_since_recent_bc_cat_Not Reported    0.000000\n",
       "215                           debt_settlement_flag_Y    0.000000\n",
       "214                           debt_settlement_flag_N    0.000000\n",
       "211                       application_type_Joint App    0.000000\n",
       "210                      application_type_Individual    0.000000\n",
       "207                                    addr_state_WY    0.000000\n",
       "206                                    addr_state_WV    0.000000\n",
       "228                 mths_since_last_delinq_cat_50-75    0.000000\n",
       "229                   mths_since_last_delinq_cat_75+    0.000000\n",
       "230          mths_since_last_delinq_cat_Not Reported    0.000000\n",
       "241     mths_since_last_major_derog_cat_Not Reported    0.000000\n",
       "247                 mths_since_last_record_cat_40-80    0.000000\n",
       "246                  mths_since_last_record_cat_0-40    0.000000\n",
       "245        mths_since_recent_bc_dlq_cat_Not Reported    0.000000\n",
       "244                 mths_since_recent_bc_dlq_cat_75+    0.000000\n",
       "243               mths_since_recent_bc_dlq_cat_50-75    0.000000\n",
       "242                mths_since_recent_bc_dlq_cat_0-50    0.000000\n",
       "240              mths_since_last_major_derog_cat_75+    0.000000\n",
       "231                       mths_since_rcnt_il_cat_50+    0.000000\n",
       "239            mths_since_last_major_derog_cat_50-75    0.000000\n",
       "238             mths_since_last_major_derog_cat_0-50    0.000000\n",
       "237  mths_since_recent_revol_delinq_cat_Not Reported    0.000000\n",
       "272                       earliest_cr_line_year_1944    0.000000\n",
       "235         mths_since_recent_revol_delinq_cat_50-75    0.000000\n",
       "234          mths_since_recent_revol_delinq_cat_0-50    0.000000\n",
       "271                       earliest_cr_line_year_1934    0.000000\n",
       "284                       earliest_cr_line_year_1957    0.000000\n",
       "273                       earliest_cr_line_year_1945    0.000000\n",
       "334                       earliest_cr_line_year_2007    0.000000\n",
       "332                       earliest_cr_line_year_2005    0.000000\n",
       "331                       earliest_cr_line_year_2004    0.000000\n",
       "330                       earliest_cr_line_year_2003    0.000000\n",
       "329                       earliest_cr_line_year_2002    0.000000\n",
       "328                       earliest_cr_line_year_2001    0.000000\n",
       "327                       earliest_cr_line_year_2000    0.000000\n",
       "326                       earliest_cr_line_year_1999    0.000000\n",
       "325                       earliest_cr_line_year_1998    0.000000\n",
       "324                       earliest_cr_line_year_1997    0.000000\n",
       "323                       earliest_cr_line_year_1996    0.000000\n",
       "322                       earliest_cr_line_year_1995    0.000000\n",
       "321                       earliest_cr_line_year_1994    0.000000\n",
       "320                       earliest_cr_line_year_1993    0.000000\n",
       "319                       earliest_cr_line_year_1992    0.000000\n",
       "318                       earliest_cr_line_year_1991    0.000000\n",
       "317                       earliest_cr_line_year_1990    0.000000\n",
       "316                       earliest_cr_line_year_1989    0.000000\n",
       "333                       earliest_cr_line_year_2006    0.000000\n",
       "335                       earliest_cr_line_year_2008    0.000000\n",
       "274                       earliest_cr_line_year_1946    0.000000\n",
       "336                       earliest_cr_line_year_2009    0.000000\n",
       "353                        earliest_cr_line_month_12    0.000000\n",
       "352                        earliest_cr_line_month_11    0.000000\n",
       "351                        earliest_cr_line_month_10    0.000000\n",
       "350                        earliest_cr_line_month_09    0.000000\n",
       "349                        earliest_cr_line_month_08    0.000000\n",
       "348                        earliest_cr_line_month_07    0.000000\n",
       "347                        earliest_cr_line_month_06    0.000000\n",
       "346                        earliest_cr_line_month_05    0.000000\n",
       "345                        earliest_cr_line_month_04    0.000000\n",
       "344                        earliest_cr_line_month_03    0.000000\n",
       "343                        earliest_cr_line_month_02    0.000000\n",
       "342                        earliest_cr_line_month_01    0.000000\n",
       "341                       earliest_cr_line_year_2014    0.000000\n",
       "340                       earliest_cr_line_year_2013    0.000000\n",
       "339                       earliest_cr_line_year_2012    0.000000\n",
       "338                       earliest_cr_line_year_2011    0.000000\n",
       "337                       earliest_cr_line_year_2010    0.000000\n",
       "315                       earliest_cr_line_year_1988    0.000000\n",
       "314                       earliest_cr_line_year_1987    0.000000\n",
       "313                       earliest_cr_line_year_1986    0.000000\n",
       "312                       earliest_cr_line_year_1985    0.000000\n",
       "291                       earliest_cr_line_year_1964    0.000000\n",
       "290                       earliest_cr_line_year_1963    0.000000\n",
       "289                       earliest_cr_line_year_1962    0.000000\n",
       "288                       earliest_cr_line_year_1961    0.000000\n",
       "287                       earliest_cr_line_year_1960    0.000000\n",
       "286                       earliest_cr_line_year_1959    0.000000\n",
       "285                       earliest_cr_line_year_1958    0.000000\n",
       "204                                    addr_state_WA    0.000000\n",
       "283                       earliest_cr_line_year_1956    0.000000\n",
       "282                       earliest_cr_line_year_1955    0.000000\n",
       "281                       earliest_cr_line_year_1954    0.000000\n",
       "280                       earliest_cr_line_year_1953    0.000000\n",
       "279                       earliest_cr_line_year_1952    0.000000\n",
       "278                       earliest_cr_line_year_1951    0.000000\n",
       "277                       earliest_cr_line_year_1950    0.000000\n",
       "276                       earliest_cr_line_year_1949    0.000000\n",
       "275                       earliest_cr_line_year_1948    0.000000\n",
       "292                       earliest_cr_line_year_1965    0.000000\n",
       "293                       earliest_cr_line_year_1966    0.000000\n",
       "294                       earliest_cr_line_year_1967    0.000000\n",
       "304                       earliest_cr_line_year_1977    0.000000\n",
       "311                       earliest_cr_line_year_1984    0.000000\n",
       "310                       earliest_cr_line_year_1983    0.000000\n",
       "309                       earliest_cr_line_year_1982    0.000000\n",
       "308                       earliest_cr_line_year_1981    0.000000\n",
       "307                       earliest_cr_line_year_1980    0.000000\n",
       "306                       earliest_cr_line_year_1979    0.000000\n",
       "305                       earliest_cr_line_year_1978    0.000000\n",
       "303                       earliest_cr_line_year_1976    0.000000\n",
       "295                       earliest_cr_line_year_1968    0.000000\n",
       "302                       earliest_cr_line_year_1975    0.000000\n",
       "301                       earliest_cr_line_year_1974    0.000000\n",
       "300                       earliest_cr_line_year_1973    0.000000\n",
       "299                       earliest_cr_line_year_1972    0.000000\n",
       "298                       earliest_cr_line_year_1971    0.000000\n",
       "297                       earliest_cr_line_year_1970    0.000000\n",
       "296                       earliest_cr_line_year_1969    0.000000\n",
       "205                                    addr_state_WI    0.000000\n",
       "183                                    addr_state_MT    0.000000\n",
       "203                                    addr_state_VT    0.000000\n",
       "83                                  issue_d_Aug-2016    0.000000\n",
       "90                                  issue_d_Feb-2013    0.000000\n",
       "89                                  issue_d_Dec-2016    0.000000\n",
       "88                                  issue_d_Dec-2015    0.000000\n",
       "87                                  issue_d_Dec-2014    0.000000\n",
       "86                                  issue_d_Dec-2013    0.000000\n",
       "85                                  issue_d_Dec-2012    0.000000\n",
       "84                                  issue_d_Aug-2017    0.000000\n",
       "82                                  issue_d_Aug-2015    0.000000\n",
       "202                                    addr_state_VA    0.000000\n",
       "81                                  issue_d_Aug-2014    0.000000\n",
       "79                                  issue_d_Aug-2012    0.000000\n",
       "77                                  issue_d_Apr-2016    0.000000\n",
       "76                                  issue_d_Apr-2015    0.000000\n",
       "75                                  issue_d_Apr-2014    0.000000\n",
       "74                                  issue_d_Apr-2013    0.000000\n",
       "72               verification_status_Source Verified    0.000000\n",
       "91                                  issue_d_Feb-2014    0.000000\n",
       "92                                  issue_d_Feb-2015    0.000000\n",
       "93                                  issue_d_Feb-2016    0.000000\n",
       "94                                  issue_d_Feb-2017    0.000000\n",
       "109                                 issue_d_Jun-2017    0.000000\n",
       "108                                 issue_d_Jun-2016    0.000000\n",
       "107                                 issue_d_Jun-2015    0.000000\n",
       "106                                 issue_d_Jun-2014    0.000000\n",
       "105                                 issue_d_Jun-2013    0.000000\n",
       "104                                 issue_d_Jul-2017    0.000000\n",
       "103                                 issue_d_Jul-2016    0.000000\n",
       "102                                 issue_d_Jul-2015    0.000000\n",
       "101                                 issue_d_Jul-2014    0.000000\n",
       "100                                 issue_d_Jul-2013    0.000000\n",
       "99                                  issue_d_Jan-2017    0.000000\n",
       "98                                  issue_d_Jan-2016    0.000000\n",
       "97                                  issue_d_Jan-2015    0.000000\n",
       "96                                  issue_d_Jan-2014    0.000000\n",
       "95                                  issue_d_Jan-2013    0.000000\n",
       "70                               home_ownership_RENT    0.000000\n",
       "69                                home_ownership_OWN    0.000000\n",
       "68                              home_ownership_OTHER    0.000000\n",
       "43                                     num_op_rev_tl    0.000000\n",
       "40                                       num_bc_sats    0.000000\n",
       "39                                   num_actv_rev_tl    0.000000\n",
       "37                             num_accts_ever_120_pd    0.000000\n",
       "34                             mo_sin_rcnt_rev_tl_op    0.000000\n",
       "32                                       delinq_amnt    0.000000\n",
       "31                          chargeoff_within_12_mths    0.000000\n",
       "8                                          revol_bal    0.000000\n",
       "28                                       avg_cur_bal    0.000000\n",
       "25                                       tot_cur_bal    0.000000\n",
       "24                                      tot_coll_amt    0.000000\n",
       "23                                    acc_now_delinq    0.000000\n",
       "22                        collections_12_mths_ex_med    0.000000\n",
       "18                           collection_recovery_fee    0.000000\n",
       "16                                total_rec_late_fee    0.000000\n",
       "9                                         revol_util    0.000000\n",
       "41                                         num_bc_tl    0.000000\n",
       "44                                     num_rev_accts    0.000000\n",
       "67                               home_ownership_NONE    0.000000\n",
       "45                               num_rev_tl_bal_gt_0    0.000000\n",
       "66                           home_ownership_MORTGAGE    0.000000\n",
       "65                                home_ownership_ANY    0.000000\n",
       "4                                        delinq_2yrs    0.000000\n",
       "6                                           open_acc    0.000000\n",
       "62                                 emp_length_floats    0.000000\n",
       "61                           earliest_cr_line_months    0.000000\n",
       "59                                 sec_app_fico_best    0.000000\n",
       "58                        total_il_high_credit_limit    0.000000\n",
       "54                                         tax_liens    0.000000\n",
       "53                              pub_rec_bankruptcies    0.000000\n",
       "7                                            pub_rec    0.000000\n",
       "49                                num_tl_90g_dpd_24m    0.000000\n",
       "48                                      num_tl_30dpd    0.000000\n",
       "47                                  num_tl_120dpd_2m    0.000000\n",
       "46                                          num_sats    0.000000\n",
       "110                                 issue_d_Mar-2013    0.000000\n",
       "111                                 issue_d_Mar-2014    0.000000\n",
       "112                                 issue_d_Mar-2015    0.000000\n",
       "184                                    addr_state_NC    0.000000\n",
       "182                                    addr_state_MS    0.000000\n",
       "181                                    addr_state_MO    0.000000\n",
       "180                                    addr_state_MN    0.000000\n",
       "179                                    addr_state_MI    0.000000\n",
       "177                                    addr_state_MD    0.000000\n",
       "176                                    addr_state_MA    0.000000\n",
       "175                                    addr_state_LA    0.000000\n",
       "174                                    addr_state_KY    0.000000\n",
       "173                                    addr_state_KS    0.000000\n",
       "172                                    addr_state_IN    0.000000\n",
       "171                                    addr_state_IL    0.000000\n",
       "170                                    addr_state_ID    0.000000\n",
       "169                                    addr_state_IA    0.000000\n",
       "168                                    addr_state_HI    0.000000\n",
       "167                                    addr_state_GA    0.000000\n",
       "12                                     out_prncp_inv    0.000000\n",
       "185                                    addr_state_ND    0.000000\n",
       "165                                    addr_state_DE    0.000000\n",
       "186                                    addr_state_NE    0.000000\n",
       "201                                    addr_state_UT    0.000000\n",
       "200                                    addr_state_TX    0.000000\n",
       "199                                    addr_state_TN    0.000000\n",
       "198                                    addr_state_SD    0.000000\n",
       "197                                    addr_state_SC    0.000000\n",
       "196                                    addr_state_RI    0.000000\n",
       "195                                    addr_state_PA    0.000000\n",
       "194                                    addr_state_OR    0.000000\n",
       "193                                    addr_state_OK    0.000000\n",
       "192                                    addr_state_OH    0.000000\n",
       "191                                    addr_state_NY    0.000000\n",
       "190                                    addr_state_NV    0.000000\n",
       "189                                    addr_state_NM    0.000000\n",
       "188                                    addr_state_NJ    0.000000\n",
       "187                                    addr_state_NH    0.000000\n",
       "166                                    addr_state_FL    0.000000\n",
       "164                                    addr_state_DC    0.000000\n",
       "113                                 issue_d_Mar-2016    0.000000\n",
       "135                                 issue_d_Sep-2017    0.000000\n",
       "133                                 issue_d_Sep-2015    0.000000\n",
       "132                                 issue_d_Sep-2014    0.000000\n",
       "129                                 issue_d_Oct-2016    0.000000\n",
       "128                                 issue_d_Oct-2015    0.000000\n",
       "127                                 issue_d_Oct-2014    0.000000\n",
       "124                                 issue_d_Nov-2016    0.000000\n",
       "123                                 issue_d_Nov-2015    0.000000\n",
       "122                                 issue_d_Nov-2014    0.000000\n",
       "121                                 issue_d_Nov-2013    0.000000\n",
       "120                                 issue_d_Nov-2012    0.000000\n",
       "119                                 issue_d_May-2017    0.000000\n",
       "118                                 issue_d_May-2016    0.000000\n",
       "117                                 issue_d_May-2015    0.000000\n",
       "116                                 issue_d_May-2014    0.000000\n",
       "115                                 issue_d_May-2013    0.000000\n",
       "134                                 issue_d_Sep-2016    0.000000\n",
       "138                              loan_status_Default    0.000000\n",
       "163                                    addr_state_CT    0.000000\n",
       "140                      loan_status_In Grace Period    0.000000\n",
       "162                                    addr_state_CO    0.000000\n",
       "161                                    addr_state_CA    0.000000\n",
       "160                                    addr_state_AZ    0.000000\n",
       "159                                    addr_state_AR    0.000000\n",
       "158                                    addr_state_AL    0.000000\n",
       "157                                    addr_state_AK    0.000000\n",
       "156                                  purpose_wedding    0.000000\n",
       "155                                 purpose_vacation    0.000000\n",
       "153                         purpose_renewable_energy    0.000000\n",
       "150                                  purpose_medical    0.000000\n",
       "149                           purpose_major_purchase    0.000000\n",
       "146                              purpose_educational    0.000000\n",
       "143                                      purpose_car    0.000000\n",
       "142                   loan_status_Late (31-120 days)    0.000000\n",
       "141                    loan_status_Late (16-30 days)    0.000000\n",
       "178                                    addr_state_ME    0.000000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature importance dataframe from the gradient boosted tree without grade and subgrade. \n",
    "grad_features=pd.DataFrame({'feat_score':grad_model.feature_importances_, 'columns': col_names}).sort_values(by='feat_score', ascending = False)\n",
    "grad_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# mean_squared_error(y_test, ada_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_model = GradientBoostingRegressor()\n",
    "grad_model_hist = grad_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is:  0.97998429316\n",
      "MSE is: 0.448876801253\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosted Tree\n",
    "grad_pred = grad_model_hist.predict(X_test)\n",
    "print('R2 is: ', r2_score(y_test, grad_pred))\n",
    "print('MSE is:', mean_squared_error(y_test, grad_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>feat_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>grade_A</td>\n",
       "      <td>0.121174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub_grade</td>\n",
       "      <td>0.099989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>grade_E</td>\n",
       "      <td>0.082811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>grade_F</td>\n",
       "      <td>0.080437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>grade_B</td>\n",
       "      <td>0.079938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>grade_G</td>\n",
       "      <td>0.069139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>mths_since_rcnt_il_cat_Not Reported</td>\n",
       "      <td>0.040754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>grade_D</td>\n",
       "      <td>0.022437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>grade_C</td>\n",
       "      <td>0.020738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>issue_d_Dec-2015</td>\n",
       "      <td>0.019895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>total_rec_prncp</td>\n",
       "      <td>0.018123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>issue_d_Oct-2013</td>\n",
       "      <td>0.017758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>issue_d_Oct-2015</td>\n",
       "      <td>0.016605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>issue_d_Jul-2015</td>\n",
       "      <td>0.016592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>issue_d_Nov-2013</td>\n",
       "      <td>0.016261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>issue_d_Nov-2015</td>\n",
       "      <td>0.016050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>issue_d_Jan-2016</td>\n",
       "      <td>0.013320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>issue_d_May-2015</td>\n",
       "      <td>0.012284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>issue_d_Aug-2015</td>\n",
       "      <td>0.012224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>issue_d_Sep-2013</td>\n",
       "      <td>0.012174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>issue_d_Dec-2013</td>\n",
       "      <td>0.011831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>issue_d_Apr-2015</td>\n",
       "      <td>0.011752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>issue_d_Mar-2016</td>\n",
       "      <td>0.011737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>issue_d_Apr-2013</td>\n",
       "      <td>0.011283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>issue_d_Jun-2013</td>\n",
       "      <td>0.010950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>issue_d_May-2013</td>\n",
       "      <td>0.010856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>out_prncp_inv</td>\n",
       "      <td>0.010368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>issue_d_Feb-2016</td>\n",
       "      <td>0.009302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>issue_d_Sep-2015</td>\n",
       "      <td>0.009176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>issue_d_Jan-2013</td>\n",
       "      <td>0.009131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>issue_d_Jan-2014</td>\n",
       "      <td>0.009115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>issue_d_Mar-2015</td>\n",
       "      <td>0.009057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>issue_d_Feb-2013</td>\n",
       "      <td>0.008939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>issue_d_Jun-2015</td>\n",
       "      <td>0.008468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>issue_d_Mar-2013</td>\n",
       "      <td>0.008182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>issue_d_Aug-2013</td>\n",
       "      <td>0.006106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>issue_d_Jul-2013</td>\n",
       "      <td>0.005879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>initial_list_status_w</td>\n",
       "      <td>0.005429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>total_rec_int</td>\n",
       "      <td>0.004521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>initial_list_status_f</td>\n",
       "      <td>0.003735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>issue_d_Apr-2016</td>\n",
       "      <td>0.003185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>issue_d_Feb-2015</td>\n",
       "      <td>0.003090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>issue_d_Nov-2012</td>\n",
       "      <td>0.003070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>issue_d_Dec-2012</td>\n",
       "      <td>0.003066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>issue_d_Feb-2014</td>\n",
       "      <td>0.002980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>issue_d_Oct-2012</td>\n",
       "      <td>0.002979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>verification_status_Verified</td>\n",
       "      <td>0.002888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>issue_d_Sep-2012</td>\n",
       "      <td>0.002874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>issue_d_Apr-2014</td>\n",
       "      <td>0.002866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>out_prncp</td>\n",
       "      <td>0.002572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>loan_status_Fully Paid</td>\n",
       "      <td>0.001928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>loan_status_Current</td>\n",
       "      <td>0.001564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>last_credit_pull_year_2017</td>\n",
       "      <td>0.001444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>last_pymnt_amnt</td>\n",
       "      <td>0.000412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>issue_d_Mar-2017</td>\n",
       "      <td>0.000294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>num_il_tl</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>debt_settlement_flag_Y</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>last_credit_pull_year_2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>last_credit_pull_year_2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>debt_settlement_flag_N</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>last_credit_pull_year_2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>disbursement_method_DirectPay</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>mths_since_recent_inq_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>disbursement_method_Cash</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>application_type_Joint App</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>last_credit_pull_year_2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>application_type_Individual</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>earliest_cr_line_year_1933</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>addr_state_WY</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>addr_state_WV</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>addr_state_WI</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>earliest_cr_line_year_1934</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>earliest_cr_line_year_1944</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>addr_state_WA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>addr_state_VT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>earliest_cr_line_year_1945</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>addr_state_VA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>mths_since_recent_bc_cat_50+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>last_credit_pull_month_9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>last_credit_pull_year_2012</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>mths_since_recent_revol_delinq_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>mths_since_recent_bc_dlq_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>mths_since_recent_bc_dlq_cat_75+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>mths_since_recent_bc_dlq_cat_50-75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>mths_since_recent_bc_dlq_cat_0-50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>mths_since_last_major_derog_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>mths_since_last_major_derog_cat_75+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>mths_since_last_major_derog_cat_50-75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>mths_since_last_major_derog_cat_0-50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>mths_since_recent_revol_delinq_cat_75+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>mths_since_last_record_cat_40-80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>mths_since_recent_revol_delinq_cat_50-75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>mths_since_recent_revol_delinq_cat_0-50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>mths_since_recent_inq_cat_5-15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>mths_since_rcnt_il_cat_&lt;50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>mths_since_rcnt_il_cat_50+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>mths_since_last_delinq_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>mths_since_last_delinq_cat_75+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>mths_since_last_delinq_cat_50-75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>mths_since_last_record_cat_0-40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>mths_since_last_record_cat_80+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>mths_since_last_delinq_cat_0-50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>last_credit_pull_month_7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>last_credit_pull_month_8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>mths_since_recent_bc_cat_&lt;50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>mths_since_recent_bc_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>mo_sin_old_il_acct_cat_0-150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>mo_sin_old_il_acct_cat_150-200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>mo_sin_old_il_acct_cat_200+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>mo_sin_old_il_acct_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>mths_since_recent_inq_cat_0-5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>last_credit_pull_month_6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>mths_since_last_record_cat_Not Reported</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>last_credit_pull_month_5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>last_credit_pull_month_4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>last_credit_pull_month_3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>mths_since_recent_inq_cat_15+</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>last_credit_pull_month_12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>last_credit_pull_month_11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>last_credit_pull_month_10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>last_credit_pull_month_1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>last_credit_pull_month_2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>earliest_cr_line_year_1946</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>earliest_cr_line_year_2000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>earliest_cr_line_year_2008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>earliest_cr_line_year_2007</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>earliest_cr_line_year_2006</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>earliest_cr_line_year_2005</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>earliest_cr_line_year_2004</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>earliest_cr_line_year_2003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>earliest_cr_line_year_2002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>earliest_cr_line_year_2001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>earliest_cr_line_year_1999</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>earliest_cr_line_year_1989</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>earliest_cr_line_year_1998</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>earliest_cr_line_year_1997</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>earliest_cr_line_year_1996</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>earliest_cr_line_year_1995</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>earliest_cr_line_year_1994</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>earliest_cr_line_year_1993</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>earliest_cr_line_year_1992</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>earliest_cr_line_year_1991</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>earliest_cr_line_year_2009</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>earliest_cr_line_year_2010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>earliest_cr_line_year_2011</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>earliest_cr_line_year_2012</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>sec_app_flag_0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>term_60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>term_36</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>earliest_cr_line_month_12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>earliest_cr_line_month_11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>earliest_cr_line_month_10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>earliest_cr_line_month_09</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>earliest_cr_line_month_08</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>earliest_cr_line_month_07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>earliest_cr_line_month_06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>earliest_cr_line_month_05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>earliest_cr_line_month_04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>earliest_cr_line_month_03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>earliest_cr_line_month_02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>earliest_cr_line_month_01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>earliest_cr_line_year_2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>earliest_cr_line_year_2013</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>earliest_cr_line_year_1990</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>earliest_cr_line_year_1988</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>earliest_cr_line_year_1948</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>earliest_cr_line_year_1957</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>earliest_cr_line_year_1965</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>earliest_cr_line_year_1964</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>earliest_cr_line_year_1963</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>earliest_cr_line_year_1962</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>earliest_cr_line_year_1961</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>earliest_cr_line_year_1960</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>earliest_cr_line_year_1959</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>earliest_cr_line_year_1958</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>earliest_cr_line_year_1956</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>earliest_cr_line_year_1987</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>addr_state_TX</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>earliest_cr_line_year_1955</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>earliest_cr_line_year_1954</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>earliest_cr_line_year_1953</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>earliest_cr_line_year_1952</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>earliest_cr_line_year_1951</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>earliest_cr_line_year_1950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>earliest_cr_line_year_1949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>earliest_cr_line_year_1966</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>earliest_cr_line_year_1967</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>earliest_cr_line_year_1968</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>earliest_cr_line_year_1969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>earliest_cr_line_year_1986</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>earliest_cr_line_year_1985</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>earliest_cr_line_year_1984</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>earliest_cr_line_year_1983</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>earliest_cr_line_year_1982</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>earliest_cr_line_year_1981</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>earliest_cr_line_year_1980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>earliest_cr_line_year_1979</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>earliest_cr_line_year_1978</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>earliest_cr_line_year_1977</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>earliest_cr_line_year_1976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>earliest_cr_line_year_1975</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>earliest_cr_line_year_1974</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>earliest_cr_line_year_1973</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>earliest_cr_line_year_1972</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>earliest_cr_line_year_1971</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>earliest_cr_line_year_1970</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>addr_state_UT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>addr_state_KS</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>addr_state_TN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>tax_liens</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>earliest_cr_line_months</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>annual_inc_final</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>sec_app_fico_best</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>total_il_high_credit_limit</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>total_bc_limit</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>total_bal_ex_mort</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>tot_hi_cred_lim</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>pub_rec_bankruptcies</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>average_fico</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>percent_bc_gt_75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>pct_tl_nvr_dlq</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>num_tl_op_past_12m</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>num_tl_90g_dpd_24m</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>num_tl_30dpd</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>num_tl_120dpd_2m</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>num_sats</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>emp_length_floats</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>dti_final</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>addr_state_SD</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>issue_d_Aug-2012</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>issue_d_Jan-2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>issue_d_Feb-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>issue_d_Dec-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>issue_d_Dec-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>issue_d_Aug-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>issue_d_Aug-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>issue_d_Aug-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>issue_d_Apr-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>home_ownership_ANY</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>verification_status_Source Verified</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>verification_status_Not Verified</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>home_ownership_RENT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>home_ownership_OWN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>home_ownership_OTHER</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>home_ownership_NONE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>home_ownership_MORTGAGE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>num_rev_tl_bal_gt_0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>num_rev_accts</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>num_op_rev_tl</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>total_acc</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>last_fico_range_low</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>last_fico_range_high</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>collection_recovery_fee</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>recoveries</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>total_rec_late_fee</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>total_pymnt_inv</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>total_pymnt</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>revol_util</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>num_bc_tl</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>revol_bal</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pub_rec</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>open_acc</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>inq_last_6mths</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>delinq_2yrs</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dti</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>collections_12_mths_ex_med</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>acc_now_delinq</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tot_coll_amt</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tot_cur_bal</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>num_bc_sats</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>num_actv_rev_tl</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>num_actv_bc_tl</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>num_accts_ever_120_pd</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>mort_acc</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>mo_sin_rcnt_tl</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>mo_sin_rcnt_rev_tl_op</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mo_sin_old_rev_tl_op</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>delinq_amnt</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>chargeoff_within_12_mths</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bc_util</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bc_open_to_buy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>avg_cur_bal</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>acc_open_past_24mths</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>total_rev_hi_lim</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>issue_d_Jan-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>issue_d_Jul-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>issue_d_Jul-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>addr_state_IL</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>addr_state_ME</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>addr_state_MD</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>addr_state_MA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>addr_state_LA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>addr_state_KY</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>installment</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>addr_state_IN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>addr_state_ID</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>addr_state_CA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>addr_state_IA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>addr_state_HI</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>addr_state_GA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>addr_state_FL</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>addr_state_DE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>addr_state_DC</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>addr_state_CT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>addr_state_MI</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>addr_state_MN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>addr_state_MO</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>addr_state_MS</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>addr_state_SC</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>addr_state_RI</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>addr_state_PA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>addr_state_OR</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>addr_state_OK</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>addr_state_OH</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>addr_state_NY</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>addr_state_NV</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>addr_state_NM</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>addr_state_NJ</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>addr_state_NH</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>addr_state_NE</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>addr_state_ND</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>addr_state_NC</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>addr_state_MT</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>addr_state_CO</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>addr_state_AZ</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>issue_d_Jul-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>issue_d_Nov-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>loan_status_Default</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>loan_status_Charged Off</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>issue_d_Sep-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>issue_d_Sep-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>issue_d_Sep-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>issue_d_Oct-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>issue_d_Oct-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>issue_d_Nov-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>addr_state_AR</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>issue_d_May-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>issue_d_May-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>issue_d_May-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>issue_d_Mar-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>issue_d_Jun-2017</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>issue_d_Jun-2016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>issue_d_Jun-2014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>loan_status_In Grace Period</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>loan_status_Late (16-30 days)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>loan_status_Late (31-120 days)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>purpose_car</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>addr_state_AL</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>addr_state_AK</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>purpose_wedding</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>purpose_vacation</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>purpose_small_business</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>purpose_renewable_energy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>purpose_other</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>purpose_moving</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>purpose_medical</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>purpose_major_purchase</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>purpose_house</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>purpose_home_improvement</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>purpose_educational</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>purpose_debt_consolidation</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>purpose_credit_card</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>sec_app_flag_1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             columns  feat_score\n",
       "67                                           grade_A    0.121174\n",
       "2                                          sub_grade    0.099989\n",
       "71                                           grade_E    0.082811\n",
       "72                                           grade_F    0.080437\n",
       "68                                           grade_B    0.079938\n",
       "73                                           grade_G    0.069139\n",
       "242              mths_since_rcnt_il_cat_Not Reported    0.040754\n",
       "70                                           grade_D    0.022437\n",
       "69                                           grade_C    0.020738\n",
       "97                                  issue_d_Dec-2015    0.019895\n",
       "16                                   total_rec_prncp    0.018123\n",
       "135                                 issue_d_Oct-2013    0.017758\n",
       "137                                 issue_d_Oct-2015    0.016605\n",
       "111                                 issue_d_Jul-2015    0.016592\n",
       "130                                 issue_d_Nov-2013    0.016261\n",
       "132                                 issue_d_Nov-2015    0.016050\n",
       "107                                 issue_d_Jan-2016    0.013320\n",
       "126                                 issue_d_May-2015    0.012284\n",
       "91                                  issue_d_Aug-2015    0.012224\n",
       "140                                 issue_d_Sep-2013    0.012174\n",
       "95                                  issue_d_Dec-2013    0.011831\n",
       "85                                  issue_d_Apr-2015    0.011752\n",
       "122                                 issue_d_Mar-2016    0.011737\n",
       "83                                  issue_d_Apr-2013    0.011283\n",
       "114                                 issue_d_Jun-2013    0.010950\n",
       "124                                 issue_d_May-2013    0.010856\n",
       "13                                     out_prncp_inv    0.010368\n",
       "102                                 issue_d_Feb-2016    0.009302\n",
       "142                                 issue_d_Sep-2015    0.009176\n",
       "104                                 issue_d_Jan-2013    0.009131\n",
       "105                                 issue_d_Jan-2014    0.009115\n",
       "121                                 issue_d_Mar-2015    0.009057\n",
       "99                                  issue_d_Feb-2013    0.008939\n",
       "116                                 issue_d_Jun-2015    0.008468\n",
       "119                                 issue_d_Mar-2013    0.008182\n",
       "89                                  issue_d_Aug-2013    0.006106\n",
       "109                                 issue_d_Jul-2013    0.005879\n",
       "218                            initial_list_status_w    0.005429\n",
       "17                                     total_rec_int    0.004521\n",
       "217                            initial_list_status_f    0.003735\n",
       "86                                  issue_d_Apr-2016    0.003185\n",
       "101                                 issue_d_Feb-2015    0.003090\n",
       "129                                 issue_d_Nov-2012    0.003070\n",
       "94                                  issue_d_Dec-2012    0.003066\n",
       "100                                 issue_d_Feb-2014    0.002980\n",
       "134                                 issue_d_Oct-2012    0.002979\n",
       "82                      verification_status_Verified    0.002888\n",
       "139                                 issue_d_Sep-2012    0.002874\n",
       "84                                  issue_d_Apr-2014    0.002866\n",
       "12                                         out_prncp    0.002572\n",
       "148                           loan_status_Fully Paid    0.001928\n",
       "146                              loan_status_Current    0.001564\n",
       "276                       last_credit_pull_year_2017    0.001444\n",
       "21                                   last_pymnt_amnt    0.000412\n",
       "123                                 issue_d_Mar-2017    0.000294\n",
       "44                                         num_il_tl    0.000269\n",
       "224                           debt_settlement_flag_Y    0.000000\n",
       "273                       last_credit_pull_year_2014    0.000000\n",
       "274                       last_credit_pull_year_2015    0.000000\n",
       "223                           debt_settlement_flag_N    0.000000\n",
       "275                       last_credit_pull_year_2016    0.000000\n",
       "222                    disbursement_method_DirectPay    0.000000\n",
       "235           mths_since_recent_inq_cat_Not Reported    0.000000\n",
       "221                         disbursement_method_Cash    0.000000\n",
       "220                       application_type_Joint App    0.000000\n",
       "272                       last_credit_pull_year_2013    0.000000\n",
       "219                      application_type_Individual    0.000000\n",
       "277                       earliest_cr_line_year_1933    0.000000\n",
       "216                                    addr_state_WY    0.000000\n",
       "215                                    addr_state_WV    0.000000\n",
       "214                                    addr_state_WI    0.000000\n",
       "278                       earliest_cr_line_year_1934    0.000000\n",
       "279                       earliest_cr_line_year_1944    0.000000\n",
       "213                                    addr_state_WA    0.000000\n",
       "212                                    addr_state_VT    0.000000\n",
       "280                       earliest_cr_line_year_1945    0.000000\n",
       "211                                    addr_state_VA    0.000000\n",
       "225                     mths_since_recent_bc_cat_50+    0.000000\n",
       "270                         last_credit_pull_month_9    0.000000\n",
       "271                       last_credit_pull_year_2012    0.000000\n",
       "246  mths_since_recent_revol_delinq_cat_Not Reported    0.000000\n",
       "254        mths_since_recent_bc_dlq_cat_Not Reported    0.000000\n",
       "253                 mths_since_recent_bc_dlq_cat_75+    0.000000\n",
       "252               mths_since_recent_bc_dlq_cat_50-75    0.000000\n",
       "251                mths_since_recent_bc_dlq_cat_0-50    0.000000\n",
       "250     mths_since_last_major_derog_cat_Not Reported    0.000000\n",
       "249              mths_since_last_major_derog_cat_75+    0.000000\n",
       "248            mths_since_last_major_derog_cat_50-75    0.000000\n",
       "247             mths_since_last_major_derog_cat_0-50    0.000000\n",
       "245           mths_since_recent_revol_delinq_cat_75+    0.000000\n",
       "256                 mths_since_last_record_cat_40-80    0.000000\n",
       "244         mths_since_recent_revol_delinq_cat_50-75    0.000000\n",
       "243          mths_since_recent_revol_delinq_cat_0-50    0.000000\n",
       "234                   mths_since_recent_inq_cat_5-15    0.000000\n",
       "241                       mths_since_rcnt_il_cat_<50    0.000000\n",
       "240                       mths_since_rcnt_il_cat_50+    0.000000\n",
       "239          mths_since_last_delinq_cat_Not Reported    0.000000\n",
       "238                   mths_since_last_delinq_cat_75+    0.000000\n",
       "237                 mths_since_last_delinq_cat_50-75    0.000000\n",
       "255                  mths_since_last_record_cat_0-40    0.000000\n",
       "257                   mths_since_last_record_cat_80+    0.000000\n",
       "236                  mths_since_last_delinq_cat_0-50    0.000000\n",
       "268                         last_credit_pull_month_7    0.000000\n",
       "269                         last_credit_pull_month_8    0.000000\n",
       "226                     mths_since_recent_bc_cat_<50    0.000000\n",
       "227            mths_since_recent_bc_cat_Not Reported    0.000000\n",
       "228                     mo_sin_old_il_acct_cat_0-150    0.000000\n",
       "229                   mo_sin_old_il_acct_cat_150-200    0.000000\n",
       "230                      mo_sin_old_il_acct_cat_200+    0.000000\n",
       "231              mo_sin_old_il_acct_cat_Not Reported    0.000000\n",
       "232                    mths_since_recent_inq_cat_0-5    0.000000\n",
       "267                         last_credit_pull_month_6    0.000000\n",
       "258          mths_since_last_record_cat_Not Reported    0.000000\n",
       "266                         last_credit_pull_month_5    0.000000\n",
       "265                         last_credit_pull_month_4    0.000000\n",
       "264                         last_credit_pull_month_3    0.000000\n",
       "233                    mths_since_recent_inq_cat_15+    0.000000\n",
       "262                        last_credit_pull_month_12    0.000000\n",
       "261                        last_credit_pull_month_11    0.000000\n",
       "260                        last_credit_pull_month_10    0.000000\n",
       "259                         last_credit_pull_month_1    0.000000\n",
       "263                         last_credit_pull_month_2    0.000000\n",
       "0                                          loan_amnt    0.000000\n",
       "281                       earliest_cr_line_year_1946    0.000000\n",
       "334                       earliest_cr_line_year_2000    0.000000\n",
       "342                       earliest_cr_line_year_2008    0.000000\n",
       "341                       earliest_cr_line_year_2007    0.000000\n",
       "340                       earliest_cr_line_year_2006    0.000000\n",
       "339                       earliest_cr_line_year_2005    0.000000\n",
       "338                       earliest_cr_line_year_2004    0.000000\n",
       "337                       earliest_cr_line_year_2003    0.000000\n",
       "336                       earliest_cr_line_year_2002    0.000000\n",
       "335                       earliest_cr_line_year_2001    0.000000\n",
       "333                       earliest_cr_line_year_1999    0.000000\n",
       "323                       earliest_cr_line_year_1989    0.000000\n",
       "332                       earliest_cr_line_year_1998    0.000000\n",
       "331                       earliest_cr_line_year_1997    0.000000\n",
       "330                       earliest_cr_line_year_1996    0.000000\n",
       "329                       earliest_cr_line_year_1995    0.000000\n",
       "328                       earliest_cr_line_year_1994    0.000000\n",
       "327                       earliest_cr_line_year_1993    0.000000\n",
       "326                       earliest_cr_line_year_1992    0.000000\n",
       "325                       earliest_cr_line_year_1991    0.000000\n",
       "343                       earliest_cr_line_year_2009    0.000000\n",
       "344                       earliest_cr_line_year_2010    0.000000\n",
       "345                       earliest_cr_line_year_2011    0.000000\n",
       "346                       earliest_cr_line_year_2012    0.000000\n",
       "363                                   sec_app_flag_0    0.000000\n",
       "362                                          term_60    0.000000\n",
       "361                                          term_36    0.000000\n",
       "360                        earliest_cr_line_month_12    0.000000\n",
       "359                        earliest_cr_line_month_11    0.000000\n",
       "358                        earliest_cr_line_month_10    0.000000\n",
       "357                        earliest_cr_line_month_09    0.000000\n",
       "356                        earliest_cr_line_month_08    0.000000\n",
       "355                        earliest_cr_line_month_07    0.000000\n",
       "354                        earliest_cr_line_month_06    0.000000\n",
       "353                        earliest_cr_line_month_05    0.000000\n",
       "352                        earliest_cr_line_month_04    0.000000\n",
       "351                        earliest_cr_line_month_03    0.000000\n",
       "350                        earliest_cr_line_month_02    0.000000\n",
       "349                        earliest_cr_line_month_01    0.000000\n",
       "348                       earliest_cr_line_year_2014    0.000000\n",
       "347                       earliest_cr_line_year_2013    0.000000\n",
       "324                       earliest_cr_line_year_1990    0.000000\n",
       "322                       earliest_cr_line_year_1988    0.000000\n",
       "282                       earliest_cr_line_year_1948    0.000000\n",
       "291                       earliest_cr_line_year_1957    0.000000\n",
       "299                       earliest_cr_line_year_1965    0.000000\n",
       "298                       earliest_cr_line_year_1964    0.000000\n",
       "297                       earliest_cr_line_year_1963    0.000000\n",
       "296                       earliest_cr_line_year_1962    0.000000\n",
       "295                       earliest_cr_line_year_1961    0.000000\n",
       "294                       earliest_cr_line_year_1960    0.000000\n",
       "293                       earliest_cr_line_year_1959    0.000000\n",
       "292                       earliest_cr_line_year_1958    0.000000\n",
       "290                       earliest_cr_line_year_1956    0.000000\n",
       "321                       earliest_cr_line_year_1987    0.000000\n",
       "209                                    addr_state_TX    0.000000\n",
       "289                       earliest_cr_line_year_1955    0.000000\n",
       "288                       earliest_cr_line_year_1954    0.000000\n",
       "287                       earliest_cr_line_year_1953    0.000000\n",
       "286                       earliest_cr_line_year_1952    0.000000\n",
       "285                       earliest_cr_line_year_1951    0.000000\n",
       "284                       earliest_cr_line_year_1950    0.000000\n",
       "283                       earliest_cr_line_year_1949    0.000000\n",
       "300                       earliest_cr_line_year_1966    0.000000\n",
       "301                       earliest_cr_line_year_1967    0.000000\n",
       "302                       earliest_cr_line_year_1968    0.000000\n",
       "303                       earliest_cr_line_year_1969    0.000000\n",
       "320                       earliest_cr_line_year_1986    0.000000\n",
       "319                       earliest_cr_line_year_1985    0.000000\n",
       "318                       earliest_cr_line_year_1984    0.000000\n",
       "317                       earliest_cr_line_year_1983    0.000000\n",
       "316                       earliest_cr_line_year_1982    0.000000\n",
       "315                       earliest_cr_line_year_1981    0.000000\n",
       "314                       earliest_cr_line_year_1980    0.000000\n",
       "313                       earliest_cr_line_year_1979    0.000000\n",
       "312                       earliest_cr_line_year_1978    0.000000\n",
       "311                       earliest_cr_line_year_1977    0.000000\n",
       "310                       earliest_cr_line_year_1976    0.000000\n",
       "309                       earliest_cr_line_year_1975    0.000000\n",
       "308                       earliest_cr_line_year_1974    0.000000\n",
       "307                       earliest_cr_line_year_1973    0.000000\n",
       "306                       earliest_cr_line_year_1972    0.000000\n",
       "305                       earliest_cr_line_year_1971    0.000000\n",
       "304                       earliest_cr_line_year_1970    0.000000\n",
       "210                                    addr_state_UT    0.000000\n",
       "182                                    addr_state_KS    0.000000\n",
       "208                                    addr_state_TN    0.000000\n",
       "56                                         tax_liens    0.000000\n",
       "63                           earliest_cr_line_months    0.000000\n",
       "62                                  annual_inc_final    0.000000\n",
       "61                                 sec_app_fico_best    0.000000\n",
       "60                        total_il_high_credit_limit    0.000000\n",
       "59                                    total_bc_limit    0.000000\n",
       "58                                 total_bal_ex_mort    0.000000\n",
       "57                                   tot_hi_cred_lim    0.000000\n",
       "55                              pub_rec_bankruptcies    0.000000\n",
       "65                                      average_fico    0.000000\n",
       "54                                  percent_bc_gt_75    0.000000\n",
       "53                                    pct_tl_nvr_dlq    0.000000\n",
       "52                                num_tl_op_past_12m    0.000000\n",
       "51                                num_tl_90g_dpd_24m    0.000000\n",
       "50                                      num_tl_30dpd    0.000000\n",
       "49                                  num_tl_120dpd_2m    0.000000\n",
       "48                                          num_sats    0.000000\n",
       "64                                 emp_length_floats    0.000000\n",
       "66                                         dti_final    0.000000\n",
       "207                                    addr_state_SD    0.000000\n",
       "88                                  issue_d_Aug-2012    0.000000\n",
       "106                                 issue_d_Jan-2015    0.000000\n",
       "103                                 issue_d_Feb-2017    0.000000\n",
       "98                                  issue_d_Dec-2016    0.000000\n",
       "96                                  issue_d_Dec-2014    0.000000\n",
       "93                                  issue_d_Aug-2017    0.000000\n",
       "92                                  issue_d_Aug-2016    0.000000\n",
       "90                                  issue_d_Aug-2014    0.000000\n",
       "87                                  issue_d_Apr-2017    0.000000\n",
       "74                                home_ownership_ANY    0.000000\n",
       "81               verification_status_Source Verified    0.000000\n",
       "80                  verification_status_Not Verified    0.000000\n",
       "79                               home_ownership_RENT    0.000000\n",
       "78                                home_ownership_OWN    0.000000\n",
       "77                              home_ownership_OTHER    0.000000\n",
       "76                               home_ownership_NONE    0.000000\n",
       "75                           home_ownership_MORTGAGE    0.000000\n",
       "47                               num_rev_tl_bal_gt_0    0.000000\n",
       "46                                     num_rev_accts    0.000000\n",
       "45                                     num_op_rev_tl    0.000000\n",
       "11                                         total_acc    0.000000\n",
       "23                               last_fico_range_low    0.000000\n",
       "22                              last_fico_range_high    0.000000\n",
       "20                           collection_recovery_fee    0.000000\n",
       "19                                        recoveries    0.000000\n",
       "18                                total_rec_late_fee    0.000000\n",
       "15                                   total_pymnt_inv    0.000000\n",
       "14                                       total_pymnt    0.000000\n",
       "10                                        revol_util    0.000000\n",
       "43                                         num_bc_tl    0.000000\n",
       "9                                          revol_bal    0.000000\n",
       "8                                            pub_rec    0.000000\n",
       "7                                           open_acc    0.000000\n",
       "6                                     inq_last_6mths    0.000000\n",
       "5                                        delinq_2yrs    0.000000\n",
       "4                                                dti    0.000000\n",
       "3                                         annual_inc    0.000000\n",
       "24                        collections_12_mths_ex_med    0.000000\n",
       "25                                    acc_now_delinq    0.000000\n",
       "26                                      tot_coll_amt    0.000000\n",
       "27                                       tot_cur_bal    0.000000\n",
       "42                                       num_bc_sats    0.000000\n",
       "41                                   num_actv_rev_tl    0.000000\n",
       "40                                    num_actv_bc_tl    0.000000\n",
       "39                             num_accts_ever_120_pd    0.000000\n",
       "38                                          mort_acc    0.000000\n",
       "37                                    mo_sin_rcnt_tl    0.000000\n",
       "36                             mo_sin_rcnt_rev_tl_op    0.000000\n",
       "35                              mo_sin_old_rev_tl_op    0.000000\n",
       "34                                       delinq_amnt    0.000000\n",
       "33                          chargeoff_within_12_mths    0.000000\n",
       "32                                           bc_util    0.000000\n",
       "31                                    bc_open_to_buy    0.000000\n",
       "30                                       avg_cur_bal    0.000000\n",
       "29                              acc_open_past_24mths    0.000000\n",
       "28                                  total_rev_hi_lim    0.000000\n",
       "108                                 issue_d_Jan-2017    0.000000\n",
       "110                                 issue_d_Jul-2014    0.000000\n",
       "112                                 issue_d_Jul-2016    0.000000\n",
       "180                                    addr_state_IL    0.000000\n",
       "187                                    addr_state_ME    0.000000\n",
       "186                                    addr_state_MD    0.000000\n",
       "185                                    addr_state_MA    0.000000\n",
       "184                                    addr_state_LA    0.000000\n",
       "183                                    addr_state_KY    0.000000\n",
       "1                                        installment    0.000000\n",
       "181                                    addr_state_IN    0.000000\n",
       "179                                    addr_state_ID    0.000000\n",
       "170                                    addr_state_CA    0.000000\n",
       "178                                    addr_state_IA    0.000000\n",
       "177                                    addr_state_HI    0.000000\n",
       "176                                    addr_state_GA    0.000000\n",
       "175                                    addr_state_FL    0.000000\n",
       "174                                    addr_state_DE    0.000000\n",
       "173                                    addr_state_DC    0.000000\n",
       "172                                    addr_state_CT    0.000000\n",
       "188                                    addr_state_MI    0.000000\n",
       "189                                    addr_state_MN    0.000000\n",
       "190                                    addr_state_MO    0.000000\n",
       "191                                    addr_state_MS    0.000000\n",
       "206                                    addr_state_SC    0.000000\n",
       "205                                    addr_state_RI    0.000000\n",
       "204                                    addr_state_PA    0.000000\n",
       "203                                    addr_state_OR    0.000000\n",
       "202                                    addr_state_OK    0.000000\n",
       "201                                    addr_state_OH    0.000000\n",
       "200                                    addr_state_NY    0.000000\n",
       "199                                    addr_state_NV    0.000000\n",
       "198                                    addr_state_NM    0.000000\n",
       "197                                    addr_state_NJ    0.000000\n",
       "196                                    addr_state_NH    0.000000\n",
       "195                                    addr_state_NE    0.000000\n",
       "194                                    addr_state_ND    0.000000\n",
       "193                                    addr_state_NC    0.000000\n",
       "192                                    addr_state_MT    0.000000\n",
       "171                                    addr_state_CO    0.000000\n",
       "169                                    addr_state_AZ    0.000000\n",
       "113                                 issue_d_Jul-2017    0.000000\n",
       "133                                 issue_d_Nov-2016    0.000000\n",
       "147                              loan_status_Default    0.000000\n",
       "145                          loan_status_Charged Off    0.000000\n",
       "144                                 issue_d_Sep-2017    0.000000\n",
       "143                                 issue_d_Sep-2016    0.000000\n",
       "141                                 issue_d_Sep-2014    0.000000\n",
       "138                                 issue_d_Oct-2016    0.000000\n",
       "136                                 issue_d_Oct-2014    0.000000\n",
       "131                                 issue_d_Nov-2014    0.000000\n",
       "168                                    addr_state_AR    0.000000\n",
       "128                                 issue_d_May-2017    0.000000\n",
       "127                                 issue_d_May-2016    0.000000\n",
       "125                                 issue_d_May-2014    0.000000\n",
       "120                                 issue_d_Mar-2014    0.000000\n",
       "118                                 issue_d_Jun-2017    0.000000\n",
       "117                                 issue_d_Jun-2016    0.000000\n",
       "115                                 issue_d_Jun-2014    0.000000\n",
       "149                      loan_status_In Grace Period    0.000000\n",
       "150                    loan_status_Late (16-30 days)    0.000000\n",
       "151                   loan_status_Late (31-120 days)    0.000000\n",
       "152                                      purpose_car    0.000000\n",
       "167                                    addr_state_AL    0.000000\n",
       "166                                    addr_state_AK    0.000000\n",
       "165                                  purpose_wedding    0.000000\n",
       "164                                 purpose_vacation    0.000000\n",
       "163                           purpose_small_business    0.000000\n",
       "162                         purpose_renewable_energy    0.000000\n",
       "161                                    purpose_other    0.000000\n",
       "160                                   purpose_moving    0.000000\n",
       "159                                  purpose_medical    0.000000\n",
       "158                           purpose_major_purchase    0.000000\n",
       "157                                    purpose_house    0.000000\n",
       "156                         purpose_home_improvement    0.000000\n",
       "155                              purpose_educational    0.000000\n",
       "154                       purpose_debt_consolidation    0.000000\n",
       "153                              purpose_credit_card    0.000000\n",
       "364                                   sec_app_flag_1    0.000000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_features=pd.DataFrame({'feat_score':grad_model.feature_importances_, 'columns': col_names}).sort_values(by='feat_score', ascending = False)\n",
    "grad_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.09998885,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00257181,  0.01036794,  0.        ,\n",
       "        0.        ,  0.01812335,  0.00452129,  0.        ,  0.        ,\n",
       "        0.        ,  0.00041165,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00026932,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.12117436,  0.0799377 ,  0.02073806,\n",
       "        0.02243717,  0.08281099,  0.08043656,  0.06913856,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00288787,  0.01128283,  0.00286611,\n",
       "        0.01175196,  0.00318457,  0.        ,  0.        ,  0.00610627,\n",
       "        0.        ,  0.01222362,  0.        ,  0.        ,  0.00306612,\n",
       "        0.01183148,  0.        ,  0.01989498,  0.        ,  0.008939  ,\n",
       "        0.00298028,  0.0030899 ,  0.00930237,  0.        ,  0.00913057,\n",
       "        0.00911453,  0.        ,  0.01332007,  0.        ,  0.00587931,\n",
       "        0.        ,  0.01659203,  0.        ,  0.        ,  0.01094977,\n",
       "        0.        ,  0.00846772,  0.        ,  0.        ,  0.00818244,\n",
       "        0.        ,  0.0090568 ,  0.01173662,  0.00029439,  0.01085615,\n",
       "        0.        ,  0.01228406,  0.        ,  0.        ,  0.00307015,\n",
       "        0.0162614 ,  0.        ,  0.01604962,  0.        ,  0.0029787 ,\n",
       "        0.01775846,  0.        ,  0.0166049 ,  0.        ,  0.00287392,\n",
       "        0.01217369,  0.        ,  0.00917618,  0.        ,  0.        ,\n",
       "        0.        ,  0.00156434,  0.        ,  0.00192816,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00373513,  0.00542862,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.04075362,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.0014437 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt\n",
      "installment\n",
      "sub_grade\n",
      "annual_inc\n",
      "dti\n",
      "delinq_2yrs\n",
      "inq_last_6mths\n",
      "open_acc\n",
      "pub_rec\n",
      "revol_bal\n",
      "revol_util\n",
      "total_acc\n",
      "out_prncp\n",
      "out_prncp_inv\n",
      "total_pymnt\n",
      "total_pymnt_inv\n",
      "total_rec_prncp\n",
      "total_rec_int\n",
      "total_rec_late_fee\n",
      "recoveries\n",
      "collection_recovery_fee\n",
      "last_pymnt_amnt\n",
      "last_fico_range_high\n",
      "last_fico_range_low\n",
      "collections_12_mths_ex_med\n",
      "acc_now_delinq\n",
      "tot_coll_amt\n",
      "tot_cur_bal\n",
      "total_rev_hi_lim\n",
      "acc_open_past_24mths\n",
      "avg_cur_bal\n",
      "bc_open_to_buy\n",
      "bc_util\n",
      "chargeoff_within_12_mths\n",
      "delinq_amnt\n",
      "mo_sin_old_rev_tl_op\n",
      "mo_sin_rcnt_rev_tl_op\n",
      "mo_sin_rcnt_tl\n",
      "mort_acc\n",
      "num_accts_ever_120_pd\n",
      "num_actv_bc_tl\n",
      "num_actv_rev_tl\n",
      "num_bc_sats\n",
      "num_bc_tl\n",
      "num_il_tl\n",
      "num_op_rev_tl\n",
      "num_rev_accts\n",
      "num_rev_tl_bal_gt_0\n",
      "num_sats\n",
      "num_tl_120dpd_2m\n",
      "num_tl_30dpd\n",
      "num_tl_90g_dpd_24m\n",
      "num_tl_op_past_12m\n",
      "pct_tl_nvr_dlq\n",
      "percent_bc_gt_75\n",
      "pub_rec_bankruptcies\n",
      "tax_liens\n",
      "tot_hi_cred_lim\n",
      "total_bal_ex_mort\n",
      "total_bc_limit\n",
      "total_il_high_credit_limit\n",
      "sec_app_fico_best\n",
      "annual_inc_final\n",
      "earliest_cr_line_months\n",
      "emp_length_floats\n",
      "average_fico\n",
      "dti_final\n",
      "grade_A\n",
      "grade_B\n",
      "grade_C\n",
      "grade_D\n",
      "grade_E\n",
      "grade_F\n",
      "grade_G\n",
      "home_ownership_ANY\n",
      "home_ownership_MORTGAGE\n",
      "home_ownership_NONE\n",
      "home_ownership_OTHER\n",
      "home_ownership_OWN\n",
      "home_ownership_RENT\n",
      "verification_status_Not Verified\n",
      "verification_status_Source Verified\n",
      "verification_status_Verified\n",
      "issue_d_Apr-2013\n",
      "issue_d_Apr-2014\n",
      "issue_d_Apr-2015\n",
      "issue_d_Apr-2016\n",
      "issue_d_Apr-2017\n",
      "issue_d_Aug-2012\n",
      "issue_d_Aug-2013\n",
      "issue_d_Aug-2014\n",
      "issue_d_Aug-2015\n",
      "issue_d_Aug-2016\n",
      "issue_d_Aug-2017\n",
      "issue_d_Dec-2012\n",
      "issue_d_Dec-2013\n",
      "issue_d_Dec-2014\n",
      "issue_d_Dec-2015\n",
      "issue_d_Dec-2016\n",
      "issue_d_Feb-2013\n",
      "issue_d_Feb-2014\n",
      "issue_d_Feb-2015\n",
      "issue_d_Feb-2016\n",
      "issue_d_Feb-2017\n",
      "issue_d_Jan-2013\n",
      "issue_d_Jan-2014\n",
      "issue_d_Jan-2015\n",
      "issue_d_Jan-2016\n",
      "issue_d_Jan-2017\n",
      "issue_d_Jul-2013\n",
      "issue_d_Jul-2014\n",
      "issue_d_Jul-2015\n",
      "issue_d_Jul-2016\n",
      "issue_d_Jul-2017\n",
      "issue_d_Jun-2013\n",
      "issue_d_Jun-2014\n",
      "issue_d_Jun-2015\n",
      "issue_d_Jun-2016\n",
      "issue_d_Jun-2017\n",
      "issue_d_Mar-2013\n",
      "issue_d_Mar-2014\n",
      "issue_d_Mar-2015\n",
      "issue_d_Mar-2016\n",
      "issue_d_Mar-2017\n",
      "issue_d_May-2013\n",
      "issue_d_May-2014\n",
      "issue_d_May-2015\n",
      "issue_d_May-2016\n",
      "issue_d_May-2017\n",
      "issue_d_Nov-2012\n",
      "issue_d_Nov-2013\n",
      "issue_d_Nov-2014\n",
      "issue_d_Nov-2015\n",
      "issue_d_Nov-2016\n",
      "issue_d_Oct-2012\n",
      "issue_d_Oct-2013\n",
      "issue_d_Oct-2014\n",
      "issue_d_Oct-2015\n",
      "issue_d_Oct-2016\n",
      "issue_d_Sep-2012\n",
      "issue_d_Sep-2013\n",
      "issue_d_Sep-2014\n",
      "issue_d_Sep-2015\n",
      "issue_d_Sep-2016\n",
      "issue_d_Sep-2017\n",
      "loan_status_Charged Off\n",
      "loan_status_Current\n",
      "loan_status_Default\n",
      "loan_status_Fully Paid\n",
      "loan_status_In Grace Period\n",
      "loan_status_Late (16-30 days)\n",
      "loan_status_Late (31-120 days)\n",
      "purpose_car\n",
      "purpose_credit_card\n",
      "purpose_debt_consolidation\n",
      "purpose_educational\n",
      "purpose_home_improvement\n",
      "purpose_house\n",
      "purpose_major_purchase\n",
      "purpose_medical\n",
      "purpose_moving\n",
      "purpose_other\n",
      "purpose_renewable_energy\n",
      "purpose_small_business\n",
      "purpose_vacation\n",
      "purpose_wedding\n",
      "addr_state_AK\n",
      "addr_state_AL\n",
      "addr_state_AR\n",
      "addr_state_AZ\n",
      "addr_state_CA\n",
      "addr_state_CO\n",
      "addr_state_CT\n",
      "addr_state_DC\n",
      "addr_state_DE\n",
      "addr_state_FL\n",
      "addr_state_GA\n",
      "addr_state_HI\n",
      "addr_state_IA\n",
      "addr_state_ID\n",
      "addr_state_IL\n",
      "addr_state_IN\n",
      "addr_state_KS\n",
      "addr_state_KY\n",
      "addr_state_LA\n",
      "addr_state_MA\n",
      "addr_state_MD\n",
      "addr_state_ME\n",
      "addr_state_MI\n",
      "addr_state_MN\n",
      "addr_state_MO\n",
      "addr_state_MS\n",
      "addr_state_MT\n",
      "addr_state_NC\n",
      "addr_state_ND\n",
      "addr_state_NE\n",
      "addr_state_NH\n",
      "addr_state_NJ\n",
      "addr_state_NM\n",
      "addr_state_NV\n",
      "addr_state_NY\n",
      "addr_state_OH\n",
      "addr_state_OK\n",
      "addr_state_OR\n",
      "addr_state_PA\n",
      "addr_state_RI\n",
      "addr_state_SC\n",
      "addr_state_SD\n",
      "addr_state_TN\n",
      "addr_state_TX\n",
      "addr_state_UT\n",
      "addr_state_VA\n",
      "addr_state_VT\n",
      "addr_state_WA\n",
      "addr_state_WI\n",
      "addr_state_WV\n",
      "addr_state_WY\n",
      "initial_list_status_f\n",
      "initial_list_status_w\n",
      "application_type_Individual\n",
      "application_type_Joint App\n",
      "disbursement_method_Cash\n",
      "disbursement_method_DirectPay\n",
      "debt_settlement_flag_N\n",
      "debt_settlement_flag_Y\n",
      "mths_since_recent_bc_cat_50+\n",
      "mths_since_recent_bc_cat_<50\n",
      "mths_since_recent_bc_cat_Not Reported\n",
      "mo_sin_old_il_acct_cat_0-150\n",
      "mo_sin_old_il_acct_cat_150-200\n",
      "mo_sin_old_il_acct_cat_200+\n",
      "mo_sin_old_il_acct_cat_Not Reported\n",
      "mths_since_recent_inq_cat_0-5\n",
      "mths_since_recent_inq_cat_15+\n",
      "mths_since_recent_inq_cat_5-15\n",
      "mths_since_recent_inq_cat_Not Reported\n",
      "mths_since_last_delinq_cat_0-50\n",
      "mths_since_last_delinq_cat_50-75\n",
      "mths_since_last_delinq_cat_75+\n",
      "mths_since_last_delinq_cat_Not Reported\n",
      "mths_since_rcnt_il_cat_50+\n",
      "mths_since_rcnt_il_cat_<50\n",
      "mths_since_rcnt_il_cat_Not Reported\n",
      "mths_since_recent_revol_delinq_cat_0-50\n",
      "mths_since_recent_revol_delinq_cat_50-75\n",
      "mths_since_recent_revol_delinq_cat_75+\n",
      "mths_since_recent_revol_delinq_cat_Not Reported\n",
      "mths_since_last_major_derog_cat_0-50\n",
      "mths_since_last_major_derog_cat_50-75\n",
      "mths_since_last_major_derog_cat_75+\n",
      "mths_since_last_major_derog_cat_Not Reported\n",
      "mths_since_recent_bc_dlq_cat_0-50\n",
      "mths_since_recent_bc_dlq_cat_50-75\n",
      "mths_since_recent_bc_dlq_cat_75+\n",
      "mths_since_recent_bc_dlq_cat_Not Reported\n",
      "mths_since_last_record_cat_0-40\n",
      "mths_since_last_record_cat_40-80\n",
      "mths_since_last_record_cat_80+\n",
      "mths_since_last_record_cat_Not Reported\n",
      "last_credit_pull_month_1\n",
      "last_credit_pull_month_10\n",
      "last_credit_pull_month_11\n",
      "last_credit_pull_month_12\n",
      "last_credit_pull_month_2\n",
      "last_credit_pull_month_3\n",
      "last_credit_pull_month_4\n",
      "last_credit_pull_month_5\n",
      "last_credit_pull_month_6\n",
      "last_credit_pull_month_7\n",
      "last_credit_pull_month_8\n",
      "last_credit_pull_month_9\n",
      "last_credit_pull_year_2012\n",
      "last_credit_pull_year_2013\n",
      "last_credit_pull_year_2014\n",
      "last_credit_pull_year_2015\n",
      "last_credit_pull_year_2016\n",
      "last_credit_pull_year_2017\n",
      "earliest_cr_line_year_1933\n",
      "earliest_cr_line_year_1934\n",
      "earliest_cr_line_year_1944\n",
      "earliest_cr_line_year_1945\n",
      "earliest_cr_line_year_1946\n",
      "earliest_cr_line_year_1948\n",
      "earliest_cr_line_year_1949\n",
      "earliest_cr_line_year_1950\n",
      "earliest_cr_line_year_1951\n",
      "earliest_cr_line_year_1952\n",
      "earliest_cr_line_year_1953\n",
      "earliest_cr_line_year_1954\n",
      "earliest_cr_line_year_1955\n",
      "earliest_cr_line_year_1956\n",
      "earliest_cr_line_year_1957\n",
      "earliest_cr_line_year_1958\n",
      "earliest_cr_line_year_1959\n",
      "earliest_cr_line_year_1960\n",
      "earliest_cr_line_year_1961\n",
      "earliest_cr_line_year_1962\n",
      "earliest_cr_line_year_1963\n",
      "earliest_cr_line_year_1964\n",
      "earliest_cr_line_year_1965\n",
      "earliest_cr_line_year_1966\n",
      "earliest_cr_line_year_1967\n",
      "earliest_cr_line_year_1968\n",
      "earliest_cr_line_year_1969\n",
      "earliest_cr_line_year_1970\n",
      "earliest_cr_line_year_1971\n",
      "earliest_cr_line_year_1972\n",
      "earliest_cr_line_year_1973\n",
      "earliest_cr_line_year_1974\n",
      "earliest_cr_line_year_1975\n",
      "earliest_cr_line_year_1976\n",
      "earliest_cr_line_year_1977\n",
      "earliest_cr_line_year_1978\n",
      "earliest_cr_line_year_1979\n",
      "earliest_cr_line_year_1980\n",
      "earliest_cr_line_year_1981\n",
      "earliest_cr_line_year_1982\n",
      "earliest_cr_line_year_1983\n",
      "earliest_cr_line_year_1984\n",
      "earliest_cr_line_year_1985\n",
      "earliest_cr_line_year_1986\n",
      "earliest_cr_line_year_1987\n",
      "earliest_cr_line_year_1988\n",
      "earliest_cr_line_year_1989\n",
      "earliest_cr_line_year_1990\n",
      "earliest_cr_line_year_1991\n",
      "earliest_cr_line_year_1992\n",
      "earliest_cr_line_year_1993\n",
      "earliest_cr_line_year_1994\n",
      "earliest_cr_line_year_1995\n",
      "earliest_cr_line_year_1996\n",
      "earliest_cr_line_year_1997\n",
      "earliest_cr_line_year_1998\n",
      "earliest_cr_line_year_1999\n",
      "earliest_cr_line_year_2000\n",
      "earliest_cr_line_year_2001\n",
      "earliest_cr_line_year_2002\n",
      "earliest_cr_line_year_2003\n",
      "earliest_cr_line_year_2004\n",
      "earliest_cr_line_year_2005\n",
      "earliest_cr_line_year_2006\n",
      "earliest_cr_line_year_2007\n",
      "earliest_cr_line_year_2008\n",
      "earliest_cr_line_year_2009\n",
      "earliest_cr_line_year_2010\n",
      "earliest_cr_line_year_2011\n",
      "earliest_cr_line_year_2012\n",
      "earliest_cr_line_year_2013\n",
      "earliest_cr_line_year_2014\n",
      "earliest_cr_line_month_01\n",
      "earliest_cr_line_month_02\n",
      "earliest_cr_line_month_03\n",
      "earliest_cr_line_month_04\n",
      "earliest_cr_line_month_05\n",
      "earliest_cr_line_month_06\n",
      "earliest_cr_line_month_07\n",
      "earliest_cr_line_month_08\n",
      "earliest_cr_line_month_09\n",
      "earliest_cr_line_month_10\n",
      "earliest_cr_line_month_11\n",
      "earliest_cr_line_month_12\n",
      "term_36\n",
      "term_60\n",
      "sec_app_flag_0\n",
      "sec_app_flag_1\n"
     ]
    }
   ],
   "source": [
    "for i in col_names:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####OLD FORMAT WITHOUT LAYER BUILDING \n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "def iter_ANN_prelu(inputs = X_train.shape[1],\n",
    "             lasso_p = .1,\n",
    "             ridge_p = .1,\n",
    "             layer_activation = 'relu',\n",
    "             nodes = 30, #25 best so far\n",
    "             do = .1, #.1 best so far\n",
    "             num_layers = 4, #3 best so far\n",
    "             output_activation = None, #Reformat\n",
    "             optimizer = 'adam',\n",
    "             loss = 'mean_squared_error',        #Reformatted\n",
    "             metrics = None):\n",
    "    \n",
    "    # Set up Sequential ANN and Input Layer\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nodes, input_shape=(inputs,)))\n",
    "    #Define the 'prelu'\n",
    "    act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\n",
    "    \n",
    "    model.add(Dense(nodes, kernel_regularizer=regularizers.l1_l2(l1 = lasso_p, l2 = ridge_p)))\n",
    "    #add PreLU\n",
    "    model.add(act)\n",
    "    model.add(Dropout(do))\n",
    "    model.add(Dense(nodes, kernel_regularizer=regularizers.l1_l2(l1 = lasso_p, l2 = ridge_p))) #\n",
    "    #add PreLU\n",
    "    model.add(act)\n",
    "    model.add(Dropout(do))                       \n",
    "    model.add(Dense(nodes, kernel_regularizer=regularizers.l1_l2(l1 = lasso_p, l2 = ridge_p),\n",
    "                 activation = layer_activation))\n",
    "    model.add(Dropout(do))\n",
    "    \n",
    "    # Set Up Output Layer \n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(1, activation = output_activation))\n",
    "    \n",
    "    model.compile(optimizer = optimizer,\n",
    "                           loss = loss)\n",
    "    print(\"Loss function: \" + model.loss)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
